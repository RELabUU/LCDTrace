[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 823.909 seconds
Cross-validation score: 0.7085029333191097
Test score: 0.42168674698795183
Best Hyperparameters: {'classifier__num_leaves': 26, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 700, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.6}
2569.5272483310036
1276.6614716475651
7474.538756780246
147.44615494912432
8.71322214823158
1.3915219213813543
0.6216594033758156
0.0
43.88744948152453
0.0
14923.976868411954
8.597509910992812
0.13108199834823608
0.0
31.842985122348182
0.0
0.32985475577879697
0.0
7.641203544102609
0.025456000119447708
14.299020290374756
13.976986379362643
0.13098500669002533
263.2693226051633
0.5521149933338165
0.000727317004930228
3.507319927215576
0.0
0.0
177.23335225030314
18.74914658988382
0.0
13.15808476224629
55.77246654911323
71.2204786189759
0.0
0.0
0.0
44.71841320551721
2780.656569312936
29.607128591975197
282.9788127170068
8.501652096398175
87.92338288162682
15.911063419785933
1.2723524263346917
0.02401094690139871
0.0
0.0
0.0
22.673233073692245
9.361951336144557
43.114211635989705
0.16375166032230482
0.912699422087826
15.533767681801692
17.071964131784625
0.0011472840269561857
75.77808054472916
840.5428702619756
404.45943102900856
77.34910756586032
9.324648991896538
0.04375585248635616
0.000131948012384823
76.70047351540416
6.004995043564122
0.22396500408649445
3.0532124832370755
586.9159923835996
28.233401421974122
35.97959182970226
1.5647718844484189
0.0
0.0
0.19622500234982
0.0
11.768163463799283
76.54224271906241
0.08089300291794643
0.11707443338673329
245.57131145593303
92.12866784538346
350.9712041965977
219.76473455849919
491.0190124511719
38.33667478948337
0.0
0.06031360478664283
9.32388985157013
11.522881932673045
8.797800064086914
0.9987224431206414
1861.3225512454483
0.50485427365129
5.4659565419424325
273.1425684318856
0.0
17.778323386100965
598.6891047351728
40.65993295190856
128.7845665863133
30.89091390266549
0.0
4.807038104372623
78.97171649063239
0.0
311.2487595741695
0.0
0.0
0.2801479995250702
154.0743348452088
0.0
4.6827048367704265
4184.377858853321
0.0
0.3860644340165891
94.16808091672283
0.0
0.5740318009629846
0.061677198857069016
4.558513476931694
159.54154676482904
0.0002187133977713529
13.75976838171482
0.5532684496320144
399.3890395821363
24.322542601501482
10.88888464262709
59.66680133342743
5.906000399667391
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall      F1        F2      F0.5  \
0  0.997324   0.466667  0.411765  0.4375  0.421687  0.454545   

   Average Precision  
0           0.193644  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.59347365 0.59622858 0.5936966  0.59249449 0.63096427 0.592173
 0.60627343 0.59785133 0.58926201 0.61102837 0.60131291 0.57394225
 0.61294262        nan 0.62636858 0.59442038 0.62787272 0.62201417
 0.60041061 0.60585211 0.59278527 0.58246095 0.62196528 0.61451233
 0.59164985 0.59095584 0.60552686 0.66100321 0.62184069 0.59304215
 0.56751075 0.57956313        nan 0.58588691 0.62104442        nan
 0.57659048 0.57982863        nan 0.57751338 0.59245137 0.57464096
 0.57850663 0.60797921 0.59295724 0.58232031 0.5936463  0.60437271
 0.56981251 0.61194125 0.58340704 0.61905837 0.60261627 0.61503474
        nan 0.62631233 0.61294939 0.60373605 0.61937715 0.61020477
 0.56126799 0.58408444 0.6174381         nan 0.58229881 0.61917178
 0.61374333 0.58602171 0.59122659 0.60593853 0.59016088        nan
 0.59036889 0.61176754 0.54052131 0.61731954 0.59245299 0.59825576
 0.58344619 0.59231372 0.60566113 0.63041842 0.59735365 0.61107243
 0.59753961 0.59283037 0.58991446 0.58337479 0.5729362  0.58209044
 0.57408348 0.59369248 0.60766318 0.55999262 0.58734842 0.60059646
 0.60177554        nan 0.5726987  0.59112688]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 875.777 seconds
Cross-validation score: 0.6610032124738008
Test score: 0.7865168539325842
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 900, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.3}
1914.6869121905765
4477.594729000237
17794.720341720837
451.64015952499176
8.024317660718225
15.057884993031621
3.0948884151875973
1.1933264804829378
36.45706010470167
0.6244570016860962
21108.074610266543
0.6809097691439092
1.1499582172837108
0.0
10.73757093079621
0.0
135.3868888099969
0.0
0.762342884670943
0.034885363886132836
0.27700638957321644
0.27521447648177855
121.75884484295966
211.59795765858144
103.31244876980782
14.02180004119873
0.0
2.670712814986473
0.0
705.9060329105705
838.6311876000836
18.438697518780828
20.063986809742346
20.098380116236513
47.65270026877988
0.0
0.0
0.0
5.574519411689835
3116.3045408018806
1.5089069393870886
10.15389378456166
0.8462822064466309
354.58516706389673
324.443898548343
22.72386817674851
8.99116414191667
0.0
0.0
0.0
226.5979995924863
93.59827736509033
95.4287209535978
81.5393430457334
8.48158609867096
0.05319710075855255
15.211315674125217
0.0
0.6369053357193479
1281.7165825579432
179.18266586167738
217.34796540404204
2.735396961623337
4.014549940824509
24.664293927140534
1.2219075495377183
76.56146489479579
41.271705120801926
5.2109540948295034
294.85741250315914
309.2636613858049
47.95737799676135
478.36229935307347
0.0
0.0
0.0
0.009259429760277271
11.89711440858082
150.74628759731422
109.1683315038681
8.45553939207457
3273.505046770937
373.8250109545188
4.252829565259162
104.55321343726246
0.3421264085918665
223.03331679929397
0.0374050997197628
5.144579887390137
15.544080257415771
38.88948806701228
0.11553442961303517
20.044741127989255
4815.233345299392
0.14686082147818524
0.08065977168735117
47.24532925772655
60.755430579185486
7.470566303178202
1904.588670032419
232.80163588542928
61.34511084132828
25.475910181034124
0.0
13.329565491294488
127.3409518605331
0.0
168.70461052071187
540.0507293742849
0.0
0.04206784098641947
501.2301099229953
0.0
63.18593810634047
1046.2785455150297
0.0
14.833855951346095
320.9656611762621
0.0
74.45072857575724
10.827535613556392
108.11033130210126
74.31150784267811
81.20688804192469
19.114803224802017
3.2485447810031474
1778.1365602786827
62.677529595879605
203.6481096575444
7.315588855592068
217.89207833143882
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998513   0.666667  0.823529  0.736842  0.786517  0.693069   

   Average Precision  
0           0.549466  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.6139781  0.61269372        nan 0.6007074  0.61753361 0.62741616
        nan 0.6072694  0.63445348 0.63923611 0.62283656 0.63438123
 0.66199752 0.63010765 0.65281597        nan 0.6252069  0.56348594
 0.64221895 0.64941546 0.61938318        nan 0.64467173 0.63403956
 0.63843201 0.6038793  0.58960062 0.62853096 0.61980311        nan
 0.66201086 0.58218853 0.64079573 0.60504967 0.63281148 0.59000928
 0.60701978 0.61536307        nan 0.64716429        nan 0.61603141
 0.62295986 0.63015383 0.61133631 0.6307452  0.6236933  0.62036258
 0.63413304 0.57691691 0.64782497 0.60461551 0.64488945        nan
 0.61597362 0.60840717 0.62222472 0.60885332 0.62844927 0.62672468
 0.62731756 0.61797187 0.60647209        nan        nan 0.61567462
 0.62011538 0.62244733 0.62996395 0.61685285        nan 0.61620834
 0.63354364 0.64861424 0.63221219 0.61378842 0.61486504 0.60407338
 0.60014784 0.63645673 0.62407886 0.59574315 0.61946218 0.62520921
 0.65561122 0.6112368  0.63597889 0.61641265 0.63011222 0.61584268
 0.58429843 0.59943827 0.62569428 0.64605846 0.60372997 0.59077018
 0.62900383        nan 0.63527087 0.65214468]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 844.419 seconds
Cross-validation score: 0.6620108588324907
Test score: 0.7303370786516854
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 100, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.3}
2065.6482716492505
2473.9412578639044
39142.89362377327
296.77070356298196
51.74516212365627
3.3088656023722036
35.316329660295985
3.6248321625898825
0.09141777741528756
40.62278453687759
6796.82095819805
33.85078142864937
18.987734549795277
0.0
43.46531596227667
0.0
263.69993910359506
0.00047825800720602274
15.48094444596859
0.1570292678417886
13.727170005131
2.5704378515491024
5.212099506132255
183.88136021219134
0.6264675212613309
7.802455943354289
5.231691668605756
6.3845828103701985
1.1417292551659368
1183.8302326767123
92.56436252264811
6.300132108328749
10.438382696061735
122.40363668460562
56.075014823682594
0.0
0.0
0.0
0.9698887523678933
2957.286069459649
20.9388529289915
6.885738016317741
5.602718253769126
27.713207854514792
25.004110226257005
6.890376751916847
9.057090890997047
0.0
0.0
0.0
55.434934383948445
108.9608254551254
69.99952271426345
1.0968132756061355
4.710386528332947
9.29915527574842
51.36281807255287
39.91948817385013
0.09822532301144576
543.0164440739024
2.4485533295257738
991.8642706647031
15.516620416957878
0.3562764005996115
53.28139106944161
2.7117028821396585
48.132240487221516
20.282258624610943
67.08278241832159
7.633706374464338
95.05511034697166
64.12335193526576
996.6472874164792
7.844355020910146e-06
0.8714295858129436
14.52709610396039
2.9193270835094154
13.48424543994564
108.79980523039637
166.0257341389285
11.794005764203801
2662.485891855861
579.0313985429104
22.022211814833536
38.70247892563286
433.8252361364496
1761.881225983996
104.51963574484739
0.009055565662606568
0.2189743402414024
24.017487686805786
8.344320176281728e-06
10.791178363082294
2104.8263047805626
9.690986838266568
0.029780733780505386
550.9677296293459
0.004230819991789758
26.79363445697354
509.56073817058484
15.174713842653656
2.6006810945621837
3.004117153222378
0.0
24.15802203396328
505.6070946563507
0.0
74.9069205275731
157.1003272620603
0.0
0.8406170022283912
463.8062650525658
0.0
208.314206810815
529.743168772103
0.0
40.96343083671236
595.0287180267555
0.0
17.28153903091587
3.392199277475863
52.593034186934034
22.894426472624286
0.17331700714085585
41.2803132425142
124.42037787721681
767.1644951095932
19.441902364370552
54.13652194992131
2.4013540820057506
152.5539346590208
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998216   0.619048  0.764706  0.684211  0.730337  0.643564   

   Average Precision  
0           0.473984  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.61035138 0.6499694  0.6416213  0.62124742 0.64263952 0.61536033
 0.59460494 0.64435193 0.63426511 0.59569803 0.59219775 0.61411646
 0.60939555 0.6174855         nan 0.63098562 0.61148305 0.6249478
 0.60768111        nan 0.65043666 0.60526534 0.61433834        nan
 0.62844396 0.62501266        nan 0.60988202 0.59258491 0.62226185
 0.62268732 0.63085591 0.62028251        nan 0.60565235 0.66336158
 0.62166851 0.60973791 0.63683866 0.62745465 0.59307413 0.63440678
 0.63406628 0.62106653 0.61431972 0.60625361 0.61598494 0.62877375
 0.59540244 0.62111331 0.62934277 0.62630328 0.60281978 0.63372575
        nan 0.61037797 0.60601432 0.63687538 0.64939496 0.6125075
 0.61131067 0.59692234 0.61632652 0.61281323 0.62828207 0.64986731
 0.61456625 0.61452054 0.62775099        nan 0.59453943        nan
 0.61346899 0.61143429 0.6378685  0.6305354  0.59873923 0.62033321
        nan 0.61883171 0.63566666 0.62275232        nan 0.62137825
 0.61706647 0.6115264  0.60298574 0.63712239 0.63217869 0.64806396
 0.63191522 0.62693039 0.63177679 0.59899804 0.63757896 0.6341896
 0.61696626 0.61699319 0.62276852 0.63914848]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 849.442 seconds
Cross-validation score: 0.6633615769489134
Test score: 0.7142857142857143
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 100, 'classifier__max_bin': 50, 'classifier__learning_rate': 0.4}
3318.3076675673947
1068.9475701348856
23840.594230391784
2114.4938564784825
36.96173846721649
0.7731362879276276
27.62350661866367
0.00995889026671648
0.0
0.6050300002098083
8032.340840297751
181.3744505699724
191.42533719539642
0.0
27.20275839162059
0.0
152.28701758384705
0.0
0.047837600111961365
0.09029960073530674
4.466779857873917
29.46274631493725
82.8640261106193
56.78604280948639
19.946815848350525
0.0
0.005965719930827618
53.06875622831285
0.03715920075774193
1314.7654319251888
5.126405677758157
1.1271715089678764
9.617127023404464
175.00290380441584
430.3872543964535
0.0
0.0
0.0
7.225088119506836
4205.335635701194
11.354573993012309
16.953414745628834
38.465732033364475
226.47248644800857
11.103643730282784
76.768106643809
0.9458093303255737
0.0
0.0
0.0
75.1085431361571
130.48929346399382
58.78391736000776
0.06645700335502625
10.830100059509277
0.04124109819531441
166.04293212248012
0.0
13.670900344848633
854.6134930616245
334.09156938456
1048.873940599151
4.1887640953063965
0.6560369729995728
0.845081003382802
0.005493490025401115
39.796495486050844
7.363150119781494
7.846575755625963
296.5076732477173
426.96910048788413
29.78117797523737
18.540248654782772
0.0
0.0
5.452247083187103
128.47273828275502
21.72959864512086
9.913774684537202
25.59549957420677
0.4717940092086792
1840.316555625759
755.8421961246058
1.170022392179817
366.07402184279636
0.2592246066778898
9.009207079187036
11.416596046183258
16.17839660309255
0.33759400248527527
3.5925233066082
0.0
86.18449939368293
1204.8247445910238
256.9170937538147
0.4869149923324585
1.581522210035473
0.0
63.18083869921975
761.5780524434522
23.601887106895447
2.3543881326913834
139.01364300306886
0.0
19.299040666315705
50.937084403820336
0.0
0.04982509836554527
1.7840465372428298
0.0
5.37012343108654
256.34470616374165
0.0
13.582179482560605
685.881371694617
0.0
0.09073290228843689
182.82387599931099
0.0
5.152576573193073
61.75616805255413
39.71385442279279
408.9561047363095
0.03639080002903938
38.35889148712158
8.277126088738441
272.2495578927919
141.40043694712222
25.57353650731966
0.7242240160703659
48.77908592671156
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998662       0.75  0.705882  0.727273  0.714286  0.740741   

   Average Precision  
0           0.530155  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.61601373 0.63507915 0.55213617 0.57132048 0.59027767
 0.60413936 0.6101075  0.60005628 0.60258113 0.56614614 0.61860188
 0.58663251 0.62616924 0.61846638 0.60723585 0.63981412 0.61997665
        nan 0.55572525 0.60806881        nan 0.59890678 0.57370201
 0.57432033 0.59381003 0.59551094 0.57813695 0.53325998 0.61860816
 0.61038589        nan 0.6108566  0.61331153 0.58502692 0.57608793
 0.60670165 0.55622784 0.6140257  0.62329121 0.57257582 0.57513939
 0.57764997 0.54504645        nan 0.58000432 0.62021715 0.53619916
 0.58771564 0.57596698 0.5936355  0.58601612 0.55789309        nan
        nan 0.61634272 0.60744103 0.60671163 0.63254342 0.62309409
 0.59321685 0.57506977 0.58502364 0.55709669 0.5896714  0.61168521
 0.61178192 0.60802751 0.58777934 0.6193793  0.62352573 0.58529309
 0.57696988 0.63433818 0.61748132 0.61186168 0.60485474 0.58743669
 0.58779399 0.59891118 0.60575993 0.60177787 0.61780494 0.61850327
        nan 0.61667013 0.59153784 0.58083861 0.60287738 0.5965953
 0.60335586 0.55361762 0.59503093 0.58851338 0.62490568 0.60970744
 0.57538021 0.59963819 0.58891527 0.57982393]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 862.663 seconds
Cross-validation score: 0.6398141236860277
Test score: 0.7954545454545455
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 800, 'classifier__max_bin': 200, 'classifier__learning_rate': 0.2}
7978.191399157047
3257.6424384638667
14470.896840438247
4027.0477956160903
50.570833802223206
0.0
27.181391775608063
0.0
0.0
6.708661451935768
35526.779768653214
157.41892063617706
20.4574975669384
0.0
16.610674791038036
0.0
0.3672730028629303
3.020699977874756
6.682229995727539
4.849321961402893
7.9875220358371735
15.870076835155487
0.9721110165119171
217.24340057373047
141.57986640930176
2.4878599643707275
0.0
0.26903900504112244
0.1685519963502884
854.0268424749374
34.582200050354004
1.01807801425457
10.389518111944199
295.11210983991623
333.84195939451456
0.0
0.0
0.0
12.21734543144703
2774.1249912679195
205.5455026626587
1.0417580008506775
98.37888038158417
1624.135680615902
244.85441318154335
18.069799423217773
630.692360162735
0.0
0.0
0.0
174.73617736250162
86.20374450832605
103.18118965625763
0.0
11.702270269393921
1.1492619812488556
144.26966166496277
1.827632024884224
0.27490898966789246
2426.069792494178
2369.031999118626
1138.126634374261
128.0564820766449
0.0
0.18278299272060394
7.466930985450745
0.19463400542736053
16.203055188059807
85.05752289295197
522.896361514926
408.0155151113868
7.0887298583984375
592.9133917242289
0.0
11.397600173950195
0.0
0.0
28.67764613777399
908.8214610517025
48.0432006791234
42.19631265848875
1242.5057584643364
276.5471877902746
789.5027635991573
1388.1837967783213
3.0434799194335938
162.1741155385971
4.828399896621704
0.0
0.0
0.0
12.041500091552734
12.546960681676865
4917.227057360113
21.75473453849554
2.7578189969062805
22.42259891331196
0.0
63.40785115212202
2738.470965489745
260.30504700541496
14.136287212371826
37.11712473630905
0.0
51.64687778055668
238.68454313278198
0.0
417.4922070130706
4.791107960045338
0.0
0.6173250079154968
475.5353174209595
0.0
171.61608751118183
2037.1115705221891
0.0
10.15999984741211
1563.7423833310604
0.0
35.429259300231934
20.68902015686035
34.36848521232605
229.45899541676044
0.0
4.691051155328751
16.205989599227905
2003.6739382743835
142.3836469054222
729.5600697696209
0.6149939894676208
437.4546140730381
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998662        0.7  0.823529  0.756757  0.795455  0.721649   

   Average Precision  
0           0.576917  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.62750762 0.62695552 0.64465649 0.59475426 0.66128405 0.6385067
 0.65092723 0.60776326 0.6473362  0.65929431 0.65504562 0.67736484
 0.63989598        nan 0.65102759 0.62501347 0.63988145 0.65992374
 0.63093959 0.61187931 0.62059152 0.6055623  0.61231992 0.654792
 0.65865945 0.61656724 0.60855713 0.67475816 0.64132587 0.64097954
 0.64051477 0.63177874 0.60988402 0.61399898 0.60400864 0.6445135
 0.64842052 0.6540748  0.62023116 0.67878151 0.65888683 0.6566647
 0.62350381 0.60909091 0.64513671        nan 0.63579976 0.6171903
 0.65201439 0.66796508 0.67161673 0.67121791 0.62782835 0.67655055
 0.63581236 0.60490723 0.65404969 0.64050415 0.63507409 0.60386125
 0.62115065 0.62500471 0.6858624  0.61909694 0.61959983 0.65140073
 0.63179752 0.6539534  0.62906072 0.65162989 0.62768801 0.62533153
        nan 0.65138117 0.61221486 0.61371724 0.61709197 0.61351269
 0.62913369        nan 0.59581129 0.63104189 0.60653546        nan
 0.6057682  0.66647642 0.61750291 0.65335537 0.69107686 0.63452379
 0.60532752 0.64164899 0.6466106  0.56207571 0.63086365 0.63910486
 0.70473976 0.65139149 0.60844967        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 901.275 seconds
Cross-validation score: 0.7047397574487357
Test score: 0.625
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': -1, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.1}
18107.32679763064
7050.296537870225
24378.09891143395
1380.2658645792508
15.271063311241733
18.047798582105216
15.862183003784594
3.2494099140167236
25.86434181034565
4.157259941101074
65961.76144304131
52.25284066790712
16.516560316085815
0.0
152.32469812385898
0.0
340.62212942291853
0.6418970227241516
4.963352730129344
5.253370129675318
43.40345357998922
10.87398146013129
0.7947874098702314
516.6559782693269
108.40010154247284
16.88596953541736
3.76568182142114
7.303095331970553
2.1943498145087688
1861.8103908713902
307.372017700317
16.517280435817767
227.54272702594244
109.0960760476836
434.33693453515025
0.0
0.0
0.0
167.29341089339232
5393.508674637956
110.55232813168277
17.236938042362567
163.77036607429
2117.950810792634
205.91671781783066
37.76943516053143
101.07715272685618
0.0
0.0
0.0
95.49584171020447
166.37749101118948
107.02419420803301
33.04886033217902
4.275219683737305
44.137070236923705
1688.839962584534
6.220096961632976
28.998537455931
3198.775995686278
1907.2632631781785
2166.583574647581
2.228722158819437
3.6250001192092896
38.55540017583917
29.941758166990798
237.2168807077341
64.42849494525217
0.020507579031686873
2603.790463989023
889.1926905709406
343.9905966386841
600.6335935131156
3.7802271246910095
3.0301268998427986
107.65203249454498
0.0036918839905411005
13.112912499469985
1298.9478910312014
6.0906980441686756
3.9865399822508607
9660.941193799385
1143.8803101679518
738.8757597903293
5256.369659474194
26.352345921242886
75.840265634969
75.58398939256585
7.116450769544002
0.520669334739523
52.972394271432734
12.581536133575355
198.83643218044017
7674.428812326257
0.2111911615356803
1.7503535067278904
153.77642061851796
5.297128170052133
1144.1128403233936
4855.955087437305
223.5260512889363
130.10832434296026
383.3136807468836
0.0
54.54220243430291
786.0739012876248
0.0
226.43578079700498
46.25511599484503
0.0
155.13580556171746
3669.9591728437226
0.0
291.6847456259233
1963.096530416513
0.0
178.01598956503048
416.3550512210227
0.0
140.17389222690872
21.61206034936522
24.362846275776832
220.3961590142976
10.402062676981586
40.3973740111544
27.909521049674368
6294.350085002463
282.96559711573633
1689.1656338436699
23.354984893312576
1627.1159439814705
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1     F2     F0.5  Average Precision
0   0.99777       0.55  0.647059  0.594595  0.625  0.56701           0.356774

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.64610599 0.64709086 0.63603624 0.6189278  0.62895948 0.68610435
 0.6383305  0.64223498 0.6097533  0.64520181 0.62468499 0.65833918
 0.62938756 0.61598027 0.63308019 0.65250634 0.63958349 0.63656034
 0.61270533 0.65968245        nan 0.66459004 0.62418612 0.66518709
 0.6489231  0.6493716  0.63181891 0.62013409 0.63315327 0.65250914
 0.64436979 0.646526   0.62352238 0.63227714 0.65989678 0.63958529
 0.67239957 0.65824482 0.62926744        nan 0.67050601 0.61657223
 0.63258185 0.64337026 0.63850614 0.63596797 0.65650052 0.63551949
 0.64808282 0.6476453  0.67122914 0.6217977  0.63555275 0.62642191
 0.6543611  0.61899936 0.64973076        nan 0.61367323 0.60543231
 0.6231238  0.64195932 0.65498068        nan 0.65469225 0.63145973
 0.65135604 0.64668563        nan 0.64625405 0.65992063 0.61111627
 0.62456234 0.60049718 0.66543173 0.65650634 0.64475058 0.6799115
 0.64195559        nan 0.62642881 0.663227   0.6524419  0.66180997
        nan 0.63663728 0.63561868 0.65870219 0.63635934 0.6514078
 0.63348462 0.64353992 0.65791537 0.63939651        nan 0.67248483
 0.65904293        nan 0.62567329        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 902.970 seconds
Cross-validation score: 0.6861043452058517
Test score: 0.7317073170731708
Best Hyperparameters: {'classifier__num_leaves': 26, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': 200, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.4}
3472.12466358074
5088.976638043794
7192.239663576085
830.3900957787373
5.214432092296192
3.818378247480723
7.9693960003132815
0.004929369781166315
19.526663485972676
1.454200029373169
23543.090430441
0.0006166460225358605
38.30654001235962
0.0
9.3205757746764
0.0
18.09981388610322
0.008532157575245947
0.07332242885604501
1.793239951133728
1.6326270513236523
3.438704546773806
75.67976303629075
72.9823294878006
11.795416278298944
0.0
0.001185150002129376
7.158840179443359
0.0
231.4997068331577
0.9794337470084429
0.1828904957510531
5.754563932743622
121.58039502356405
99.62270782697306
0.0
0.0
0.0
4.41411050100578
1496.179935022883
65.09346041391291
33.6681639651506
70.51173282041782
597.8551826014591
2.2411658187884314
0.05713467150053475
7.129183444310911
0.0
0.0
0.0
41.70792679988517
4.34504659272352
116.12352671516783
19.620835798443295
10.294399216247257
0.27057128958404064
72.32580581119328
0.5274938838556409
6.7891938925822615
1340.7631449964829
47.86343587411102
227.17994777698914
13.689052850677399
0.04364282179813017
9.420379978546407
0.13851096492726356
5.8933599330812285
2.3606529068929376
3.293302276171744
400.9889607780533
127.45073743766625
25.587769508361816
302.16710683774727
0.0
0.6654401766136289
0.0
1.3433975540101528
1.2546212886663852
301.27265743410317
26.19423396041384
3.9561383188411128
1984.586625485099
766.6361508847149
38.16087787860306
5.370182184269652
1.136906759988051
397.9124223077233
1.8979679213371128
2.238279299112037
0.0
14.154117452835635
0.0
10.334784487262368
2833.925188170404
25.064839883456443
0.5925580263137817
41.8204872743263
0.0
26.642690684657282
547.5642190876679
67.24368755782052
4.186767209612299
2.61951324299298
0.0
28.46403313355404
105.97683485581729
0.0
158.67460225895047
80.72845983631123
0.0
23.406405575573444
336.7259920237957
0.0
31.6185435364132
1866.3261928821084
0.0
46.39241789697553
243.53492630971596
0.0
0.9575439695618115
7.0845421105623245
33.62324318395986
6.94374504640291
5.650416171178222
0.162242021295242
1.7941411891952157
543.011365373095
90.44257055476191
32.97867927534389
537.6193890273571
24.381534256477607
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998959   0.857143  0.705882  0.774194  0.731707  0.821918   

   Average Precision  
0           0.605785  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\metrics\_plot\precision_recall_curve.py:125: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.62493958 0.66386261 0.60739231 0.61861662 0.63821261 0.67375056
 0.64040147 0.63300931 0.6345906  0.62571539        nan 0.57996985
 0.5888938  0.61662972 0.62259186 0.60945928 0.64378273 0.61152207
 0.60274189        nan 0.63502832        nan 0.59118641 0.63240495
 0.63383174 0.6230247  0.6301962  0.63212446 0.62683594 0.6159149
 0.60922933 0.62381428 0.61506329 0.6370535  0.63174985 0.61289902
 0.62579555 0.62500117 0.6210047  0.61850688 0.61348736        nan
 0.58833001 0.62448138 0.62587021 0.65660319 0.62682224 0.66258837
 0.62465142 0.63794334 0.61532234 0.62788126        nan 0.64298499
 0.63246738 0.63176239 0.6194853  0.60939545 0.62206201 0.58016479
        nan 0.60617001 0.63536114 0.58640436 0.6182956  0.62481872
 0.6254688  0.65884626 0.61368034        nan 0.66305157        nan
 0.64645854 0.62087596 0.61584139 0.6227655  0.63052513 0.66069104
 0.61763791 0.64459586 0.63374757 0.61019173 0.61737216 0.64504605
 0.63627898 0.64138443 0.61785405 0.61761825        nan 0.6061401
 0.63289632 0.59178785 0.63098363 0.60904217 0.60060443 0.63161186
 0.64869878 0.60351158 0.66369665 0.61070325]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 856.547 seconds
Cross-validation score: 0.6737505643755644
Test score: 0.6896551724137931
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 400, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.4}
5457.343302166148
3158.2063709926733
7076.1673390479355
775.9012148857582
45.07424195672502
9.376171133219032
5.173157347366214
0.19980058167129755
12.711921794572845
0.0
19987.80589167695
3.6559151597321033
2.975024089217186
0.0
30.016289538121782
0.0
115.09443610208109
0.0
1.112362202256918
26.87623686520965
0.4941129069775343
7.068681710312376
2.9179354747757316
807.542070388794
0.0064049900975078344
292.3690028190613
1.0017817839980125
0.42976900935173035
0.06300540268421173
1122.0924551344651
12.322511428596044
103.85738506168127
21.295308909378946
189.4100686639431
121.28802994187572
0.0
0.0
0.0
10.067360910063144
1554.00490081124
12.013855286408216
48.195778385037556
0.020703774542198516
471.9281605296128
39.40699415744166
1354.7138222370995
13.205293226375943
0.0
0.0
0.0
65.62016753673379
63.666006269922946
52.18710458910209
3.9133534152060747
138.99923460994614
3.804500102996826
707.856407886371
0.14434916168102063
6.776880380974035
451.8893061320996
99.99857957218774
199.78340430271055
0.4778948425228009
19.17620086669922
6.7966435216367245
12.387048613512889
0.6710277054226026
22.16864805840305
4.755034603207605
321.0975431298284
215.02476812532404
41.086822104407474
678.6731744772405
0.0
0.44982433022232726
3.046410083770752
51.40014100819826
14.98995771058253
308.54297590843635
32.5113394446671
1.0646100984304212
1742.5471868634922
25.181510912429076
203.13978308811784
299.88201121019665
0.00724398996680975
210.4952009061817
0.0
46.44919230273808
0.0
33.81683052028529
0.03280939906835556
68.20024860312697
1516.0989213355497
0.9379600016400218
0.0
68.77357033343287
0.0
89.03094326353312
1062.7043988645019
432.1579531998141
112.45164122927235
9.520472244301345
0.0
27.68210814444319
132.4327232753567
0.0
43.46484037628397
1.7194000482559204
0.0
6.047124285250902
365.9719567579741
0.0
53.83396267789067
3570.7644805587915
0.0
55.97767483157804
69.95010488017579
0.0
43.938113493379205
17.4377875354985
26.936142506310716
15.149498248589225
0.0
7.981265906244516
5.44951204280369
298.2771903287503
8.268731312011369
106.21662293409463
0.19834369549062103
62.70467064593686
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998216   0.631579  0.705882  0.666667  0.689655  0.645161   

   Average Precision  
0           0.446564  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.60863794 0.56964147        nan 0.59704583 0.56152075 0.63266172
 0.61729214 0.57077519 0.6203997  0.61519232 0.59658389 0.57271162
 0.58250524 0.5390995  0.56352268 0.57952759 0.63736461        nan
 0.6316604  0.58587657 0.6241586  0.56032536 0.61705762 0.56027648
 0.60892321        nan 0.59334858 0.53150835 0.59498569 0.57913737
 0.60223221 0.60433311 0.58470161 0.5568855  0.6218654  0.61241048
        nan 0.56604846 0.60120909 0.61775643        nan        nan
 0.61681109 0.59744969 0.52602653 0.61820476 0.62535734 0.60545723
 0.56869447 0.59333126        nan 0.63518066 0.60700814 0.59228625
 0.58606141 0.54663564 0.56298454 0.6025207  0.64090967 0.59241461
 0.5641398  0.5999313  0.57682237 0.62401251 0.51442374 0.62263584
 0.61970382 0.60470867        nan 0.57434375 0.59374724 0.61308349
 0.59280797 0.59002122 0.60662134        nan        nan 0.58949485
        nan 0.56200231 0.54931452 0.59775841        nan 0.59302112
        nan 0.58406202 0.59942012 0.58904472        nan 0.63539314
 0.57737241 0.62639211 0.60354026 0.60839226 0.57880614        nan
 0.63352718 0.56121344 0.56331549 0.59635554]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 820.300 seconds
Cross-validation score: 0.6409096679607516
Test score: 0.7386363636363636
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 600, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.5}
2951.522863451988
1752.967046555772
9962.057497622809
588.8256704786036
1.1886224509216845
56.58730353182182
43.19245476904325
0.02102229930460453
0.7406988910224754
0.5942950248718262
16646.33290101304
2.339716943766689
4.147454662015662
0.0
14.384544752538204
0.0
64.42874784744345
0.0007463879883289337
11.665104180225171
3.165879562497139
0.0
11.261617760930676
0.0034886898938566446
107.74941017478704
20.491893040016294
2.190084026195109
5.656001043273136
0.0022252399940043688
1.213095283077564
438.86424548365176
8.601926293689758
5.259158842498437
10.206714815751184
239.9634418137357
5.14809980042628
0.0
0.0
0.0
264.70542588981334
922.9110188814811
0.5344260438287165
69.06354982993798
2.7826041110383812
556.0415488059225
404.9339904785156
0.004824115982046351
11.365315065952018
0.0
0.0
0.0
23.15110639957129
83.94739299602224
39.07687904802151
0.8060553802642971
0.0
2.667463359888643
304.17111268866574
0.06286629941314459
10.40393091738224
1529.246327838191
545.9610009591124
400.6771784766461
7.382667804136872
0.0
87.74716566875577
0.14948657294735312
28.317353426013142
62.010101318359375
56.40420150756836
55.694071705423994
311.9506805233541
0.0
191.77860484446865
0.0
0.0
0.012775000184774399
0.024181300774216652
0.0
15.635949835181236
0.0
0.9553317615645938
654.3894342325802
188.664691171085
1953.949512720108
202.98932229704224
65.40262049017474
11.15444857091643
0.0011077189701609313
0.00995252002030611
0.0
0.32686490495689213
0.0
5.575751368276542
2477.4265470814717
0.0004016229941044003
0.016460899263620377
3.40796674581361
0.0
60.391714750032406
166.95590193453245
30.04538081062492
424.739990234375
51.884475405327976
0.0
5.099450477806386
45.85064256563783
0.0
6.99793004989624
3.803424794401508
0.0
0.0
71.56241989659611
0.0
169.43739643105073
299.44084914249834
0.0
1.5329727689968422
321.5137066525058
0.0
0.0
66.22376066446304
51.199741260265
727.3800057867775
0.0
45.70274555683136
0.08191119879484177
1243.301680635661
5.727140134666115
146.04511043988168
0.38580096735677216
0.23493413280812092
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998365       0.65  0.764706  0.702703  0.738636  0.670103   

   Average Precision  
0           0.497654  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68518763 0.65400036 0.68016371 0.65615303 0.68088829 0.67864033
 0.6766691         nan 0.67525465        nan 0.66525055 0.67246047
 0.67069812 0.6337288  0.65162363 0.67874055 0.67297947 0.69604817
 0.66934578 0.66757009 0.63980853 0.63030134 0.64811129 0.65308188
 0.66190441 0.65092981 0.6473735  0.66465969 0.70035284 0.66502813
 0.70089572 0.66950729 0.68249535 0.65549828 0.66532565 0.63177617
        nan 0.64851955 0.65081583 0.66130675 0.67301523 0.682806
 0.66069848 0.66689107 0.6852664  0.66677617 0.65784939 0.67950873
 0.68996382 0.68838384 0.65607107 0.66796339 0.672383          nan
 0.64740426 0.62700199 0.68630329 0.65263435 0.69135014 0.69534471
        nan 0.64686545 0.6488323  0.67999811 0.65974589 0.64282559
 0.66361841 0.68162091 0.65675276 0.66406358 0.64160814 0.67348302
 0.64690913 0.66135456 0.67416381 0.70763405 0.64928806 0.64908646
 0.67770651 0.66597184 0.67413448 0.64312489 0.67512828 0.66174623
 0.64694145 0.67033189 0.6542494  0.68383023 0.69725997 0.67781169
 0.66112617 0.66505226        nan 0.65386189 0.5946341  0.65325461
 0.66575932 0.69242822 0.64319963 0.68055018]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 890.303 seconds
Cross-validation score: 0.7076340500489107
Test score: 0.632183908045977
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 300, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.1}
6196.437051194846
10810.659758036156
71856.14088786511
7200.032016114005
13.479961020039468
50.92518157107064
45.10083847532928
0.0
18.41594203014847
0.0021908499766141176
36163.37922551766
36.00325674685871
42.43988034315407
0.0
21.710771505535263
0.0
16.05414995605679
9.960693755268949
197.00266179140672
12.071499834305996
1.9787403737435199
13.41696827847045
188.9608219456486
184.76123391666988
347.3776027993299
6.637195652027003
10.288119546077723
12.197484214302676
103.37200012383983
1112.9332505848047
80.31446073241392
15.847601417251553
41.13132892349813
100.9843847210177
316.01081377875846
0.0
0.0
0.0
193.57248928161607
5992.693462853145
42.73648365977252
159.84988847108548
171.86713237080096
6243.903884559879
141.63290079947183
11.986482946653268
411.0823217554046
0.0
0.0
0.0
53.56075963659896
64.33482435747987
189.31687603728824
12.759570400172379
1.2125620429537776
3.165952249742986
461.36733291920063
0.7868236492959113
44.53054105525227
1926.5305561151515
7263.620924638786
523.4598149699523
48.51676807867625
2.8295716755092144
7.179735766480007
0.098303932002068
34.74123895311459
2.8464188101662558
2.272167864955918
1639.150958021153
223.4550943480308
98.21060577305434
1759.137781404007
0.03437024978302361
3.624995611149643
19.741941854674963
9.244144538479517
20.63918570808164
1084.6697434042676
17.211408568997285
9.013599789782802
1427.6755089591586
1461.1291237573655
1692.2048773813913
5495.089102076143
76.19971335423179
1829.823738401851
269.39215257939213
48.31879480135103
1.5069519724102065
87.70336740531434
27.106410443381037
401.38582764277817
5983.62291267078
255.57079973989323
30.284413730691085
174.388460222606
0.7669224344317627
112.91713074223122
3073.351148026436
50.18844716196463
58.022039105423914
151.56564203836007
0.0
28.26283978195559
215.90623174352754
0.0
44.97046119763036
698.3787946078508
0.0
1.864716849081816
1356.7578842238772
0.0
367.55462385957617
1997.1123426543666
0.0
46.10388309229165
693.2396818873531
0.0
59.56176993355621
104.37022094068234
38.22472241925789
655.8790923822344
8.787616693181917
94.02871696661896
94.13017010211479
1384.6913041669575
410.87277302953055
33.94508125461397
45.39148595235429
259.69318879630373
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997919   0.578947  0.647059  0.611111  0.632184  0.591398   

   Average Precision  
0           0.375505  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.66170988 0.68790047 0.66074852 0.68784058 0.67846091 0.68692772
 0.69184823 0.6907312  0.68334018 0.69035661 0.66548173 0.70702212
 0.62764082 0.72489867 0.71832928 0.6690042  0.64349221 0.63356258
 0.63349499 0.68475992 0.67316526 0.67038574 0.71243878 0.70969834
        nan 0.62771463        nan 0.66797309 0.67738739 0.65417513
 0.64650201 0.63568764 0.65394851 0.67677804 0.652853   0.63251188
 0.6619289  0.69121081 0.66325446 0.68007479 0.6840303  0.67967858
        nan 0.68946461 0.71023767 0.65197242 0.66875057 0.70391214
        nan        nan 0.66377597 0.71546077 0.68872663 0.63207374
 0.66813381        nan 0.66508043 0.64323676 0.71208793 0.66347885
 0.66867555 0.66716915 0.63399448 0.65350177 0.71337252 0.68851092
 0.65319766 0.70524182 0.68600726 0.68817631 0.66948758 0.70248032
 0.66266927 0.70116972 0.69597416 0.6660141  0.67587824 0.6974234
 0.69163503 0.66625208 0.67008129 0.66860889 0.69367793 0.68892178
 0.69603778 0.66859212 0.67911704 0.68079049 0.64787024 0.69851159
        nan 0.68230761 0.68284009 0.70547852 0.64934848        nan
 0.68942404        nan 0.64535407 0.67425307]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 903.796 seconds
Cross-validation score: 0.7248986749949522
Test score: 0.6470588235294119
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 500, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.1}
10997.768745349487
3796.324318913743
75838.26969552733
12478.838454522192
45.75091131031513
15.518985092639923
42.95351028442383
0.0
8.751978099346161
1.9559400081634521
33839.41156703187
46.18353492021561
42.01572246477008
0.0
33.73785984516144
0.0
6.313098967075348
11.125199913978577
50.37337923049927
10.298866927623749
3.6887310445308685
13.462999820709229
12.09693906456232
157.30944204330444
110.40252029895782
6.690153151750565
7.91366982460022
1705.6577787101269
149.3524285554886
1530.4339201301336
62.68311765789986
84.6158390045166
49.888875871896744
24.132965445518494
385.7892777621746
0.0
0.0
0.0
137.89164867252111
5088.815858885646
171.98586705885828
564.8672753274441
161.92954111099243
588.8932251930237
66.2917544406373
449.6037485897541
28.5369689501822
0.0
0.0
0.0
91.11003391444683
143.0384620130062
207.9260377883911
0.8025299906730652
2.3095099925994873
8.325809061527252
993.2084841430187
93.00842223316431
46.926812052726746
2135.7833676040173
996.7209466919303
579.7949802577496
5.563077285885811
6.203564427793026
203.84363889694214
0.9679279923439026
173.37223097682
0.513155996799469
5.143975019454956
903.0735604465008
577.2472987174988
85.59236895292997
368.8829976916313
0.0
4.4781999588012695
30.181559085845947
65.02619016170502
18.217649817466736
3752.648523747921
105.30439525842667
68.4118583202362
8227.646520763636
2246.416353367269
122.77672077715397
144.01829880475998
817.7203843295574
260.26677414774895
15.649610042572021
5.1100099086761475
0.7186709940433502
118.73959206044674
0.0
1130.6845473647118
4731.604095757008
2.0771599411964417
6.269520998001099
157.29792633280158
0.3827210068702698
95.36013408564031
1257.96143970103
113.96802580356598
199.64077425003052
339.73382127285004
0.0
41.753050446510315
361.73796650371514
0.0
198.94041430950165
908.2314258813858
0.0
24.53230094909668
338.4587100446224
0.0
550.3234023153782
4251.896595656872
0.0
145.56019532680511
1033.2017948031425
0.0
64.07294967770576
33.781503558158875
83.99821099638939
336.72535151219927
31.317989349365234
8.822273880243301
66.76500889658928
3308.8848071694374
698.754567347467
141.3706603050232
720.5398502349854
1328.4986898899078
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998216   0.647059  0.647059  0.647059  0.647059  0.647059   

   Average Precision  
0           0.419577  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.64034991 0.63871734 0.60345499 0.54522207 0.64196127        nan
 0.64992706 0.5602608  0.56537768 0.56761048 0.59258397 0.62250359
 0.57118154 0.65163246 0.64573832 0.6310538  0.635628   0.6334386
 0.61337315 0.62381244 0.59430075        nan 0.56697565 0.64783923
        nan 0.60128225 0.59350549 0.58211578 0.66198767 0.64326478
 0.62432838 0.60842962 0.58137636 0.64426328 0.58871894 0.66869072
 0.60165733 0.60582599 0.57792895 0.61462232        nan        nan
 0.63892471 0.59468101 0.59867408 0.57916832 0.58394948 0.63544747
 0.61128072 0.61117841 0.58348661 0.63819485 0.65596792 0.63230913
 0.605938   0.60167703 0.61397235 0.62406486 0.60315854        nan
        nan        nan 0.63555993 0.60378709        nan 0.67228296
 0.62586506 0.64612286 0.60164763        nan        nan 0.63127878
 0.63884213 0.64242935 0.6063565  0.60415368 0.59078659 0.59268026
 0.57169396 0.61733036 0.65333743 0.59751012 0.64588    0.64114695
 0.63710376 0.59553625 0.61063677 0.63342059 0.6262965  0.66269556
 0.65495047 0.61631353 0.60157754 0.6577966  0.62964001 0.64645729
 0.61284115 0.59700228 0.6146652  0.60690769]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 863.045 seconds
Cross-validation score: 0.6722829581188714
Test score: 0.7303370786516854
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 800, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.1}
21933.18032464175
6118.095348965656
36188.678556617044
5073.8611093080945
33.47126788181993
11.502206678724633
43.12723004817963
0.0
12.236200332641602
1.0797100067138672
61598.47388151195
52.707165547477416
13.19320564539521
0.0
62.17945051438795
0.0
127.55655348712571
0.33547387446742505
17.83511463438117
23.234765391417056
57.14164208346904
172.92002442502417
79.32452980056405
325.8143362250048
17.770299069241446
0.0024197255480089552
0.015452751133125275
90.21325943213867
4.173675800033379
1959.5594669829516
1202.3671263119322
0.08759958839800674
532.3947388709648
653.4872338916175
137.62695406542844
0.0
0.0
0.0
419.22119026574364
9944.01351930594
427.473107308939
698.5349745258864
270.4648323094535
2948.759426039635
77.9367847454414
1412.9342716106025
344.21954715817236
0.0
0.0
0.0
79.47296654824095
245.34025558072608
220.49751214611902
13.241433354094625
6.403567548841238
4.591019990431846
636.760550193314
4.7278545137814945
161.32014974101912
3719.3340427332255
2571.1009304076642
554.8415730597661
36.096687783894595
1.9371985989964742
2.9140043081964535
6.821997854625806
116.33636923189624
14.911650563124567
223.85394930839539
1536.7035019877515
1482.6357564726463
3.5227947679700264
886.4350992531308
12.35551354465133
8.751408635493135
4.007589936256409
63.4753999710083
102.31468321190914
96.94076474918984
148.8269219382511
88.80612233758438
2262.0494846610354
2518.149323321345
1472.6232210788694
2559.9823635853677
124.52901712982566
413.98765684007753
28.607897967100143
0.5118004283540358
2.6501821831426398
21.93079090899863
0.08670042319863569
254.5918497128041
5813.305060528015
18.688815294764936
4.103995603087242
5.020365424546981
1.3370621717513131
112.33859011510503
4266.608141598259
196.62716916603677
15.820106280525579
81.2082139880783
0.0
42.69650416813738
86.69075919291299
0.0
24.68285126677074
0.0
0.0
75.74364442529895
705.9097385546047
0.0
500.43275803275264
2081.3127050565777
0.0
111.01231218591596
748.8699722766726
0.0
7.177669156693833
38.569379706597374
46.54182434487075
1991.9449558624474
4.784946084022522
1.7289077659679606
49.079441606223554
1572.2409388390167
360.6740045236875
227.32366643488058
35.47211563775065
66.20582173087524
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998216   0.619048  0.764706  0.684211  0.730337  0.643564   

   Average Precision  
0           0.473984  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68029839 0.65737547 0.6605266         nan 0.66760036 0.6844541
 0.67403942 0.65587497        nan 0.64349321 0.66604002 0.68029152
 0.67120237        nan 0.65709863 0.6599485  0.70702673 0.68271463
 0.70199908        nan 0.68007912 0.68435728 0.64304324 0.70138813
 0.67730309 0.66273265 0.64053822 0.68593704 0.6629126  0.66500771
 0.67026819 0.64108261        nan 0.65026837 0.66225754 0.64666999
 0.69001609        nan 0.68342529 0.65172134 0.66941189        nan
 0.63882622 0.66686631 0.64945162 0.6461622  0.67335949 0.68657305
 0.64608089 0.64180722 0.6717437  0.67190391 0.70984049 0.69324755
 0.66205799 0.69323672 0.68483964        nan 0.67717782 0.68825111
 0.6349831  0.6879529  0.65438019 0.68144335 0.68177957 0.67124589
 0.6398233  0.6470608  0.66182871 0.6650391  0.70988481 0.65573126
 0.69384909 0.68921858 0.6682192  0.67775615 0.6670576  0.69929674
 0.67592975 0.66417623 0.63302608 0.68774386 0.66629455 0.65043279
 0.66582434 0.67113825 0.68027598 0.6694947  0.67581864 0.66640857
 0.67918878 0.65202605 0.67369782 0.6950602  0.70324772 0.66576817
 0.68723542 0.65396779 0.6899482  0.69100904]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 853.592 seconds
Cross-validation score: 0.7098848119358955
Test score: 0.44871794871794873
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 600, 'classifier__max_bin': 200, 'classifier__learning_rate': 0.2}
6243.943410409614
1533.8733778484166
52444.36292076111
399.7910064458847
12.9093649238348
0.022854400798678398
0.037541601806879044
17.0964413061738
43.46645885705948
3.179149854928255
11653.89085817337
0.5054791122674942
34.65600047074258
0.0
15.499453499913216
0.0
11.300070226192474
21.705344818532467
0.0
0.7958850264549255
48.63904222659767
0.756610170006752
24.062116503715515
151.54813209176064
0.0
0.03886429965496063
0.8583400249481201
2.0046239644289017
0.03261759877204895
598.8730675987899
44.46208316460252
5.175149992108345
8.3873807862401
332.1058381535113
286.4234225191176
0.0
0.0
0.0
13.178527668118477
2822.5335269887
62.847690142691135
19.736811000853777
3.9421286433935165
818.3580733425915
21.994269531220198
8.258839964866638
61.78412929922342
0.0
0.0
0.0
34.12205348908901
74.66691061854362
343.57485014759004
8.832742676138878
1.1105232052505016
2.1764080971479416
538.276274472475
0.4892140105366707
9.425561599433422
471.4401205703616
61.957381546497345
170.8839756771922
18.52289528399706
4.632830172777176
2.8556882850825787
26.051946982741356
220.19215554744005
113.15842294692993
27.60704356431961
193.57652869820595
807.8188263736665
186.44402170181274
833.8824525978416
0.0
3.6671830266714096
7.346045091748238
25.357640940696
8.628381257876754
540.339691977948
3.7847766652703285
27.27451804280281
7184.444597251713
404.39304073154926
456.1010745614767
1098.0130815207958
33.787699952721596
2226.5805471539497
126.24406583607197
44.97335776127875
0.0
83.54699590802193
0.3655933067202568
38.43830684199929
2868.555784197524
1.7041084878146648
2.2096007391810417
14.4674262534827
0.11361700296401978
73.54640294797719
857.7396512925625
69.92156866937876
54.19708734937012
194.12501260265708
0.0
11.72379133850336
163.3723318427801
0.0
60.254278026521206
25.562242038547993
0.0
8.180174112319946
963.763484723866
0.0
139.7639077566564
447.1556767821312
0.0
7.520970940589905
195.86429032683372
0.0
35.85892963409424
79.42245074734092
18.503139147534966
248.82400140725076
0.35084599256515503
63.95349521934986
1.4969060570001602
1114.5624420717359
42.403924237936735
643.4968000948429
193.5338172763586
105.22965078055859
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998067        0.7  0.411765  0.518519  0.448718  0.614035   

   Average Precision  
0           0.289722  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.60099529 0.63744728 0.6204533  0.62185456 0.5912528         nan
 0.59730254 0.60947993 0.56422063 0.61419315 0.63994715 0.6090978
 0.58772227 0.58914033 0.60911609 0.59627388 0.58293767 0.59389058
 0.6168006  0.62658027 0.6512367  0.57801744 0.60999658 0.61396252
 0.60094579 0.57443623 0.62476008        nan 0.64666589 0.64223498
 0.57362216        nan 0.61350717 0.61278129 0.58726765 0.61889736
 0.60643977        nan 0.61287052 0.55872565 0.63189419 0.61336486
 0.58705299 0.63802947 0.60316592 0.61974468 0.60273852 0.5824847
 0.60416736 0.64956805 0.64088298        nan 0.60694082 0.60723694
 0.61866144 0.62794927 0.62163421 0.60329585 0.62197996 0.59977028
 0.59000304 0.62683415 0.64765133 0.61896123 0.63201271 0.61397989
 0.58917648 0.61986255 0.60656988 0.65017892 0.59797183        nan
        nan 0.61144961 0.61219513 0.60119579 0.60722527 0.57764614
 0.61285298 0.6082313  0.60997328 0.58557803 0.61853148 0.61229465
 0.60582269 0.62250737 0.59467589 0.61405422 0.61921459 0.60442274
 0.6334885  0.63772523 0.59962311 0.60088466 0.58416915 0.59057248
 0.63053767 0.61224429 0.60350697        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 869.735 seconds
Cross-validation score: 0.6512367008206791
Test score: 0.6707317073170732
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 400, 'classifier__max_bin': 500, 'classifier__learning_rate': 1.0}
3822.5804153480585
305.2642910279337
3583.7582691873667
3077.5459523419995
3.8749651396647096
2.9252658449079263
1.452635020017624
0.0
0.5317751988768578
0.0
11723.705136282795
63.46724546211772
14.17528934776783
0.0
4.678618336240836
0.0
2.752542034970247
0.0
0.7416727878153324
0.4226015744947631
0.00022075300512369722
5.962459908914752e-06
0.08943868029746227
4.261619778844761e-06
1.3396999747783411e-05
0.04470270127058029
0.062346116660023654
0.026353719264790243
0.10685285614256168
679.9949457058683
2.207468418460081
2.851369941781223
3.247405366439141
36.51081151969732
4.777658775849197
0.0
0.0
0.0
187.38257367748247
960.5183823461039
33.58841617084683
1.3280322095139923
64.98262287268881
53.683216215858764
0.0620238077302413
163.93531870589183
7.873724802607525
0.0
0.0
0.0
17.941082206070405
3.05908221095342
23.67404838165773
12.931403758179062
2.9357798744106267e-13
0.0
0.20262996722342352
0.0
0.47890489800346536
243.43716145652618
76.25781659978679
36.697886428059476
0.017998005453954944
1.330807340949832
1.6735566979596115
0.002990000353747746
10.026802674721694
0.0010711799841374159
0.005886243050099993
2.550555207034554
71.10681128589108
0.45201572359894726
135.83472412139727
0.0
1.3796112672425807
1.217420049215434e-07
8.659890245283023
5.469942411140437
50.775251737001604
0.010782407684018835
8.807996297424324
594.4624554736596
345.2862842698778
383.4703445489183
21.249383422313258
36.22555699657369
156.72008431560283
23.485672622415223
46.26937541390532
0.001125763808602187
71.07326619370953
0.0
13.369723686343207
319.06074371876184
1.353238510978358
0.01980454869044479
3.956782945898698
0.0
66.38718888169024
409.37925298055325
0.19951686682184652
0.08411602724252382
83.5599059452297
0.0
0.0807742084640708
252.50682651875235
0.0
4.744439910386866e-11
5.707411766983569
0.0
0.006648950176895596
205.5378010318793
0.0
9.90349587327728
1997.8950038331766
0.0
150.1852100693149
152.98387951847008
0.0
8.22229890338349
1.058636030270577
0.8192480279220763
17.592182356197327
0.014395325240911916
9.57732999399217
1.8588201772538349
270.18814767360163
6.017700313725072
11.157307115884635
3.642557256796863
1219.8515694525054
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998662   0.785714  0.647059  0.709677  0.670732  0.753425   

   Average Precision  
0           0.509295  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.65487127 0.6722349  0.65221806 0.68146308 0.64818229 0.67340948
 0.67044195 0.67796334 0.67459051 0.65657115 0.64167486 0.64063959
 0.66061321 0.65862579 0.6742288  0.68597094 0.67109363 0.60988973
 0.66954313 0.67518009        nan 0.65932149 0.6498334  0.67063678
 0.6627491  0.66389123 0.68462569 0.67353847 0.66420475 0.67572599
 0.67033832 0.65779537 0.66307321 0.64186228 0.64460735 0.67157593
 0.62819583 0.62536958 0.67644758 0.62572615        nan 0.65192047
 0.68019196 0.66164278 0.66197092 0.67272218 0.66011355 0.70130885
 0.69280004 0.65351135 0.64449458 0.65561827 0.65908055 0.69597471
 0.69544321 0.70228628 0.6835874         nan 0.67816537 0.65298705
        nan 0.66815023 0.67467728 0.64241344 0.6926507  0.6561564
 0.67201242 0.68163437 0.63561626 0.65720978 0.64677092 0.65277427
 0.65003771 0.65469129 0.68162283 0.65558055 0.66554482        nan
 0.66049765 0.68443178 0.63617641 0.66486451 0.68504105        nan
 0.64848334 0.69025332 0.68336304        nan 0.69672262 0.68510568
 0.6598823  0.66208031        nan 0.6735202  0.66359752 0.69976262
        nan 0.63257652 0.68229283 0.65617295]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 834.749 seconds
Cross-validation score: 0.7022862755310573
Test score: 0.5882352941176471
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 900, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.4}
3375.6570648496563
969.5823481391344
28985.779300512375
331.599992478441
2.0433113632025197
7.293645052748616
14.342533706309041
0.9501105244125938
24.025719046592712
0.038615599274635315
7815.191333316776
0.0
141.81400440470316
0.0
3.196399977139663
0.0
76.46494612842798
0.0
0.005869640037417412
1.7332700490951538
0.041145617258735
0.30639391962904483
29.19289987994125
87.53175264550373
0.019836900755763054
0.040347300469875336
1.4376976396888494
10.855067603290081
0.0016453549906145781
33.16088826395571
75.88527819259616
2.9555739839270245
0.7867987645731773
22.387815468828194
44.28810766241804
0.0
0.0
0.0
453.84460846800357
1133.2511936855444
32.09252501415904
70.91031600168208
0.3843924192187842
38.40848349167209
6.0582458334974945
42.38459695003985
33.09907287347596
0.0
0.0
0.0
8.930583143199328
1.1827340434247162
36.44499936656939
0.047617481206543744
29.479322196915746
0.05469475983409211
182.23599243164062
0.0
26.665402007609373
362.3970263742376
568.4883567153738
234.9035893017135
0.020873089786618948
21.664966099895537
2.8347651478106854
2.7937909468309954
99.14169132363895
4.631020039319992
0.029707419220358133
391.56750894204015
192.96953882186062
66.9605170417708
190.8328136578275
0.0
1.3489230275154114
3.368077039718628
0.5040209889411926
0.14417331259755883
446.65444546056096
0.0
4.933931894498528
540.9809962396394
416.05055944394553
3.2687017277348787
1451.1182115138909
0.31239963813277427
254.68350821095373
12.632468443363905
0.0012772299814969301
19.704403451876715
0.577176125254482
0.29189160093665123
3.8053309954630095
2121.6032234018858
2.7828707040753216
4.0487799644470215
206.4011568989663
0.41461101174354553
282.34230111563375
820.6065122288128
79.99614530883991
18.477227395720547
216.96115169746918
0.0
1.119972407075693
154.0118069218879
0.0
32.159705313213635
29.572335965487582
0.0
0.0
689.281744907712
0.0
0.6961693255434511
1732.7622762202373
0.0
197.62088051065803
25.820860519073904
0.0
11.082690745592117
2.4211413003504276
5.033791260066209
20.15614492246823
6.083062946796417
1.9075975082814693
264.10590838687494
481.0191139691742
93.42420804523863
9.000004768924555
48.1509912237525
239.75001690466888
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997919   0.588235  0.588235  0.588235  0.588235  0.588235   

   Average Precision  
0           0.347061  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.7352366  0.66360591 0.69675734 0.73400341 0.71922658 0.68865427
 0.69372316 0.68102756 0.68409285 0.65479454 0.66150934        nan
 0.70653913        nan 0.67308957 0.67259947 0.6717154  0.67611336
 0.6851148  0.6985081  0.70985115        nan 0.71694817 0.70636176
 0.66709858 0.73353753        nan 0.69329456 0.73793569 0.71167277
 0.71851494        nan 0.69924211 0.70156162        nan 0.68473195
 0.73536236 0.6931564  0.70571043 0.6995125  0.67492567 0.69585764
        nan 0.70810113 0.64848738 0.68025281 0.69516276 0.70766038
 0.70288462 0.68746703 0.70946564 0.72115305 0.6996421         nan
 0.66106695 0.70226371 0.6913681  0.72938312 0.69061601        nan
 0.72010195 0.67118241 0.6935274  0.72587715 0.70281065        nan
        nan 0.71534711 0.71744953 0.70015564 0.65360215 0.70246153
 0.67355767        nan 0.66895913 0.69027712 0.71636868 0.68855716
 0.69617481 0.67978459 0.72726725 0.70521993 0.70058545 0.69415868
 0.70899935 0.67505195 0.69010203        nan 0.71338096 0.67752126
 0.69699838 0.66490921 0.71610057 0.70213018 0.6712826         nan
 0.68764272 0.72357597 0.67078692        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 777.976 seconds
Cross-validation score: 0.7379356922540439
Test score: 0.47619047619047616
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 300, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.2}
10370.140257155015
2426.881806295711
19255.775780328866
318.4712741177717
7.433492885281877
3.8709025048467645
24.009457542560995
0.0
8.049479947658256e-05
0.14154300093650818
32956.53221469602
41.821166005311795
25.534473406386724
0.0
59.87428356050605
0.0
62.60037075344803
0.0010789720108732581
43.57525190233982
2.2359346390185237
18.753068510001658
91.63887813471956
56.793258754005166
500.84338154788065
0.3815087162074633
0.0010349152071285062
0.18569391456276207
4.51695654113405
0.0
706.4412308193096
349.4790242243671
24.2374144313027
2.3907599722655926
821.5103350479885
107.15516536414718
0.0
0.0
0.0
97.3721142312957
3698.338561670363
186.79656017584284
278.49757415320573
66.87901720720048
1060.0348645443992
96.55669986814982
889.8652020414538
889.4354447708024
0.0
0.0
0.0
25.77131603498675
162.956782285979
190.97706389008684
1.9691639804777878
17.03004293315462
0.015749403926747618
759.7957435817303
73.93065630618173
15.409678595306884
2062.9267087035137
95.89066588441744
2971.5084415066412
0.48586995645564457
0.10291210169141796
51.038541101886885
0.6066805167938583
15.48837596105568
3.2190399173787227
35.79401036251511
542.3857396457976
225.94662890903305
62.39467737243649
168.08330357447355
10.242156164242033
0.4420909283508081
4.429875437733472
62.83714720666103
13.195295730903837
66.4577987616893
84.6286932572344
8.835953378478838
1659.030195353162
1249.6108083943657
1887.9228783993267
1638.6001984795855
110.97956970264447
1598.7824889312028
13.19178819201656
0.1577724173896513
1.1325859553326154
63.10072201156618
0.0
623.5893557546804
3247.893850423535
3.5435289388016513
3.2523447595551547
6.8858627236493355
1.2293799954932183e-05
41.80417501875982
532.7720083126503
26.64232885618838
64.37468974446529
104.42230030951896
0.0
68.73175342982339
278.29560741982664
0.0
2.6618210476649438
1.6333401417723508
0.0
9.256469029613072
277.7035922025169
0.0
168.63410788849717
1155.199047547829
0.0
2.050820177369171
1585.7365687047031
0.0
48.70343925601628
81.6422425118324
22.098365696554538
557.1467386768041
0.01037139860636671
19.554736364482835
14.676638783560534
1790.9398973990424
122.23266105817291
9.931143827909677
478.2435705508548
94.34490955017282
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1       F2      F0.5  \
0  0.997472        0.5  0.470588  0.484848  0.47619  0.493827   

   Average Precision  
0           0.236632  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.65327414 0.65294409 0.67101467 0.66077871 0.70043626
 0.65435991 0.67417738 0.67764132 0.66036166 0.66555914 0.66737571
 0.68133592 0.67393574 0.68854741 0.65756586 0.6525269  0.65957561
        nan 0.66435591 0.68493947 0.66579277 0.69138815 0.6242613
 0.69882183 0.64371139 0.66559006 0.65252417 0.65721778 0.65989219
 0.66770559 0.66124851 0.67923871 0.67330722        nan 0.69210094
        nan 0.64323601 0.6535332         nan 0.64351258 0.6649159
 0.66705891 0.67598051        nan 0.68562722 0.68375422 0.67168926
 0.68199064 0.68102639        nan 0.69814798 0.67268828 0.64659213
 0.66711928 0.64839143 0.68806977 0.7000794  0.71047564 0.6849469
        nan 0.67267616 0.6677171  0.68163921 0.67541805 0.66828008
 0.65245395 0.6429814  0.69021863 0.67510419 0.70870077 0.68643097
 0.69364462 0.67781694 0.6900198  0.6427382         nan 0.66537408
 0.63500947 0.68095902 0.68942689 0.6711192  0.64006769 0.66497753
 0.68962711 0.67967226 0.64696308 0.65067686 0.67639005 0.65047354
 0.70300655        nan 0.65901158 0.68817716 0.65860911 0.67488825
 0.67542725 0.66006306        nan 0.6994555 ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 820.804 seconds
Cross-validation score: 0.7104756398874045
Test score: 0.6818181818181818
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 800, 'classifier__max_bin': 50, 'classifier__learning_rate': 0.4}
6152.168268720233
935.373164673054
13649.997188331203
547.4023931107994
16.537926208051374
65.1726280776783
1.020032363710925
0.6153802557382733
8.023640301427804
0.004283070098608732
16722.08706258068
101.61368559677365
16.263327550084796
0.0
47.78330084632677
0.0
39.535152071446646
7.724660235908232e-07
2.8069004659482744
0.03478885078220628
1.4222008947890572
0.01417692013599936
0.965478371831793
413.1034728240593
2.4344672083389014
50.82162870100001
0.5424181407397555
0.2821242552899861
95.22816789150238
933.5937467343938
3.96606230980251
3.0117419076570306
80.27856975495524
172.1422850217139
38.397589452190836
0.0
0.0
0.0
23.3675909333502
1900.535061606096
7.581671359852919
442.8482000014691
0.14570919339602884
168.0099884166462
6.989104472882245
130.5432778932655
16.317743576792054
0.0
0.0
0.0
86.27419922862066
12.84098412904145
77.4692860119175
2.108144865595876
0.010837900452315807
0.7900471240282059
294.5141757892539
0.26241201162338257
2.4214196881207783
643.716601457847
4.949345144091276
590.5738159126413
0.0558814809191972
32.341336076380685
25.35581510442171
4.145467671307795
17.290470168026246
0.4331845249107573
1.0047060884535313
185.32230499111225
758.9179207766306
203.13533134663334
12.138215795156865
15.576389472931623
0.10090355854055133
11.24674164969474
17.07160056266713
0.6535689774827915
132.57169728895198
3.532935789147467
2.563701420173074
854.7019165951587
410.16019941329876
1263.6007078302916
47.33119574081212
0.52172125314322
463.1794473988017
219.89252537553693
8.368380397907458
3.7411058415636376
7.0810898193940375
7.3371436967281625
274.25054423473193
1615.1360251325668
3.607454508570158
0.6671865433090697
2.887806301905303
0.0001132158977270592
118.49206953080562
302.8627277528185
9.34553300533934
62.924463007220766
9.075755377500554
0.0
56.10970790898648
90.12503275138624
0.0
84.84814449073747
39.261643376530174
0.0
2.9305031278806837
325.38402844699095
0.0
23.995683767245026
1771.4675685905156
0.0
0.02065706807013612
157.97853598594156
0.0
31.180420012459905
33.45969486139244
17.913475689436382
42.547070699938196
0.0
0.47338761138144037
6.556373961965864
1318.3336464389286
6.48325222528438
174.13474067815517
346.684639397726
1101.5681337974056
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998067        0.6  0.705882  0.648649  0.681818  0.618557   

   Average Precision  
0           0.424273  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.64495795 0.60892435 0.64106166 0.64041318 0.63062398 0.63845997
        nan 0.63276522        nan 0.63234706 0.63848669 0.60559832
 0.59688309 0.64429464 0.6435116  0.60924907 0.64000783 0.63729889
 0.62006908 0.63253827 0.64485734 0.62663052 0.64984029 0.63814516
 0.62187474 0.6428249         nan 0.63819443 0.61318259 0.6256157
 0.62147899 0.65960231        nan 0.64547425 0.65789057 0.64016223
 0.66300969 0.60416567 0.62377387 0.61639535 0.60926151 0.61383906
 0.63351551 0.62341906 0.62809213 0.63951734 0.62552013 0.62278147
 0.62883498 0.64441126 0.63627451 0.62769283 0.62476942 0.6206597
 0.64729933 0.61144451 0.6438191  0.6285472  0.63490757 0.62063951
 0.60750698 0.63934584 0.66462594 0.64321428 0.63908923 0.62891873
 0.59048061 0.65300625 0.65423706        nan        nan 0.6506241
 0.65248025        nan 0.64906921 0.59929799 0.60155159 0.62331303
 0.63688324 0.61434573 0.63200572 0.63545134 0.59503339 0.61229014
 0.59230533 0.6289078  0.65003724 0.57857498        nan 0.65052911
        nan 0.64539678 0.59458874 0.6343285  0.62697979 0.64999349
 0.61846944 0.61442599 0.6040865  0.66111099]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 796.302 seconds
Cross-validation score: 0.6646259382591785
Test score: 0.6593406593406593
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 600, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.1}
12128.859769642353
2593.7881084680557
86782.82886949182
7314.19738599658
135.38849633932114
43.06691002845764
1.0284199714660645
3.5174999237060547
32.71765971183777
0.0
22059.445562422276
1.4527100324630737
0.0
0.0
8.80478984117508
0.0
25.407714169472456
0.0
2.062245672568679
4.287820100784302
312.8600654602051
128.950909614563
3.7069010138511658
567.7628240585327
16.711542904376984
5.912099838256836
0.0
2.190850019454956
2.0644099712371826
5134.704593062401
1133.6839594841003
10.333230018615723
53.59788978099823
482.9529422521591
207.24359606206417
0.0
0.0
0.0
35.80047047138214
5129.220916986465
817.2048006057739
155.29497495293617
49.924718141555786
2099.9555872678757
451.6901360154152
179.90639734268188
409.9136244058609
0.0
0.0
0.0
89.50391429662704
60.677080392837524
268.859664357733
39.68239974975586
0.0
0.3378088152385317
360.4762363433838
0.0
49.370649576187134
2708.6911339759827
1337.683398604393
2551.536173045635
24.388689756393433
37.58133935928345
2.087670087814331
0.2558630108833313
120.01172789931297
0.0
170.85428002476692
571.1967296600342
979.3933552503586
76.0688705444336
892.0877328813076
6.5366997718811035
0.0209416002035141
0.0
28.005300521850586
53.51398691534996
477.5885384315625
315.7995958328247
21.45198917388916
8322.926389336586
147.6969631910324
271.7182251214981
1156.2031101286411
342.12877583503723
944.5969202518463
68.23650705814362
1.4704899787902832
5.738970158621669
17.46091714501381
2.4639201164245605
349.2801153063774
5550.832095503807
2.3549259901046753
0.0
2.2872390151023865
0.0
77.28555518388748
2467.0238225460052
105.93050980567932
1550.16457593441
23.34452998638153
0.0
19.77094280719757
185.6376724243164
0.0
10.461869597434998
15.420690059661865
0.0
92.73853051662445
257.7159841656685
0.0
239.41479202266783
6691.6291779875755
0.0
43.90040111541748
1764.343882203102
0.0
3.3157889544963837
47.541918992996216
225.66004240512848
284.94154262542725
0.7136409878730774
2.9708399772644043
41.8713014125824
1520.4595548575744
134.66471660137177
621.5692553520203
3.138780117034912
1579.2359292507172
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall   F1        F2      F0.5  Average Precision
0  0.997621   0.521739  0.705882  0.6  0.659341  0.550459            0.36903

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.59494313 0.62398765 0.65661156 0.61200697 0.61000216 0.64935047
 0.62229049 0.64663867 0.59551069        nan 0.62188483 0.6151455
 0.60547141 0.63811991 0.61681571 0.63766381 0.63229912        nan
 0.64688494 0.61121959 0.62658656 0.64357768 0.57762212 0.62754047
 0.61351151 0.6544689  0.6005142  0.59053079 0.61314979 0.63742661
 0.64376275 0.62483934 0.59231045        nan 0.63615466 0.64498406
 0.62891928 0.60343146 0.62728333 0.64139767 0.61540369 0.60198864
 0.62132394 0.66328226 0.61719104 0.63493711 0.5916381  0.62044936
 0.60159176 0.63083815 0.63172773 0.61509538        nan 0.66886397
 0.59134426 0.63885078 0.63988674 0.638711          nan 0.60066993
 0.63727681 0.6614634  0.64585791 0.61849662        nan 0.64613495
 0.6279815  0.59716973 0.63526986 0.64480733 0.58381965 0.63968098
 0.64848946 0.62925196 0.62959002 0.61454666 0.64661453 0.61109182
 0.59707708 0.64201893 0.66111097 0.63162839 0.6127038  0.61276838
 0.60546513 0.58813391 0.6106454  0.61933259 0.63561671 0.60006166
 0.64533196 0.64539529 0.61486733 0.63671692 0.61678085 0.63947246
 0.62039261 0.60447464 0.63018826 0.60867422]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 900.865 seconds
Cross-validation score: 0.6688639675772029
Test score: 0.7558139534883721
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 700, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.9}
3653.440237494986
1306.2401104126147
1949.8968249021473
58.85355118892767
2.1808596586051863
2.4740101340576075
81.35400718450546
0.0034378499258309603
0.0
0.08396530151367188
10741.987157417752
8.795517508959165
0.13492999970912933
0.0
1.0976357699837536
0.0
0.09037110209465027
0.0
0.0
6.26840603351593
0.004509249934926629
31.76470634672296
1.0546599626541138
340.6690139770508
5.562824254855514
1.1553800106048584
0.8051599860191345
2.4653799533843994
0.5886465888470411
327.38226502240286
99.35925979899548
17.06920051574707
11.71013707765087
24.41720123661071
23.19133719763704
0.0
0.0
0.0
1.4815749190747738
269.812610459423
16.59465014032321
347.1029761206155
0.0006091500108595937
913.4336212181952
20.390232390724123
5.2610929604270495
464.4130914013949
0.0
0.0
0.0
56.82331399100258
30.78216067882022
23.847350307845772
0.26162290573120117
0.001849450054578483
0.0020650499500334263
0.08433062891708687
0.3043299913406372
23.303253201956977
743.245036534674
132.836255043243
0.29327519293292426
0.29073798656463623
0.0
0.016351054538972676
0.630045599245932
3.2126005926256767
0.0
0.17566310614347458
863.9024008802589
194.4423343588278
4.7138554244011175
64.55459278885428
0.0
0.0007055950118228793
78.25640106201172
0.0
0.011537716158272815
5.81938830614672
3.6054310253821313
1.1869101867705467
3730.384563410771
472.53316241226275
172.54266234758688
9.233961824793369
0.5494405354838818
7.180656934418948
5.18414999532979e-05
0.0
0.0
2.7188546288634825
0.0
0.40465131161909085
1589.6290070263767
4.5065769189968705
0.0488646999001503
10.968833567816546
0.0
67.83069430941396
1329.842095922806
58.78900558690657
0.18748145822974038
11.054878467228264
0.0
0.053741998039186
19.381662837738986
0.0
75.35649871826172
28.870630264282227
0.0
0.0015238496343954466
33.587768445404436
0.0
10.767275259422604
379.1630231535819
0.0
0.00013319299614522606
620.1755942838136
0.0
2.4572180435061455
5.651827807654627
4.766304521974234
40.82438692749565
0.0
1.547397956252098
3.45720199868083
2271.9282136181573
19.682177304915967
129.55856947181746
0.3362461548022111
0.0004764969926327467
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.998662   0.722222  0.764706  0.742857  0.755814  0.730337   

   Average Precision  
0           0.552882  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.64055803 0.67374011 0.6917419  0.67516328 0.70062501 0.72167343
 0.71305483 0.72001388 0.68309679 0.6593353  0.68559853 0.67204435
 0.71210591 0.70238267 0.68774276        nan 0.73578302 0.7178109
 0.7116214  0.73333208 0.70673366 0.68444104 0.70565895 0.68479203
 0.68417289 0.68508872 0.70132859 0.69645711 0.68882115 0.66977901
 0.62543057 0.68086226 0.68713254 0.71313922 0.71927489 0.63936942
        nan 0.66906575 0.71048905 0.70009052 0.63495915 0.65780573
 0.7187317  0.70643083 0.73476104 0.70169261 0.71284659        nan
 0.70887626 0.70598058 0.69748034 0.69283055        nan 0.68208231
 0.6891586  0.66565366 0.6905616  0.71410143 0.70886433 0.66857594
 0.70303409 0.69966955 0.70929369 0.65391217 0.69265282 0.67170087
 0.68639303 0.67452803 0.70322917 0.68099943 0.66990782 0.67515164
 0.69129285 0.74220736 0.67487356 0.68330235 0.70731835 0.69984355
 0.69834715 0.70429874 0.70830196 0.67735037 0.67616141 0.68103451
 0.6974235  0.68628317 0.70455365 0.64080714 0.68419624 0.68659009
 0.67014769 0.69281448 0.67519521 0.69770883 0.67781664 0.68726822
 0.71110678 0.70474483 0.72429675        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 877.392 seconds
Cross-validation score: 0.7422073631950792
Test score: 0.4430379746835443
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 600, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.1}
4636.218825717268
13775.784206687067
95197.9018684452
6089.745890533836
72.05960237364525
32.25184805839672
193.41093452554196
0.1962530016899109
5.5448908221524675
0.0
21908.432954411433
32.22257905240258
69.66671012231382
0.0
41.48796140507329
0.0
17.10631097786245
0.2995743118226528
83.25791731989011
75.36835661693476
55.76037469506264
54.106590899507864
7.298935091122985
486.8354751765728
52.14818182401359
1.837977654999122
1.3924918202692425
3.4317905923817307
2.6204364733566763
2461.7989672765834
196.95907515031286
13.718964009545743
16.589611175761092
155.69825501268497
1083.2779633828177
0.0
0.0
0.0
133.64718758623349
2707.869942244841
1386.005265242653
68.05594820162514
41.417709633708
1797.1553742227843
288.0889047857668
160.3366689341783
113.45958849764429
0.0
0.0
0.0
75.93854367348831
44.49082313198596
146.75395944865886
35.707085134316
0.653869017958641
7.225530304014683
110.15336129255593
77.07847662517452
93.68992305500433
2750.344820851722
2474.4635100304185
3351.927554816357
1.8989187880652025
1.8439297510813049
100.24438558815746
3.7116851061582565
8.828528351616114
12.064936135429889
8.611702181398869
954.8984465550202
65.14704208405601
77.21026655787136
492.925645731797
0.0
0.1674881987273693
69.67855214978044
0.0
109.41997182369232
1204.6528665986843
72.88140723202378
48.64200109336525
6549.364685407626
1149.8671409533417
288.38851872719533
594.5188383331988
2131.1476569303195
1123.4939895002171
102.08441600203514
2.2715827494394034
7.790340277416433
30.83377104090323
0.002669630106538534
94.41539253597148
3079.519958818375
19.61157758511581
0.05445029243128374
548.0835226992349
5.067039589164779
37.284381031175144
2489.384877752872
130.30808717152104
66.46022747270763
88.56353426538408
0.0
24.631749391555786
527.6435780893917
0.0
0.798592489794828
11.243872513899987
0.0
99.25324249826372
107.00205818202812
0.0
161.36888528087002
212.23170759005006
0.0
45.469607699196786
2561.4897985710777
0.0
0.45101215690374374
9.186859041452408
598.7209120008192
989.8710499933295
11.091859253123403
7.238439258886501
162.9553499916069
2166.873049601447
556.2872416581959
782.8907779441797
1375.8895656659734
584.5718093438481
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall   F1        F2     F0.5  Average Precision
0  0.997919   0.636364  0.411765  0.5  0.443038  0.57377           0.263519

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.63747185 0.63041661 0.64140536 0.5799331  0.64195838 0.63670687
 0.62798999 0.62708421 0.61428245 0.60744886 0.64045611 0.61177841
 0.63688412 0.6044018  0.60944444 0.60324284 0.61521466 0.60900827
 0.61999987 0.63672309 0.62881949 0.64708425 0.62754723 0.59952287
 0.59953739 0.66053666 0.63203318 0.61706793 0.63763402 0.61604141
 0.63535618 0.61227519 0.61653193 0.59972618 0.62856051 0.61826308
 0.60323405        nan 0.6236226  0.58885215 0.63456009 0.65218272
 0.65703613        nan 0.62695752 0.57232156 0.62588724 0.64361431
 0.58777934 0.61833926        nan 0.62052637 0.61870026 0.61872757
 0.63909962 0.64077034 0.61021216 0.63529394 0.64130384 0.64095641
 0.61573789 0.62352442 0.62042936 0.6038752  0.6094703  0.6260748
 0.61672562 0.61304315 0.64487916 0.6105682  0.61606582 0.63479037
 0.59426972 0.62559154 0.62232284        nan 0.64097151 0.63334831
 0.64791949 0.63927955 0.63265601 0.59914644 0.6414614  0.62925961
 0.6234552  0.65011149 0.63317817 0.62400102 0.65455231 0.62204983
 0.59555679 0.64693255 0.60456263 0.58599785 0.62025684 0.6119909
 0.63966885 0.60647417 0.62517402 0.62592574]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 892.450 seconds
Cross-validation score: 0.6605366634778399
Test score: 0.6666666666666666
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': -1, 'classifier__max_bin': 500, 'classifier__learning_rate': 0.5}
3989.774530391224
767.7995463005934
19699.903350483426
403.84073846408137
1.4749485373467905
6.37058826602879
0.5387580022215843
0.0008543350268155336
1.8649363111617276
10.169084091670811
7313.83010517068
0.6978271050247713
0.00013750999642070383
0.0
207.92855561175384
0.0
4.927014432606967
0.06260751068522818
10.23233356705714
0.003087214077822864
2.113260684418492
4.238475811391254
60.36643471714342
12.791368344720922
5.144424609355099
0.0014256699942052364
0.5785643525295497
2.5698845995900896
0.030858343583531678
1019.7073901133957
23.202258958481252
0.13871777984122957
7.718009079549574
34.4618019142449
104.20972146711347
0.0
0.0
0.0
9.254447625963053
1718.8785746181277
46.26888288363393
59.90817931605693
84.29640915839991
340.24697905645854
0.05260794118528522
0.03693659355513601
18.699176852001983
0.0
0.0
0.0
11.785014018237977
78.02996318118676
44.31060911365815
5.832315737967968
1.450924038887024
4.762584060430527
294.8230241423149
9.856969995780673e-08
122.94124913741325
315.5632303472521
892.1081392209089
477.7914187165784
1.1828320655983604
0.6982761333274372
7.979244517431653
8.589436505788854
7.198129964889176
1.0717464410263347
0.7280007232911885
20.569415841817676
243.1852704927101
20.607872136677976
363.60153803184085
10.215499877929688
27.268291580490768
19.702733745551086
41.4567921156995
4.674259572929178
864.5424542208556
0.4743401036268657
25.19126779808019
1737.6698893097112
197.4826606915849
87.31552717294835
95.58862248804743
0.9389511162279902
358.91659758198875
20.599496045382693
4.757363903307123
0.2252465649516112
12.240895634507487
0.8380360007286072
53.06466594586475
1490.176852428761
55.06976205651881
0.19096977717708796
478.17358939257304
0.0
132.46258945816498
321.7256760205662
23.442128863886637
2.169610023498535
79.47432817132085
0.0
6.7631140871579145
91.50381001936395
0.0
43.234093102044426
2.1663918726189877
0.0
20.884182581107325
475.71375810152887
0.0
62.39814765770834
1395.3813180903958
0.0
0.07226703010383062
290.6319301673798
0.0
41.64095664968045
7.310789466704591
23.441518074061605
64.27164048290935
0.289901502430439
4.20012503862381
0.6493525702571787
402.42022629855273
17.406553826857134
358.7397983130613
12.219512831363238
18.07073250202248
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99777   0.545455  0.705882  0.615385  0.666667  0.571429   

   Average Precision  
0            0.38577  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.58940185 0.61685408 0.61289887 0.59786784 0.60402989 0.60727663
 0.62419537 0.57249945 0.60212349 0.59935638 0.60185754 0.61361596
 0.58888289 0.62078908 0.58889527 0.64815844 0.63015171 0.60364007
 0.59375805 0.58868049 0.60358011 0.57265546 0.58672157 0.61602617
        nan 0.6281646         nan        nan 0.63609936 0.60233556
        nan        nan 0.63280595 0.5818529  0.59890788 0.58334019
 0.59259391 0.60468904 0.59816372 0.60504197 0.61005556 0.59834091
 0.58712093 0.61964253 0.60505968 0.60806752 0.57953443        nan
 0.58279545 0.6299778  0.62501766        nan 0.61286811 0.59345854
 0.5938059  0.60413228 0.58543276        nan        nan 0.60272957
 0.62276666 0.59416871 0.59425453 0.5733036  0.59240531 0.60746213
 0.60210373 0.62663462 0.62803298 0.63105472 0.60511573        nan
 0.59039902 0.58959171 0.62841895 0.62253187 0.62834523        nan
 0.60547132 0.58796218 0.57841851 0.61747464 0.61128439 0.60039521
 0.61709355 0.60706522 0.58594288 0.63048588 0.60452368 0.62699615
 0.6272282  0.59194111 0.60117929        nan 0.57394647 0.64924212
 0.62529125 0.60397421 0.57660152        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 837.438 seconds
Cross-validation score: 0.6492421194398279
Test score: 0.7446808510638296
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 200, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.5}
2621.4657713384536
1398.892907556071
18901.69436460109
2184.4252492902633
8.188219578842219
8.815067147959313
18.091928221983835
0.5738475506732357
0.03144159913063049
1.416990041732788
8471.133861530478
12.191238393858363
10.645165184279904
0.0
104.90854972854702
0.0
24.961650601839576
0.0
6.113761770547724
0.06789690001824056
1.3823208189438958
31.87691277407197
6.655336171141165
70.74575816967577
2.9544821648414654
5.894101188503829
1.253845157527394
54.55823609412962
0.0
258.73058975828394
36.92566107231899
0.6617476998790153
14.991495351784105
28.79383392765658
217.06710032414696
0.0
0.0
0.0
17.89885613408842
2439.409830783205
31.507257275424536
33.23038417505954
2.995681126798751
437.4831689500623
30.36206498715162
193.2803896341891
2.4538146676819834
0.0
0.0
0.0
108.36018460732926
15.733908607370562
73.84350408756023
1.9761208827186696
0.031952704414936584
3.7000451953514357
1.415768578785901
14.762179845097874
7.665514385032194
497.6007263169432
578.5591558956639
159.44835388259696
0.27023040309325463
9.42551193665713
9.18486261237219
0.007025146438309093
3.470070104220099
49.25647406650786
0.08714256020175526
33.33344988047768
12.488597358515117
32.39181783743659
18.644771399511328
2.575279951095581
14.843523760190237
0.00036192001425661147
26.367900910453635
0.028724035119348658
375.872422653738
13.02745268912986
0.3030841231658245
1527.0508543199376
183.74705842843102
16.7480435430133
304.49563790440175
0.27999296281996067
76.45654314315225
25.63165500436571
4.971032072220587
0.06927695904970382
20.331089447371845
0.0
41.14947465946665
1827.1105551793983
1.6477173754683463
0.4620859469578136
2.6641302353992877
0.49314583777595544
7.473262796781396
937.8838767107284
15.169296299950918
35.199437662318815
102.95930051131013
0.0
3.140593076508426
929.0902587858375
0.0
4.33941128623372
11.624201244325544
0.0
0.5763973748908029
109.25533210567953
0.0
7.892561861314903
141.4271007368753
0.0
312.4307530171209
266.8568437183361
0.0
9.557299307522044
1.2701795190199299
5.150828408849607
140.41469493829655
1.6782951018831227
22.51959565600191
0.852383268345895
901.8680260462359
14.225014386262465
225.2657770925775
0.6767405844665051
392.55705982304744
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99777   0.538462  0.823529  0.651163  0.744681  0.578512   

   Average Precision  
0           0.443885  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.65403384 0.63579695 0.59263979 0.60491582        nan 0.62648526
 0.63230878        nan 0.64022973 0.61153977 0.59447104 0.59889613
 0.63524112 0.56803682 0.61155755 0.58497582 0.57884385 0.6341018
 0.6198589  0.63696262 0.60654358 0.5947804         nan        nan
 0.62012278 0.59317956 0.59346743 0.61058107 0.59715397 0.64955541
 0.60801202 0.61682627 0.63074355 0.59283399 0.6580371  0.61295272
 0.66071783 0.63023176 0.64423857 0.63167501 0.58772572 0.60099973
 0.6019946  0.58782206 0.55829397 0.42969624 0.61550743 0.59584573
 0.65063183 0.60885316 0.66278555 0.621548   0.5795375  0.61070225
 0.64152937 0.64550162 0.61395155 0.60773164 0.58723426 0.63906243
 0.63184657 0.58648774 0.63312782 0.60798945 0.62178004        nan
 0.63453248        nan 0.62061486 0.58529977 0.65575445 0.6731993
        nan 0.60168275 0.57031822 0.61933386 0.6206735  0.54022421
 0.59315242 0.62879042 0.60782189 0.60527558 0.60918988 0.58180245
 0.57779483        nan 0.59209904 0.66066777 0.6183059  0.64445847
 0.57108975 0.61438923 0.59445879 0.63414954 0.66379448 0.58473416
 0.61899151 0.59487637 0.61486281 0.58706416]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 865.527 seconds
Cross-validation score: 0.6731992984701962
Test score: 0.5747126436781609
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': -1, 'classifier__max_bin': 500, 'classifier__learning_rate': 1.0}
401.71635282354237
68.56341857981533
20744.230227574695
72.1541948201484
0.0
2.8996574726453446
0.0
0.0007860080222599208
0.0027796700596809387
0.0
2952.720730049121
6.168791004874947
4.8690686617046595
0.0
2.421264961361885
0.0
0.03685833828058094
4.9802998546510935e-05
2.4157493802777026
0.02492379955947399
0.05005750060081482
28.973958409012994
0.3319849967956543
46.18600082397461
3.5917229316255543
0.06600899994373322
7.228449821472168
0.4899854939430952
1.1022369851561962
0.3311196881404612
240.48768699387347
57.807721047074665
12.203245287353639
1.9429528600849153
494.84286980797697
0.0
0.0
0.0
0.4654902191759902
646.7872860893149
35.15928562602494
34.01498922462633
0.03020005114376545
5.493618388631148
4.868024600059982
0.0
0.6723890128450876
0.0
0.0
0.0
16.138166926390138
12.35123010233383
32.21976108393574
5.1009013566654176
4.958720091963187e-05
1.7192973345518112
16.138662609380845
15.139788424172366
11.232408386185853
585.8811947931536
6.618335353163275
159.7565051542024
33.17912917329522
0.0
0.0009330890025012195
0.00010746499901870266
0.21501885636826046
10.388612780814583
0.0
424.6134775352248
14.371152122883359
3.1920106318139005
3.23706757492785
0.006608820054680109
0.010391045841970481
18.080776712391526
0.0
0.07444330790895037
118.01090852350353
9.433250243775547
1.941703914446407
200.96433448840617
0.6569023403153551
1040.9394411352841
51.812345510639716
16.181570813525468
612.0781897372217
0.0008300979970954359
0.003431742199609289
2.742609977722168
164.01945777982473
1.329781505628489
45.71023059521758
650.4001830442721
1.758375104982406
5.153745505027473
10.606005295038813
0.0
55.61196463507076
369.35679547471227
0.023147336822148645
0.35120852805493996
65.44270712486468
0.0
0.22901000082492828
112.24127839261018
0.0
89.4626935524866
24.954490223055473
0.0
2.7252124723090674
117.76074125179366
0.0
17.73338698493899
769.1337705613842
0.0
0.0
333.80706640121934
0.0
0.000810529984846653
0.10691859573125839
9.219736161408946
11.515951295468767
0.0008354019955731928
0.023339100182056427
6.463114080368541
47.131981135345995
0.012374622630886734
63.07100595370867
0.00013073539594188333
37.72784416261129
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997621   0.526316  0.588235  0.555556  0.574713  0.537634   

   Average Precision  
0           0.310638  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.59154265        nan 0.61712174 0.60346173 0.62148189 0.63692872
 0.6276974  0.6306518  0.6204595  0.59901685 0.62512956 0.59690166
 0.61140868 0.61520916 0.60744555        nan 0.62666757 0.61073084
        nan 0.61550854 0.62763348 0.62128582 0.6344186  0.60007025
 0.60506965 0.59490014 0.62221156 0.62582555 0.60986565 0.64959984
 0.61638638 0.62888752 0.64200552        nan 0.62077833 0.6151649
        nan 0.61903341 0.61499018 0.629314   0.6373747  0.5890786
        nan 0.62280945 0.58189508 0.63028129 0.61783895 0.60261885
 0.62416845 0.60738595 0.64416858 0.62056782 0.6051781  0.6278185
 0.57901932 0.60625284 0.59980218 0.63161273 0.60883215 0.62777157
 0.61839032 0.61108299 0.6081024  0.61521178        nan 0.59879713
 0.60901828 0.59177087 0.57169347 0.58128268 0.63146089 0.59220231
 0.59765041 0.60893878        nan 0.62164242 0.60573916        nan
        nan 0.57987277 0.62241017 0.61743658 0.60340923 0.57561585
 0.59712703 0.64640582 0.60862252 0.57492208 0.60656324 0.60207434
 0.62298226 0.63006802 0.62769851 0.61914542 0.61241552 0.63501773
 0.58764223 0.62306271 0.61865925 0.64337148]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 908.696 seconds
Cross-validation score: 0.6495998373329718
Test score: 0.7446808510638296
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 900, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.7}
4724.23439817952
917.0015294813202
5724.859592493569
2070.5510969450697
1.1386211403878406
87.40236238987563
0.05803369915520307
0.14051300287246704
0.0
0.0
12976.782476014014
72.19205650512595
1.2065199613571167
0.0
0.41790594905614853
0.0
1.9289032879751176
0.0
0.007002280093729496
0.0
43.61549949645996
16.543474316596985
0.0
39.650699615478516
1.2826090157032013
0.002175400033593178
0.0
1.195419479161501
0.0
134.73973083496094
102.86303770900122
2.7165400981903076
41.470560194022255
111.44061711257382
141.47723624975697
0.0
0.0
0.0
0.04196953441714868
575.8691099208154
50.69168990367325
3.2952760082225723
0.7435473447258119
463.6133026323514
0.703781062620692
25.74834589973034
8.522338109556586
0.0
0.0
0.0
5.252724468045926
11.563067854192923
39.17141956667183
0.012294241925701499
0.0
0.9304599761962891
93.96381750791625
0.0
32.43025082710665
259.595449650471
130.66787937958725
644.0982816833712
0.9941993863321841
1.1271861637942493
33.033416794613004
0.2609090906698839
47.44723681662799
17.998637587181292
29.543168549076654
541.1328029908145
96.99658205703963
77.50290347635746
443.8443645454172
0.0
0.7056429982185364
1.5345100164413452
29.95669937133789
0.00018901399744208902
13.035068155506451
5.6774001121521
0.0010189319727942348
1243.1217117305641
279.08084064236755
551.6318040478363
323.1992017640732
36.68814193259459
106.31404675974045
36.1947381272912
82.35789461934473
0.0
0.06012556359928567
0.08411859720945358
72.00988556898665
645.8898337587598
0.0
0.0008047570008784533
4.1931784788830555
0.0
9.925416772442986
138.27384797133345
87.86854228758602
17.548214871581877
9.478417672573414
0.0
10.00627332314616
16.22375853419362
0.0
172.25570425391197
538.0667081044521
0.0
9.036830186843872
257.8081730756385
0.0
8.371189312485512
1235.7022445030161
0.0
8.097778141498566
379.9938196018338
0.0
0.3978807934036013
0.0011854399926960468
20.230126558613847
8.064196930266917
4.664829969406128
0.0
3.3053244904149324
384.29670386674115
7.853324096897268
196.23060800135136
0.3699322210450191
990.3755021719262
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99777   0.538462  0.823529  0.651163  0.744681  0.578512   

   Average Precision  
0           0.443885  

--------------------------------------------------------------------

C:\Users\Randell\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.63073842 0.60155163 0.60231395 0.59401096 0.61320402 0.61408285
 0.60101786        nan 0.61137758 0.59121134 0.63391065        nan
 0.60947923 0.64809037 0.60051499 0.64137619 0.61655456 0.59494398
 0.62795655 0.64614916 0.62407152 0.64293286 0.60531575 0.59826245
 0.60361467 0.6282384  0.62421472 0.6526792  0.60320984 0.62774741
 0.64557338        nan 0.55871944 0.60969    0.62975902 0.63509742
 0.65143798 0.63181514        nan        nan 0.63365847 0.59801915
 0.57679059 0.61473324 0.60641447 0.63328926 0.63385201 0.60671043
 0.63104506 0.61454984 0.59515108 0.6039588         nan 0.63039934
 0.57991099 0.63901797 0.64022365        nan 0.62459168 0.61197536
 0.57383832 0.59158971 0.59245958 0.57855575 0.5986116  0.63129834
 0.61358764 0.64331076 0.62860757 0.59123434 0.56850396 0.62687786
 0.6404984  0.61679848 0.61059103 0.5851857  0.6012779  0.61501775
 0.61209222 0.58745187 0.65256652        nan 0.54644698 0.615695
 0.63591791 0.66496012 0.62788776 0.62763807 0.61263741 0.60267947
 0.63946365 0.61119289 0.60973457 0.60948444 0.64882025 0.63430207
 0.6108031  0.62913115 0.62105158 0.61347356]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 910.717 seconds
Cross-validation score: 0.6649601195880688
Test score: 0.625
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': -1, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.2}
6089.650607377291
1783.516506202519
44527.59106441587
3096.3683452531695
143.97511467337608
36.937605291604996
85.67780303955078
0.0
1.508478045463562
0.0
11455.452570855618
152.83424724638462
38.77053898572922
0.0
9.95967960357666
0.0
184.08255004882812
0.0
7.429056793451309
0.0
90.34885239601135
21.542767256498337
11.223699569702148
55.026793986558914
0.0
0.4862070083618164
1.0506199598312378
0.12431000173091888
0.0
2406.934873521328
702.5192539691925
15.701539874076843
11.42012295126915
298.89506432414055
275.7288385629654
0.0
0.0
0.0
7.843110084533691
4140.28034439683
16.34730789065361
64.52794679999352
87.58137905597687
1504.7422859668732
91.70120167732239
518.4419982880354
619.9500042796135
0.0
0.0
0.0
268.63475178182125
151.06815314292908
124.63974365592003
173.3635078072548
14.237340152263641
0.17041300237178802
38.94377800822258
0.6988620162010193
1.8390140235424042
1203.1351485848427
1658.5987337306142
777.0126686394215
0.3980650007724762
0.3184410035610199
1.3998340368270874
0.5804150104522705
65.07871448993683
5.091010093688965
103.91200256347656
473.1980450153351
353.4971745982766
126.68783605098724
560.7416042983532
0.0
0.0
87.20464971661568
2.0387399196624756
5.092399835586548
429.06048150360584
3.35007306933403
53.40484356880188
2895.5679059028625
432.01189786195755
0.0
1918.9389938116074
578.8992326259613
415.81573900580406
88.97081708908081
0.293844997882843
20.36870002746582
1.5610940158367157
0.0
44.442671462893486
2126.307620987296
1.283941999077797
3.325608015060425
16.55656009912491
0.0
206.79628998041153
2617.0271657928824
114.83013236522675
7.071119785308838
2.996963009238243
0.0
11.229234158992767
86.19299228489399
0.0
0.0
19.812539041042328
0.0
1.099410057067871
439.12188628315926
0.0
191.75340639054775
2122.4395453110337
0.0
157.12667310237885
629.375092715025
0.0
9.712696135044098
63.54741954803467
34.60542856156826
826.4090460240841
0.0
3.631679117679596
7.948050022125244
793.3742703199387
53.53947979211807
160.20848524570465
109.81909942626953
460.98091945052147
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1     F2     F0.5  Average Precision
0   0.99777       0.55  0.647059  0.594595  0.625  0.56701           0.356774

--------------------------------------------------------------------
