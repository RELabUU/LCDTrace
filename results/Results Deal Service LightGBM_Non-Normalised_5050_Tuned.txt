[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 3873.547 seconds
Cross-validation score: 0.7451398446007463
Test score: 0.691609977324263
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 700, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.1}
4042.4002734422684
137792.14930927753
236880.63624013215
16634.884904026985
23480.018327355385
551.6208184957504
1008.8600614070892
251.83756053447723
3281.8932353258133
45.88565635681152
939.2849580049515
25.05699050426483
7260.783883213997
14287.069264680147
432.9431099295616
0.0
1006.5319446921349
0.0
195.7087584733963
51.150819540023804
9020.598439216614
978.2203191518784
7598.811386704445
1710.9795731306076
45.952318489551544
51.358139514923096
61.0730699300766
78.58953106403351
82.88628876209259
509.4223520755768
47.06583023071289
94.62457275390625
57.236510157585144
889.6456146240234
48.77382051944733
2144.47480866313
184.84128975868225
90.62377035617828
91.80624127388
388.47988629341125
135.79301953315735
0.0
0.0
0.0
429.7255172133446
346.8284869194031
368.5361148118973
398.54231894016266
911.522976577282
169.32510769367218
115.74526047706604
198.42631936073303
322.0887916088104
940.2069509029388
78.71451234817505
2960.93176561594
0.0
0.0
0.0
382.72862815856934
30.36389511823654
159.82155895233154
11.099733084440231
1401.0982635691762
438.73506343364716
134.69161927700043
389.42378056049347
236.17529273033142
914.0564458370209
539.679589509964
307.6644891500473
8.352461099624634
75.38227188587189
148.84520936012268
42.13708972930908
92.52213764190674
787.345445394516
16.472660064697266
127.56394815444946
1108.6313827633858
59.504228711128235
122.95614957809448
441.1518018245697
3679.2269744277
49.66198456287384
22.099209785461426
172.73685771226883
196.3746520280838
22.811189770698547
270.61039662361145
153.93296337127686
895.136347413063
256.55897599458694
97.21617937088013
740.7037811279297
374.35402423143387
319.8900141119957
2828.963105201721
510.5379419326782
35.340233981609344
147.14986991882324
452.78514778614044
1728.2307755947113
450.10045540332794
182.24737468361855
226.6393004655838
525.8536143898964
231.43490707874298
863.4344169795513
405.80644845962524
402.60253143310547
326.58682334423065
38.13862991333008
315.23198491334915
10.998430013656616
528.2684860825539
38.64678931236267
10.683699607849121
710.0284451246262
92.25156773068011
91.07047620415688
0.0
190.0586198568344
1153.9391151666641
0.0
292.9855837225914
123.94559508562088
0.0
267.48545479774475
403.40860629081726
0.0
373.61618142202497
316.9873776733875
0.0
525.962019443512
66.15341007709503
0.0
75.53981247544289
1259.1351531744003
0.0
1545.8317553400993
40.583319664001465
213.16934272646904
97.57917255163193
117.0916395187378
35.37134027481079
363.25314873456955
188.45104908943176
336.3685185909271
212.04745042324066
990.4388310313225
132.07887768745422
163.2640169262886
701.2030136585236
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision   Recall        F1       F2      F0.5  \
0  0.996246   0.580952  0.72619  0.645503  0.69161  0.605159   

   Average Precision  
0           0.423171  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68801545 0.69810714 0.6832971  0.66870089 0.69464867        nan
 0.64311669 0.67721834 0.6963009         nan 0.69892133 0.65839243
 0.64170157 0.68555643 0.68037533 0.66676639 0.70325387 0.68228034
 0.67836053 0.68375211 0.66490033 0.68757781 0.6649103  0.70192484
 0.67331032 0.67349533 0.69439309 0.67477991 0.6714271  0.69772675
 0.68545395 0.69199487 0.68891362 0.67618106        nan 0.66095313
 0.65663942 0.6866461  0.65797716 0.67560465        nan 0.70117343
 0.67881284 0.67780804 0.68634667 0.68158939        nan 0.67602423
 0.68906061 0.68155925 0.68763473 0.69812494 0.6829637  0.69894343
 0.68756094 0.70348532 0.68402956 0.66622584 0.68484463 0.689665
 0.65240691 0.67266774        nan 0.69202515 0.69273475 0.68244708
 0.65977569 0.68163714 0.67968718 0.67711828 0.70157466 0.67391088
 0.70395886 0.68553054 0.70601852 0.66927621 0.67698451        nan
 0.6888248  0.69485228 0.68119537 0.69377313 0.6963726  0.6844938
 0.69505618 0.67490738 0.6960397  0.66393445 0.68996615 0.67197018
 0.6956293  0.68088238 0.6940515  0.67263306 0.61427598 0.62544854
 0.67780068 0.68674949        nan 0.69275171]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 3294.689 seconds
Cross-validation score: 0.7060185157582544
Test score: 0.7311320754716982
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 800, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.5}
190.01319523661368
33029.686309478675
63947.45512898416
7227.801847876726
3354.049132565844
91.79296561565934
396.05922846953035
230.78367427219928
83.18853576903348
4.432179226154403
146.11965141146356
0.07381689657631796
1640.2025559401118
836.8310823998763
53.23764532859059
0.0
519.2510864244978
0.0
43.06824418302131
36.52489188236359
1372.188611907186
190.93398747544416
1663.9686848430138
6.636955774622038
43.08955412639625
1.8918367008009227
52.96053643025516
62.89599852786205
8.32044100793064
5.345594802041887
3.2651369236409664
25.34179980337649
5.730492357935873
286.99300824892634
15.089246589279355
226.06305532620172
820.3008639407417
167.2523528663878
30.734615355373535
404.4263850326897
36.20594538270234
0.0
0.0
0.0
211.04816263055545
173.0239214346584
265.7722590991034
270.65377362506115
429.016613937878
118.07475120492745
31.24364792596316
17.464544426853536
12.401173689839197
9.456680777540896
0.0
23.84059853423969
0.0
0.0
0.0
49.41802872229164
232.29203666611284
36.36008657516868
35.174701461066434
16.62716326402733
11.893357524560997
23.787480188999325
418.767757871683
34.67918781546177
448.81300001536874
22.83236153231701
64.35547100272379
1.8736650476930663
15.60784645671447
22.889892986393534
29.231648889835924
5.45028149546124
1493.4916724131945
113.8676754682092
8.007229267830553
197.8590469083938
57.35236832992814
93.08405845941161
122.14011156923516
526.1357867193001
4.39635141324834
8.401791094256623
37.16560627990111
14.54922957814415
6.333039276287309
37.05834708141629
280.81994465235766
145.4326001436384
31.433793482123292
56.16767354213516
56.233006370050134
32.3861094562817
19.186006670322968
376.34698200254934
12.267253868660191
1.7488879111624556
77.89287833869457
46.77436381037842
689.237525845936
131.07497902912291
4.540645604778547
211.5383765776278
50.117784472415224
48.80704426886223
102.9507695142238
141.54341019448475
0.011733523439033888
101.23931703939888
46.58232054421751
85.20880332863453
5.497820270698867
39.20568697444833
16.317421158630168
0.7117434738902375
74.70083762472859
8.671956904414401
35.78104060739861
0.0
137.5187050665263
69.3222616474377
0.0
19.77097316583604
39.79317024329066
0.0
57.012843489297666
115.57957845629426
0.0
12.86579078988143
48.37810563322273
0.0
84.89341394353687
3.99673718529084
0.0
61.75652820803225
121.97274079654744
0.0
6.499311946230591
149.31900253886124
52.55992156585853
4.984274367427133
23.255242807237664
11.516783164217486
37.39416409248952
147.4485385054577
118.7639515824194
321.88389431904216
77.63084400867228
7.464157098282158
339.7906095604194
132.6257088604616
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall       F1        F2      F0.5  \
0   0.99731   0.704545  0.738095  0.72093  0.731132  0.711009   

   Average Precision  
0           0.521254  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.69220245 0.69226971 0.68775237        nan 0.62980123 0.66085665
 0.6677672  0.70217402 0.68702065 0.67599404 0.65030915 0.69453332
 0.59165006        nan 0.67567306 0.66682004 0.67792784 0.6843599
 0.69529373 0.68752967 0.68789777 0.6902134  0.59069482 0.68861561
 0.69100172 0.66936311 0.67147373 0.65426697 0.67214984        nan
 0.68753364 0.69616741 0.69499834 0.64872392 0.64803017 0.70037446
 0.68072309 0.64244744 0.64557973 0.67001027 0.67711744 0.69071302
 0.61490112 0.68906447 0.65873331 0.6779112  0.60927528 0.71534157
 0.67749807 0.66797115        nan 0.68281388 0.65886125 0.66314769
        nan 0.66117232 0.68300877 0.65062202 0.65223247 0.70291765
 0.66436586        nan 0.64613853 0.66634081 0.69747746        nan
 0.66331001 0.68666682 0.65844956 0.67992996 0.70045043 0.70067538
        nan 0.66231234 0.68069268        nan        nan        nan
 0.69671985        nan        nan 0.67358287 0.66413287 0.70156783
        nan        nan        nan 0.66376487 0.67019489 0.6705177
 0.69253468 0.69596861 0.69331791 0.692768   0.69288585 0.6792359
 0.69423188 0.66879069 0.68887788 0.68316105]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 3483.242 seconds
Cross-validation score: 0.7153415732991866
Test score: 0.746606334841629
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': 900, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.1}
2783.4882920980453
151518.13451439142
235501.97475287318
12313.662519872189
22652.49463737011
194.26758855581284
1675.9907546639442
380.46300077438354
234.19335687160492
35.78785181045532
898.6253893971443
130.9892017841339
6041.235318541527
2270.2664456367493
1713.3686450123787
0.0
837.3346156477928
0.0
233.4727829694748
18.099648892879486
8759.985351860523
1337.9108154773712
4822.141409635544
65.73225176334381
139.2645159959793
16.136801779270172
258.72437739372253
752.0482153296471
57.861564576625824
55.78711009025574
61.35644030570984
2.902046024799347
43.71646583080292
346.73092126846313
61.376301288604736
1731.0436860918999
514.799585044384
190.65966683626175
53.17609667778015
1159.427308499813
263.16689044237137
0.0
0.0
0.0
263.56203323602676
242.31804531812668
616.1481446623802
227.29874169826508
804.5304629802704
611.2616004347801
61.66580867767334
141.2980163693428
233.36970752477646
375.09843623638153
0.0
601.0748770236969
0.0
0.0
0.0
1332.2989576160908
378.74004834890366
109.54894524812698
18.693500518798828
833.2068167328835
117.16501468420029
63.63160943984985
5629.131841123104
903.2958997488022
1530.7197931706905
461.9431634545326
1377.8295137286186
66.58844578266144
53.32830709218979
285.9397220611572
113.0465589761734
103.9068540930748
648.4564267098904
292.72453260421753
25.406913101673126
1987.360510468483
173.80493050813675
317.30916476249695
397.60299614071846
1622.4780296981335
288.7054178714752
159.23103213310242
101.70295655727386
76.77923107147217
185.2872951924801
352.1384542584419
179.15477466583252
651.35089635849
664.123958081007
307.8742707371712
703.8399395346642
864.9577730894089
869.3933991193771
2043.5249330997467
475.5889586210251
130.24666905403137
38.10437297821045
1217.3360541462898
2004.8124345541
563.1519337892532
317.8897854089737
663.5779036283493
789.3922375142574
332.4437338709831
389.6692537367344
411.075841486454
617.5758773088455
500.31505250930786
1398.487144291401
247.33283388614655
9.160301923751831
133.80617117881775
385.5554681420326
194.0285905599594
404.2979768514633
49.09327304363251
192.071060359478
0.0
210.50626230239868
556.1746175289154
0.0
186.48638001084328
94.94536209106445
0.0
69.54779934883118
1275.8956724405289
0.0
770.910731613636
549.4313803613186
0.0
746.9556075334549
219.06383162736893
0.0
262.77570056915283
1138.3832264244556
0.0
12.584958851337433
958.8671408891678
704.1273867487907
58.03308528661728
481.9423696398735
148.05690371990204
3462.9692992568016
1059.6327214241028
269.10067796707153
362.20507967472076
194.23678410053253
88.44716787338257
1211.1674926877022
202.92949891090393
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99675   0.622642  0.785714  0.694737  0.746606  0.649606   

   Average Precision  
0           0.490227  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70203736 0.70482069 0.68204145 0.69506767 0.71055615 0.71518669
 0.71631523 0.71084909 0.69126592 0.69529689 0.689754   0.71238603
 0.71258    0.68640886 0.72454386 0.72425774 0.69303008 0.69653489
 0.68820159 0.66900036 0.7018452  0.69184369        nan 0.72370113
 0.72044349 0.71250878 0.70489842 0.69390704 0.71499895 0.712673
 0.72516688 0.70883023        nan 0.71590458 0.71033316 0.68915015
 0.69925057 0.71968193 0.69956586        nan 0.67227302 0.72347005
 0.71107537 0.69670904 0.67486473        nan 0.71035848 0.6858844
 0.70342895        nan 0.69192674 0.68529793 0.70384421 0.68075948
 0.71151222 0.67133958 0.70167975 0.6940008  0.71986885 0.70626544
 0.71990696 0.71027574 0.71356249 0.69326146 0.72337353 0.70415303
 0.70276133        nan 0.70119767 0.68292249 0.69088596 0.72793012
 0.69026983        nan 0.70571185 0.71621683 0.70753181 0.71085468
 0.69057171 0.69647666 0.68921115 0.69643451 0.69041351 0.71311721
 0.70143288 0.69832229 0.69418708 0.69437559 0.69423714        nan
 0.70357673 0.69236514 0.70572633 0.67347024 0.69402277 0.68678492
 0.70400858 0.69665988 0.71036046 0.70906738]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 3513.870 seconds
Cross-validation score: 0.7279301232034671
Test score: 0.7125890736342043
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 800, 'classifier__max_bin': 255, 'classifier__learning_rate': 1.0}
989.576102321158
17105.960301936928
44224.168254013595
4461.573176101658
1470.3987890418975
47.17365297221113
228.43059782510318
103.34900794374335
93.25373123958707
2.0219312132394407
16.916232996154577
20.11903615335177
607.2286928610883
5923.529495367613
169.43226224009413
0.0
579.2987810475752
0.0
8.356363833470823
13.798167718949117
2180.914952305473
97.37614075503006
343.8527935626789
986.2291546985361
10.97212843374291
3.0577854104340076
122.11312368298968
225.0156454279786
0.011208117357455194
14.580865863608778
0.0018444990491843782
0.10825827403823496
1.7410139306157362
64.99731676509691
12.210077945284866
165.0848897324322
20.854802298828872
0.9454409690224566
14.759801190799408
33.73042435188654
22.09139891346058
0.0
0.0
0.0
0.08205663951230235
0.0
82.00879503205215
18.39088622915733
183.81710660492536
122.90148355215206
39.361823178267514
1.459130048751831
44.22934852475737
22.03104120000353
0.0
13.463025237459078
0.0
0.0
0.0
7.146996401497745
403.0570684629474
18.96845026495066
0.003688677097670734
179.60973570353417
103.73051720592048
5.194927764125168
49.2318374812603
122.41889207821441
50.000552605761186
104.39610964027634
0.0072160191994044
22.416976602748036
0.0029417099140118808
0.0005145599716342986
2.8762722397586913
0.04118069240212208
70.46537946186072
3.0715347692603245
16.797073431531317
105.72702034928807
11.90086163484375
10.233851649725693
39.176353085174924
222.3416727894728
0.7369255203884677
0.14520199596881866
21.942896790111263
6.482516139280051
2.2206520642594114
0.013669193562236615
3.6507705463736784
39.15564221646491
100.59187916391784
3.7776722300659458
17.288144535814354
22.059880831069677
0.09208243836474139
193.23657219234155
87.28213080763817
0.4738989071447577
120.53191383089506
13.976556322086253
15.148910508161862
131.62328301966772
40.53263113589492
476.8940328134777
22.990836603923526
76.96537310266285
140.1141350744292
42.78735614963807
11.0590603381861
315.24810443263686
8.678164714932791
8.543339552645193
8.512590513681062
0.0019367581699043512
238.7119826447306
0.0
81.6149081774056
10.842054873239249
2.8906965387832315
0.0
16.118727664495964
131.94615221121785
0.0
24.365234822485945
50.29639648820739
0.0
28.4252980616875
290.416256040422
0.0
50.7519986119878
56.10467399264962
0.0
0.29617549083195627
1.1651434592204168
0.0
3.844250406284118
173.15800602576564
0.0
102.9450257067947
17.63308502914151
33.846587408392224
2.172730142570799
67.55485211420091
10.866577832348412
38.54311144510575
0.033183409832417965
71.7915700130834
10.745304134720413
0.6597101974602992
26.033643658251094
12.933251552854927
15.260583449417027
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997254   0.705882  0.714286  0.710059  0.712589  0.707547   

   Average Precision  
0           0.505546  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70132532 0.72012559 0.69183475 0.68159931        nan 0.66626086
 0.6857619         nan 0.69505032 0.71087075        nan        nan
 0.69091012        nan        nan 0.68140507 0.68442298 0.70038509
 0.68009736 0.68339792 0.68604559 0.68271166 0.68953921 0.67872806
 0.67664057 0.68856774 0.69057784 0.7064815  0.68588284 0.71424449
 0.69400847 0.67182373 0.63647687 0.69872048 0.66756815 0.67474049
 0.67000563 0.66333929 0.66915088 0.67924048 0.6458887  0.67890823
 0.70616985 0.70152016 0.6828713  0.66979366 0.68019454 0.70619033
 0.68621757 0.62163367 0.68798726 0.6809799  0.71467021 0.67251638
 0.68292915 0.6870401  0.66996435 0.68591787 0.68683027 0.69756532
 0.69146111 0.72278446 0.66824744 0.69663242 0.70553244 0.65955779
 0.70523771 0.6955634  0.6579402  0.67474032 0.70858585 0.67086495
        nan 0.69148181 0.70198167 0.69844488 0.70765781 0.69172348
 0.68868359 0.6965326  0.67829327 0.67810815        nan 0.69663206
 0.67674317 0.70805203 0.68703013        nan 0.6840361  0.66319796
        nan 0.68640329 0.67873458 0.67531698 0.679805   0.70825157
 0.6820047  0.67558145 0.69734116 0.715429  ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 3450.718 seconds
Cross-validation score: 0.7227844560042576
Test score: 0.7488479262672812
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': -1, 'classifier__max_bin': 500, 'classifier__learning_rate': 0.3}
1310.763629437366
53175.67648664769
87580.66816881593
4936.454786751361
9335.695764376724
31.826771178515628
153.5690964133246
59.83631796611007
49.23343201272655
9.813317752268631
228.54480097821215
0.05880400165915489
1439.8161481113057
2977.5724866190576
111.16347360855434
0.0
251.5756196451257
0.0
20.645530070061795
2.055679768061964
2504.0617890093126
461.78904978663195
2911.935559789883
917.9544995943143
38.4270992919337
3.617081944248639
23.659190386359114
132.77298952220008
42.02025407226756
36.88224541279487
49.371477110194974
16.16759957629256
35.44550570810679
269.2834608524572
16.25544903683476
1591.8379524121992
440.4075942306081
59.570256727631204
10.79077239864273
92.05357962893322
50.32903569302289
0.0
0.0
0.0
249.18464449228486
89.38691135670524
374.5373850770411
300.9847852328094
285.16986250015907
53.158197556680534
0.6168299913406372
9.268601111485623
196.50618566118646
353.7520636616973
0.005831780028529465
134.44239889178425
0.0
0.0
0.0
179.62291301379446
52.716235704079736
41.63083925037063
24.48498090240173
690.716746051563
34.13103967439383
21.93294582539238
2139.581317289034
88.25777727825334
344.44566574896453
34.36468096449971
296.35754248825833
10.487292528676335
4.500952064990997
117.23118711134885
12.810762576991692
98.50729631475406
698.736643466109
101.18996050057467
32.15500896354206
464.2207699830178
134.81787303375313
273.72610041103326
199.47604739968665
474.29348410081
59.48239107313566
30.49271166534163
135.44368986861082
42.01985964970663
64.58022728812648
20.891713281045668
103.2551516565727
367.03358387964545
169.565044945688
198.37057998543605
274.7825662687537
256.31509896717034
64.82517726815422
539.0636126492755
89.31443654414034
55.46862116031116
21.058564343838952
197.794427562505
653.1866737453383
386.4475659339223
3.5457437339937314
274.2078849938698
52.188689451199025
205.19719502492808
221.32343091489747
207.76571667724056
398.49151421058923
55.68167457031086
262.97278389689745
287.24091248607147
8.03074634008226
76.29685197130311
74.03225787403062
13.141908402496483
34.23610797571018
6.60803168034181
132.12241761287441
0.0
66.56991734803887
462.42857499344973
0.0
84.12250834255246
46.01377031742595
0.0
0.008776790113188326
375.5553261009627
0.0
368.7486059489893
52.97008700238075
0.0
87.6172519151005
3.8938933183671907
0.0
172.1836844298523
468.4475737992907
0.0
46.93072670462425
60.550384592381306
128.05746008391725
25.813845133350696
51.87076252436964
130.50582591502462
1789.3013031984447
0.0020756800659000874
88.66005609737476
344.6313243263867
158.98391689310665
45.35158529196633
178.4672608821129
311.0013830413809
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision   Recall        F1        F2      F0.5  \
0  0.997086   0.663265  0.77381  0.714286  0.748848  0.682773   

   Average Precision  
0           0.514306  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.69965304 0.70893474 0.71803548 0.71035754 0.68775718
 0.70648813 0.70732938 0.70653242 0.70963834 0.71977848 0.69017347
 0.72510387 0.7066955  0.71495416        nan 0.69885214 0.71152518
 0.71957623 0.69658462 0.70346045 0.70237819 0.72740063 0.70715412
 0.73042331 0.69568913 0.70421825 0.61451961 0.68839314 0.70219327
 0.6912737  0.68855088 0.68996459 0.68121766 0.68155442 0.69811138
 0.71464035        nan 0.7392354  0.69248513 0.69480462 0.71143415
 0.70269    0.69634435 0.73677622 0.66787057 0.71607044 0.70984259
 0.72290019 0.70837802 0.71867521 0.69772513 0.69071852 0.69308776
 0.71788339 0.7130824  0.7222904  0.69399681 0.70956277 0.70050585
 0.70090719        nan 0.71766227 0.69858144 0.68570753 0.71077365
        nan 0.73044177        nan 0.71433495 0.72866967 0.68752791
 0.71186481 0.71914368 0.70127473 0.71078901 0.70541726 0.69203317
 0.69202497 0.72462191 0.72763663 0.71811942 0.67104169 0.69786084
 0.68592741 0.68901149 0.7119271  0.70669072 0.69553108 0.70990772
        nan 0.69420887 0.69723974 0.7057967  0.69646997 0.71221996
 0.69506942 0.71611625 0.67623553 0.67920502]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 3526.052 seconds
Cross-validation score: 0.7392354044694049
Test score: 0.6610576923076923
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 400, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.7}
198.30314110860309
30724.768019813568
48018.3778337463
2163.3815292423533
4497.427898266396
36.08530720059571
91.16499867418315
6.928534815553576
2.6388086889637634
82.98324892969686
6.984540104866028
7.691644761558564
828.59960921327
561.0817275748414
272.2873236507221
0.0
58.1511366131017
0.0
22.200123928952962
2.842688156175427
2564.4831023001934
196.2779855042172
1922.8166730751836
35.44967759651445
8.666694104729686
11.01116359536536
5.952303547950578
2.9942083951209497
8.609079668531194e-05
4.160380214772886
24.372267280559754
1.2811739678982121
78.62419553082509
123.61059195748385
44.98950698628323
0.8971123074443312
136.49746595455508
7.040065232948109
7.627174701963668
110.56782602291787
105.09840608385275
0.0
0.0
0.0
149.5410895223613
52.003057337849896
49.856118381190754
0.8154867090051994
9.027347549097613
38.60095566331074
58.205451758083655
30.06039588159183
41.03497847367544
395.2761067367901
1.3751300573349
109.31685257671779
0.0
0.0
0.0
21.101667768445623
54.94280943202102
9.301942462068837
0.0017992940047406591
142.972914953094
15.417266653141269
9.790209878701717e-05
31.066503305570222
5.884813154343647
411.806896814629
47.183213683492795
57.00012923699251
37.92408516939031
70.38453605461837
7.100408287718892
3.537575886497507
31.211574742686935
68.10840072658902
14.95666170772165
3.1592826540472743
122.49986436895415
2.153479294069257
18.23808920536976
186.35442372211764
213.5513374763359
55.627252423091704
5.885080679086968
44.968525612603116
65.9803970332614
1.0424147734374856
64.19927458358143
9.601928476564353
39.84215797069919
39.22010170064823
9.524603733792901
65.84714586584596
35.61175500430181
32.995058208127375
470.892602909189
83.44972304493422
82.35487386850582
8.297796232625842
0.6115716969361529
499.59122644143645
299.00484988305834
80.63037536240881
0.8257821219667676
3.352094705351192
162.38921615517393
500.583353763941
65.27444263912912
84.4250149577856
276.85343454848044
549.3070401349323
23.542524671038336
33.39409638179495
97.16409424989251
82.13017251504061
0.00013701300485990942
151.4709645711264
60.18556228654052
5.0282314959476935
0.0
20.790475822315784
133.6798275782894
0.0
28.042793454813363
4.231215419218643
0.0
1.5508856719025061
249.02580314545048
0.0
61.82435917986004
133.28124420003223
0.0
21.154274755477672
2.927897508823662
0.0
71.45861790581694
142.5396126945834
0.0
47.763755873718765
69.90082973984318
37.462344471954566
7.01390718811308
0.9822601974228746
129.30855772678115
949.4940713520919
27.438199996948242
84.62009537493941
401.3244216981693
449.24952381601906
56.4810163750808
31.369228962401394
160.2859954426458
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996974     0.6875  0.654762  0.670732  0.661058  0.680693   

   Average Precision  
0           0.451774  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.66181483 0.5930709  0.68001861 0.68854831 0.67933822 0.69099219
        nan 0.67429379 0.71179971 0.70709462 0.70524918 0.67151663
 0.6534021  0.67356538 0.43637059 0.67475206        nan 0.60685758
 0.70110753 0.7075487         nan 0.7001353  0.69554098 0.68180238
 0.69097186 0.67516677        nan 0.69403963 0.6975411  0.69080174
 0.69544571 0.68947829        nan 0.68800077 0.69192357 0.67126126
 0.68843326 0.61268302 0.66849618 0.70223523 0.6901891  0.69679539
 0.54201746 0.67806641 0.70909893 0.67057603 0.67551421 0.69508495
 0.67300033 0.68295675 0.66190844 0.69790267 0.62923206 0.65418775
 0.70571932 0.68263148 0.67404125 0.69356614 0.66026149 0.66234348
        nan 0.68675527 0.70380195 0.70267719        nan 0.69111911
 0.69049222 0.66948053 0.69808778 0.70243361 0.69366348 0.67073611
 0.70248183        nan 0.67436929 0.69170696 0.6733808  0.71343178
 0.684303          nan 0.65748472 0.67880089 0.68514238 0.67754306
        nan        nan 0.69034332 0.69643457 0.69759539 0.69262781
        nan 0.68905151 0.68847881 0.68068862 0.69538814 0.66693113
 0.66987083 0.68439065 0.67695652 0.66937399]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 4049.319 seconds
Cross-validation score: 0.7134317849901947
Test score: 0.7062780269058296
Best Hyperparameters: {'classifier__num_leaves': 26, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 100, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.1}
2392.4446382522583
120366.765468359
240413.59669411182
18902.921875953674
19001.88786673546
3975.997059583664
402.69784784317017
152.3717176914215
384.1818952560425
69.66747903823853
15.187499761581421
6.092199802398682
9533.74962759018
18797.56282186508
1422.9032979011536
0.0
2789.767534971237
0.0
574.6108770370483
40.26399898529053
8153.615362882614
5694.842065215111
6863.647039175034
681.0884237289429
76.60220050811768
4.064839839935303
261.46748065948486
2416.2786288261414
629.5963969230652
1024.3780624866486
61.1924991607666
58.797119140625
39.373770236968994
946.0610811710358
21.99656057357788
2115.510140657425
566.6050171852112
120.79622960090637
81.65088057518005
2515.592109441757
162.91014909744263
0.0
0.0
0.0
204.3716104030609
668.2272548675537
306.280921459198
150.25858855247498
1275.5129871368408
210.0217423439026
70.03601002693176
275.708696603775
76.44340968132019
1462.4817702770233
64.11200141906738
1416.8582332134247
0.0
0.0
0.0
1140.5330395698547
371.35344755649567
76.19848966598511
17.45829963684082
1246.6082127094269
156.16631078720093
107.05409908294678
376.6479949951172
1241.5192952156067
1201.781640291214
85.15047001838684
418.3175411224365
85.89565849304199
48.476200103759766
310.53352546691895
16.8404803276062
104.1124906539917
2783.2487292289734
84.42002892494202
19.894670248031616
1096.8197026252747
91.89290118217468
202.59939885139465
106.50015139579773
2550.24702167511
61.544750690460205
17.954669952392578
139.26232194900513
413.04126834869385
171.70739078521729
63.18460965156555
272.5773468017578
1410.0868096351624
261.69058084487915
26.446439743041992
602.8600559234619
318.5850131511688
352.5420866012573
1412.2294130325317
232.66929006576538
96.84845924377441
25.299699783325195
531.1781904697418
1186.8501958847046
304.98358726501465
41.569429874420166
1116.7637581825256
91.6909716129303
538.4235925674438
755.0509536266327
363.86806869506836
12.969769954681396
1320.362289905548
139.23920679092407
56.09051036834717
40.39708995819092
279.86252880096436
582.0780897140503
7.842032015323639
299.0179078578949
117.67341232299805
182.3854308128357
0.0
350.12676441669464
271.6810574531555
0.0
270.67127990722656
152.4169216156006
0.0
136.86100959777832
1359.0545949935913
0.0
122.41223907470703
392.160297870636
0.0
736.6355400085449
39.86348009109497
0.0
122.15895104408264
1883.896250963211
0.0
38.58888912200928
89.26677060127258
317.3516311645508
62.26444053649902
30.610300064086914
88.47207021713257
758.5823452472687
78.20831966400146
389.0381155014038
543.7662396430969
203.27645206451416
23.317179441452026
681.6257576942444
460.6596043109894
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall        F1        F2      F0.5  \
0   0.99619   0.572727    0.75  0.649485  0.706278  0.601145   

   Average Precision  
0           0.430722  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_plot\precision_recall_curve.py:125: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.72101134 0.69382866 0.72722808 0.71353755        nan 0.71663792
 0.71039437 0.44135322 0.73346575 0.70209254 0.70978397 0.72270568
 0.71932373 0.72739754 0.71916359        nan 0.72318274 0.71082877
 0.75900275 0.72600896 0.72055719 0.65806355 0.6944157  0.71269344
 0.72990733 0.70713789 0.73596955 0.72500194 0.52491306 0.71613634
        nan 0.74181473 0.71293804 0.73253164 0.69608399 0.72437827
 0.69195871 0.72445317 0.74024085 0.73713553 0.71833271        nan
 0.73296591 0.74149023 0.74047775 0.7115406  0.70620964 0.72356948
 0.72098749        nan 0.70949643 0.72503231 0.71707886 0.71509457
        nan 0.73244457 0.6918655  0.74028686 0.70284598        nan
 0.71472199 0.73757063 0.71807457 0.680817   0.71724417 0.69113555
 0.71722453 0.72272634 0.70930272 0.7217137  0.70260834 0.68578186
 0.72224385 0.7115841  0.69546845 0.70826068 0.71751786 0.71897328
 0.73362614 0.70474979 0.72905253 0.73134036 0.71185003 0.71655821
 0.70500582 0.73603422 0.72043364 0.71117192 0.69980921 0.72588895
 0.71610228 0.71697552 0.65822792 0.70790736 0.7319351  0.71264245
 0.71191754 0.72391881 0.68626239 0.7152059 ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 4000.911 seconds
Cross-validation score: 0.7590027450093066
Test score: 0.6323185011709602
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 200, 'classifier__max_bin': 500, 'classifier__learning_rate': 0.3}
1188.7853308406684
43905.49650288843
94288.16779517054
7969.378897403032
4458.671648818723
312.7961964858114
115.30578569414226
19.468814674328314
13.294689585556625
124.19237925287553
361.4223880349309
53.89358271700621
2970.583814957805
5778.695732018336
242.40547668894578
0.0
1118.0390512325612
0.0
54.24095871589088
5.190659591928124
3447.7588244316685
2656.358913026692
3483.815837535498
62.68505813875527
11.46985331986798
10.305170466424897
124.97314156912034
248.12041977934132
3.312113035673974
115.61750758485869
14.821721790824085
10.114871933154063
16.30955112783704
357.0887760721271
52.91318251255143
304.91340957250213
157.36201523394266
50.646375590818934
55.53054329389124
293.4558452237361
66.43148221449519
0.0
0.0
0.0
159.90522710424557
15.271076236269437
241.5902427767578
31.554831566201756
217.28512763959588
324.67852239269
13.147029271611245
137.0206292871735
175.527623945527
449.1322509784077
30.504779756069183
934.4472440086683
0.0
0.0
0.0
63.95070069853682
48.49281827149389
77.15716963008163
19.745303906151094
60.05291744011629
35.733328620524844
50.97528549355047
753.3797120190575
90.74150973750511
539.5162098321016
126.70337185486278
256.6266172231408
15.727628029882908
30.939281472747098
26.6554749312927
35.77923514446593
6.78107724704023
762.1868837357179
88.33806801037281
41.87105029569284
504.00795365683734
37.74893685123243
65.46662938714144
237.7317429400464
321.8073359360569
32.170624356600456
14.132759547588648
49.981196936281776
38.76846621927689
7.224680895684287
116.06088448531227
101.3965393358958
106.39355885259283
44.868528317150776
34.1903612659371
184.145793844109
116.10517125949264
4.645031073814607
247.5274201699067
258.3754184834834
19.854060381650925
10.672816854028497
240.78694727172842
896.3046980472573
39.48819742073829
101.26494526829629
541.381094239172
232.9056693239836
123.73247202293715
302.63643620921357
151.80838852829854
492.3077773722762
315.17244062258396
50.56495940852619
45.02705547603546
9.133638797327876
98.19649163694703
199.8963611269137
407.127681522863
29.384573407965945
17.5804643088195
158.77108774134012
0.0
309.34017407734063
142.28700411165482
0.0
30.428402463148814
51.38414694905805
31.358400344848633
45.83952596380914
505.971045676808
0.0
178.96559070402873
145.14783831402383
0.0
234.9995881652576
12.252131536704837
0.0
75.88346082155476
568.7368551580003
0.0
71.63546310033416
31.680114866700023
110.75652393801283
73.02423501742305
46.46248038444173
50.60924434478511
119.45593341792119
1.4247153814067133
115.77258881302623
119.98153777956031
291.30368405091576
48.078688740657526
463.3355911402032
253.9011295979144
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996246   0.593407  0.642857  0.617143  0.632319  0.602679   

   Average Precision  
0           0.383157  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70482468        nan 0.69642984 0.71568504 0.71771037 0.72593773
 0.72344924 0.72126875 0.7111772  0.71254405 0.72055483 0.70783354
 0.70348172 0.71094569 0.70913522 0.72711055 0.71533877 0.71398797
 0.68453737        nan 0.70376197 0.70156616 0.71948618 0.74190838
 0.72370257 0.71850188 0.72747565 0.68222817 0.70076152 0.69688636
 0.70523297 0.70506846 0.52793056 0.70378893 0.6998211  0.71276109
 0.70621295 0.70497752 0.71993783 0.68996281 0.72841261 0.70543287
 0.71965212        nan 0.73660714 0.73946973 0.71665779 0.69812059
 0.71377089 0.72154021 0.71553357 0.72365895 0.70381622 0.71117198
 0.6963778  0.6875213  0.70853132 0.72268828 0.72104039        nan
 0.68853712 0.70829092 0.72545629 0.68977078 0.71448876        nan
 0.72677204 0.70601794        nan 0.69640753 0.71128252 0.70728783
 0.70547848        nan 0.6999328  0.71096678 0.67591329 0.72415151
 0.71990145 0.71653274 0.71229836 0.69032764 0.71378567 0.71912851
 0.70407347 0.71011684 0.69420703 0.7157653  0.694979   0.69632015
 0.71579267 0.69651817 0.71471618 0.70064289 0.71183287 0.7260225
 0.70032224 0.71807776 0.71202664 0.71932334]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 3667.923 seconds
Cross-validation score: 0.7419083801010167
Test score: 0.667447306791569
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 800, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.4}
299.0264628373334
38943.587914318225
76054.76241188397
8167.549209411765
5540.08702773157
147.85354889804148
77.1608065560431
81.19488133257255
49.40300389634649
151.02190061612055
20.716318186357967
3.2051361800695304
1950.2041394393164
320.5426308044698
1303.6399282651837
0.0
102.3350177386892
0.0
238.35776105539117
24.565176251460798
2605.2407881815307
72.78183982874907
858.6281430171148
447.9642789365462
0.09201103879604489
4.461606751196086
75.1293819963903
38.01639820885612
24.33875283692032
14.798532011918724
25.006518421927467
10.837656438350677
99.31505238439422
158.70413576130522
2.732180981307465
992.4626505543711
123.13362058217172
51.068563181950594
30.309185741491092
45.22897069447208
12.042528035606665
0.0
0.0
0.0
367.4918606847641
122.96824072305753
339.1218202910677
47.00638297983096
118.94025527965277
239.8185742359492
194.1867600347614
55.128423499147175
33.59687979442242
41.2308192585333
235.39346556551754
398.80493460000434
0.0
0.0
0.0
124.62871480339527
47.346751603865414
117.26396601779561
0.09401720040477812
63.36380964387354
28.27775334470789
12.328980037964357
1265.272588009102
73.16235595675936
190.51056487925234
221.4544271418199
49.267415510636056
9.889183897863404
3.1161173156287987
105.09767167334212
16.78318643950479
6.085376128772623
566.0977802414855
35.7002297451254
48.620555101224454
219.48248815858096
15.739585805713432
84.61041159291926
71.69279682918568
534.7896423211714
10.796287799770653
7.794232274463866
19.64600983618584
42.871856123209
18.832365413938533
167.99913196539273
31.65330109957722
50.582403432024876
86.10066141012794
6.329348359926371
86.79907256794831
98.99527345875686
19.990654520202952
280.6387423552078
76.78737118153367
19.642103359103203
4.412243620288791
266.5804863366866
209.8807795715038
232.87725032377057
34.56327711394988
223.92970498735667
13.003959786088672
174.84826955216704
307.97097090043826
111.46633108137758
189.0411413575057
403.7807524841337
3.689513851859374
17.807660157966893
9.265405245620059
89.4837641934937
156.5375229581914
7.13367532484699
44.707744103154255
22.073524173451005
37.87699851549405
0.0
32.94589000179258
94.31311148263194
0.0
92.10550633499224
54.24264446502639
0.0
0.02152069183648564
205.0465997628562
0.0
168.72689593174437
116.43301378349133
0.0
405.9043767087933
9.328118712553987
0.0
3.6318454878928605
164.36592447447765
0.0
405.1052077178101
1.8879198111535516
40.68154879978829
9.477886710199527
73.03146398725221
8.277914549893467
165.83229802455753
12.935677157714963
180.12167568285076
51.29368759888166
195.8095591039746
100.41694450036448
5.885392791678896
127.7634598923214
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996582   0.626374  0.678571  0.651429  0.667447  0.636161   

   Average Precision  
0           0.426552  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.71161994 0.66777142 0.6689152  0.67850578        nan        nan
 0.67150977 0.70389022 0.64320533 0.69283759 0.70128998 0.66367047
        nan 0.66791726 0.67971665 0.68693189        nan 0.67594606
 0.68939766 0.6940021  0.69603748        nan 0.65229745 0.67105724
 0.69390237 0.68800724 0.70659792 0.68252057 0.68557819 0.68912323
 0.67729721        nan 0.69162243 0.67384959 0.69250267 0.67960391
 0.66303829 0.70384068 0.6960585  0.69312522 0.68106021 0.69948799
 0.67920663 0.67901262 0.67154684 0.67582858        nan 0.68968488
 0.68438122 0.69088416 0.68021929 0.72217703 0.68437586 0.68653099
 0.65664527 0.70561993 0.66785315 0.68205117        nan 0.68924919
 0.69501542 0.65456531 0.68587626 0.64677415 0.70050075 0.68797961
 0.6882188  0.65778524 0.68376823 0.68750557 0.68023448 0.69596503
 0.68661147        nan 0.67245126 0.6795322         nan        nan
 0.67613575 0.69744245 0.69658786 0.6761999  0.69157578        nan
 0.69029582        nan 0.66202374 0.63207959 0.70937176 0.69434544
 0.66976049 0.67560291 0.65789452 0.69139146 0.67367784 0.68797923
 0.70788493 0.68217466 0.66644204 0.67583818]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 3167.100 seconds
Cross-validation score: 0.7221770325468807
Test score: 0.6995412844036697
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 200, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.2}
4084.3853324502707
59660.16604569554
128859.65449908376
10240.280052274466
16305.744093805552
187.6213056743145
99.94259941577911
98.92743530869484
559.2998306453228
7.51987612247467
445.5726578235626
4.4490620493888855
4942.403762355447
4116.829931378365
175.68855023384094
0.0
759.6848427653313
0.0
232.4499707520008
18.28946018218994
3448.7753134071827
1210.7902640998363
3578.003883242607
926.6132224202156
49.202912241220474
33.395391672849655
128.9000340104103
367.1349256336689
5.577449798583984
274.95410346984863
3.5510900020599365
73.83982628583908
84.64825561642647
170.41287672519684
19.797841101884842
393.29472041130066
1030.5310308933258
49.81501525640488
67.04141688346863
283.7346841096878
63.57965908944607
0.0
0.0
0.0
337.24325662851334
9.713462054729462
270.9228502511978
187.18163987994194
306.39882984757423
207.2231318950653
24.04716783761978
98.94363009929657
85.43714034557343
659.3928361535072
0.0
992.9718721807003
0.0
0.0
0.0
45.97042143344879
430.9616858959198
68.10198903083801
3.6052498817443848
849.4471701979637
89.46708559989929
16.56379008293152
1059.920909166336
192.72797292470932
779.4500454366207
420.57332104444504
62.992401123046875
12.934908032417297
47.58852952718735
37.021878719329834
276.35899448394775
838.8621368408203
1347.3467844128609
5.169881045818329
53.93312928080559
766.358773380518
26.57169669866562
135.94471517205238
238.83449366688728
1633.3958152383566
59.02701526880264
59.788588523864746
63.82902902364731
200.1933358311653
99.18405818939209
40.66391760110855
98.33558690547943
660.0418442189693
678.5567988455296
146.93652069568634
628.6931543946266
301.4717297554016
158.1913604736328
1485.9974108040333
553.8765533864498
13.765933901071548
157.00713205337524
272.06653133034706
549.5295394957066
496.82215118408203
37.006941080093384
756.3002423942089
132.1079759001732
357.6714571714401
381.25867772102356
144.43118008971214
50.6963005065918
218.5178416967392
66.98809945583344
17.517655164003372
48.752899408340454
272.020765542984
90.21655210852623
1.5623899698257446
10.061402976512909
45.03190040588379
123.1191974580288
0.0
265.41839426755905
314.39479529857635
0.0
71.6231977045536
19.68308076262474
0.0
70.68171221017838
1331.0055478215218
0.0
97.16954508423805
292.1912094950676
0.0
101.85642465949059
91.48086261749268
0.0
17.25774571299553
985.9798408448696
0.0
44.486821860075
25.52080076932907
142.99379533529282
101.81520110368729
125.08837741613388
294.84352791309357
559.9221000671387
175.71664607524872
251.5925399363041
460.87498128414154
931.2024955749512
44.1133394241333
264.9004217684269
236.03019124269485
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision   Recall        F1        F2      F0.5  \
0  0.996526       0.61  0.72619  0.663043  0.699541  0.630165   

   Average Precision  
0           0.444265  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.69300374 0.69089025 0.68968283 0.71039756 0.67853765 0.68029048
 0.69123892 0.68902407 0.69092311 0.70021091 0.72307204 0.69640005
 0.69550252 0.62455399 0.68364811 0.70880931 0.70545679        nan
 0.71737707 0.69490115 0.70447606 0.71216552 0.6999001  0.69319909
 0.70546823 0.70938066 0.72121317 0.70508879 0.69437003 0.71950285
 0.71008234        nan 0.69624974        nan 0.70532557 0.6878007
        nan 0.6888072  0.69050355        nan 0.70265077 0.696246
 0.71770482 0.72059009 0.69741939 0.69870402 0.70584815 0.67779534
 0.72594199 0.69042209 0.71371301 0.68867173        nan 0.71217089
 0.71061257 0.70887487 0.68923674 0.69152002 0.67662859 0.69442719
 0.68647606 0.70897101        nan 0.67528121 0.70751294 0.6937481
 0.71072615 0.7077517  0.70144956 0.71440874 0.68008437 0.70741736
 0.70636995        nan 0.70159612 0.71385202        nan 0.72607765
 0.70843354 0.71930666        nan 0.67889398 0.69435967        nan
 0.70474022 0.69898826 0.70119386 0.70745842 0.71700784 0.70892996
 0.69560656 0.67864245 0.68654633 0.7148019  0.7131073  0.69600296
 0.7013066  0.70878244 0.68647298 0.71895956]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 3156.541 seconds
Cross-validation score: 0.7260776474887504
Test score: 0.7272727272727273
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 300, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.3}
1638.224349864875
49732.53220175055
85639.50072572217
7553.48758437403
7490.445227773045
173.62916074530222
79.02523304335773
60.902646405855194
60.4698401640635
44.18955060420558
100.79691610019654
5.131610919488594
3661.270875859889
4194.198654609965
266.55004128289875
0.0
1251.9434998829383
0.0
49.99384809518233
22.70533948764205
4827.467106097611
576.1738391758408
3183.709130884381
128.21655261702836
2.030430984683335
122.12309837341309
45.746301334002055
214.84246676182374
25.732363981194794
298.7231848347001
29.716167770326138
8.075349386781454
210.00683956593275
258.1869015202392
23.42563488869928
1146.7664351302665
62.58658546372317
48.27356682205573
32.10284625389613
844.3350650210632
103.4032667602878
0.0
0.0
0.0
105.54596571438015
165.53939216793515
358.22817896376364
155.98852267768234
56.72508422844112
86.42498010897543
10.361416479572654
54.73164927959442
13.588907121447846
618.9668609995861
0.5662869811058044
710.346941226162
0.0
0.0
0.0
89.46721929800697
167.4105597891612
89.15331308811437
0.5669656898826361
279.08322955993935
10.37941921222955
20.574310183525085
698.5599677927094
102.79313691495918
542.6391637413763
253.92305977840442
102.14059007773176
4.895472269505262
15.67138491210062
191.97570771723986
39.01867595873773
0.10348600149154663
935.8256249157712
14.266294649336487
37.114015233004466
634.4505195270758
14.706619056407362
69.65929967630655
215.7083883638261
885.6066125116777
21.816386349964887
169.60611257655546
76.6672588569345
37.48300618585199
17.988329102052376
128.28644025535323
349.0806346714962
666.3420228809118
107.05577465728857
111.27173546655104
73.0839976798743
91.32264695223421
88.14758869772777
512.3604510928271
46.06063342653215
20.027437682729214
120.82756161689758
340.597249395214
742.7914530575508
417.16107931523584
0.08082516165450215
345.17950870189816
112.85886686551385
112.35571337514557
473.3006977001205
74.47967142472044
29.788013949990273
183.6888786670752
3.40884558937978
51.26506351027638
4.463638808578253
75.62950215348974
37.99883697601035
2.330494769848883
186.10819689976051
86.34048290154897
59.190939130610786
0.0
133.32333544408903
164.15864965121727
0.0
43.39211647969205
60.07277242129203
0.0
0.020632300525903702
560.2654267002363
0.0
163.92730766558088
81.51785366260447
0.0
289.28540224721655
0.13336012163199484
0.0
6.9311229032464325
105.75764911726583
0.0
29.44651502475608
100.32361606217455
133.19529841130134
25.015028675785288
37.92466497560963
38.456332015339285
303.31883406976704
22.30021871626377
222.68908314348664
739.259593912866
31.85004173638299
2.47321755066514
193.57852906861808
120.30495617259294
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2  F0.5  Average Precision
0  0.996638   0.615385  0.761905  0.680851  0.727273  0.64           0.469985

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.69397324 0.71398566 0.68442479 0.73728504        nan 0.71209029
 0.71502215 0.67879372 0.70344241 0.73284105 0.71097599 0.6936515
 0.72578779 0.72166921 0.70679128 0.73575091        nan        nan
 0.71973811 0.720491   0.70913566 0.70328148 0.70447586 0.72405132
 0.72108191 0.71294231 0.69080362 0.71882776 0.72359359 0.73605131
 0.70981078 0.70383377 0.70648086 0.72161197 0.6972768         nan
 0.69027162 0.70116063 0.70982389 0.6688799  0.708019          nan
 0.71412664 0.70855151 0.71419654 0.67968441 0.70076922        nan
 0.70831332 0.67923007 0.68937741 0.74246725 0.72117755 0.67578829
 0.72103617 0.71510713 0.70910982 0.7160536  0.72646248 0.70474471
 0.74124928        nan 0.67348287 0.70390392 0.74198451 0.70731945
        nan 0.7051431  0.72205948        nan 0.70109575 0.71216709
 0.69574586 0.69060344 0.72266746 0.69535295 0.69150563        nan
        nan 0.70428382 0.71731725 0.70104451 0.70544919 0.71382448
 0.71215123 0.7168876  0.69851924 0.69521885 0.70697309 0.72193842
 0.70154649        nan 0.69106611 0.72898779 0.70603434 0.69555396
 0.65016146 0.70122999 0.71870068 0.68969226]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 3112.346 seconds
Cross-validation score: 0.742467245180051
Test score: 0.746606334841629
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 200, 'classifier__max_bin': 50, 'classifier__learning_rate': 0.3}
761.0034013906843
57739.80917955199
84812.14919121814
4278.74256899266
10628.617270157032
72.96366837329697
96.83302874318906
96.0961812105379
348.2116854118649
6.4154199762269855
89.43038419249933
0.6748449626029469
1387.1883884069102
2692.977812924335
201.22885530383792
0.0
219.26637349376688
0.0
38.87665238918271
40.70365000329912
2950.4593857889995
470.3260759280529
3213.0396220407565
548.9555570866796
366.5668644887628
22.82669407367939
59.76481773506384
33.26028127496829
0.49732647952623665
34.063043464906514
0.12952983297873288
0.4459905184339732
32.34051164088305
99.1032443785225
48.305557331390446
1713.0391601168667
507.63245502114296
45.262334632046986
20.61609130637953
283.8758745309897
4.292039916181238
0.0
0.0
0.0
97.94292087035137
50.53696396155283
295.64482879967545
5.421965195215307
121.54853704827838
114.6383238218259
65.66402173042297
0.38844330608844757
92.47239640005864
777.429898099188
7.3251844071783125
222.060424083611
0.0
0.0
0.0
45.984279297699686
240.20323043200187
49.49846593331313
1.9973202242981642
526.4246639354387
6.916192897188012
16.915366085944697
2060.068309079681
26.373673402471468
366.7978677350329
150.58815500204219
22.382783562236
45.35701746516861
182.07813571562292
41.72143398434855
92.59430053271353
36.90905876504257
560.5492940623953
0.1533953808248043
90.17432313633617
511.6070112173911
4.552754363743588
96.89112131897127
135.97768938649097
340.00911463962984
90.96224775485462
72.15293858386576
99.55169358849525
75.79016136517748
7.954080532887019
97.68517265713308
204.66323122021277
499.49508807313396
130.11652960214997
39.14095524139702
461.91798972088145
138.5433628516039
211.08617596694967
335.02881135360803
269.6766637179535
85.32187718898058
51.43776934884954
303.27143517055083
299.9355767284287
164.9798859123839
10.257271216716617
120.88957910781028
377.31599203369115
68.17075064143864
156.21590953780105
298.2652159403078
320.6693067594897
129.62258324865252
258.42401066073216
121.50046104445937
18.19181946106255
176.3066647837113
238.73568116588285
0.08677099645137787
3.6643827646039426
25.414728128176648
143.9475696598529
0.0
105.66346787440125
218.43693635979434
0.0
207.68494935531635
31.133820083283354
0.0
6.361625414341688
635.3796396145481
0.0
191.44995166336594
270.4354647800792
0.0
100.78796135578887
25.76137277064845
0.0
0.6867857666802593
655.6270398295019
0.0
40.866379881685134
22.173190013039857
58.27674843146815
12.994644370337483
118.51744584162952
46.994940193515504
307.548784289218
33.04659283766523
157.90508504910395
150.67226846865378
293.8197801799979
1.7649909853935242
272.3476547818864
39.2112545199052
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99675   0.622642  0.785714  0.694737  0.746606  0.649606   

   Average Precision  
0           0.490227  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70771217 0.69794997 0.67501212 0.66543105 0.67500472 0.68270308
 0.68294188        nan 0.69787129 0.69594225 0.69402139 0.68803736
 0.69424316 0.70436324 0.70663815 0.69059639 0.69436236        nan
 0.6811601  0.72135577 0.69758568 0.68537043 0.68546296 0.67801462
        nan        nan 0.71785642 0.64758583 0.69971665 0.68381287
 0.66151604 0.68865414 0.66045941 0.71515586 0.69899885 0.68854912
        nan 0.71018764 0.70773924 0.70048312 0.67917462 0.69372521
 0.68917195 0.72959188        nan 0.69198347 0.66636348 0.69571455
 0.69502034 0.70956029 0.68111474 0.71582475 0.69480383 0.70382241
 0.68743957 0.6769046  0.70784007 0.70768928 0.69621646 0.7165571
        nan        nan 0.67429819 0.55066027 0.69800363 0.6800987
 0.68817077 0.68296883 0.71922666 0.68408783 0.70090228 0.69508606
 0.68225685 0.69719885 0.6983989  0.70168101 0.70378105 0.68803682
 0.68709701 0.67308192 0.68862973 0.6924641  0.68498894 0.69015172
 0.68505919        nan 0.69014107 0.70408336 0.70466741 0.68228192
 0.68981929 0.68337371 0.67515118 0.70293695 0.70252219 0.68312528
 0.71371558 0.70548458 0.70932387 0.67120286]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 3346.094 seconds
Cross-validation score: 0.7295918829048528
Test score: 0.6864988558352403
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 100, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.1}
971.2874864935875
131155.85730816796
249994.42442807555
18190.21811656654
13846.435441106558
106.95543336868286
82.25402149558067
379.332712829113
122.15895074605942
468.50101132690907
621.8553628008813
11.526749849319458
8520.920590519905
16285.95875659585
1000.7432037368417
0.0
1358.3794133663177
0.0
123.08058974146843
54.77614462375641
9456.184713721275
885.0604391098022
6653.025163114071
295.8690382242203
56.66018480062485
0.0
105.7446710318327
351.1819884777069
138.16205874085426
582.094722867012
106.15813076496124
24.559290409088135
57.95091950893402
523.2624468207359
254.0731633901596
970.3367940187454
1878.7700081467628
47.21549206972122
141.16657441854477
465.8075931072235
209.4903077185154
0.0
0.0
0.0
477.1620901823044
732.4068468809128
668.2504792511463
224.2704886123538
335.95369243621826
455.75476029515266
36.680532202124596
119.61613059043884
92.41505658626556
4194.5430635511875
13.503399848937988
482.7702273130417
0.0
0.0
0.0
128.31634539365768
474.46204018592834
99.56334710121155
38.478729486465454
1813.2413275837898
73.63442069292068
62.21721076965332
2229.570556342602
750.095179438591
939.8388584852219
276.74807047843933
56.89521485567093
380.3803083896637
109.11117708683014
111.93730890750885
216.14302957057953
54.23246902227402
1374.8507411777973
428.87476617097855
110.28124749660492
1117.8525087535381
152.27093821763992
298.80718529224396
861.1206196546555
2035.8194749355316
105.70692449808121
30.83895805478096
138.02242404222488
310.12368154525757
70.61732402443886
329.8421280384064
201.52264422178268
762.621729850769
788.4749230146408
360.63645958155394
1416.7205488979816
575.2167028784752
2171.058388352394
1961.9830736219883
579.0498377084732
99.52018159627914
43.441160917282104
934.1435128450394
1088.6866402029991
1335.3368279337883
32.276365518569946
413.8821397423744
547.5229454040527
492.93384701013565
842.3352715373039
504.3199373781681
37.71779012680054
597.2101557552814
46.799089670181274
639.9617919325829
28.4921897649765
154.51166987419128
466.0909999012947
10.97159993648529
290.75556367635727
113.94240987300873
326.7636070251465
0.0
318.99171257019043
315.34273558855057
0.0
151.5179431438446
65.98455952107906
0.0
248.4338712990284
1774.7209210768342
0.0
539.5491872578859
619.0138121843338
0.0
348.07608050107956
118.97274088859558
0.0
256.3902460336685
254.48198622465134
0.0
486.0394756793976
62.257179260253906
419.8918533921242
184.7533459663391
35.36687123775482
184.62887454032898
510.55937230587006
98.63822656869888
607.4723410606384
261.5487691164017
138.4814173579216
215.34010136127472
564.4566892385483
767.627523124218
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996358   0.594059  0.714286  0.648649  0.686499  0.614754   

   Average Precision  
0           0.425673  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.71218961 0.70536279        nan 0.71541386 0.72767424 0.71111178
 0.73198481        nan 0.68494858 0.70743731        nan 0.70917302
        nan        nan 0.70035361 0.71807012 0.70801008 0.7438383
 0.71409005 0.71387857        nan        nan 0.73720298 0.7222763
 0.69926386 0.72647947 0.72372083 0.72131546 0.72323715 0.71994459
 0.7394589  0.7096777  0.72536396 0.71182469 0.71082378 0.72634921
 0.69829914 0.64098011 0.74080433 0.71950121 0.71324526 0.72303455
 0.70886464 0.72226372 0.48176355 0.72754596 0.7142403  0.7113577
 0.70980973        nan 0.72645304 0.7315089         nan 0.73764356
 0.72714569 0.70236066 0.70883402 0.71372248 0.72118803 0.7193336
 0.70912733 0.69441889        nan 0.71918914 0.7149162  0.72170451
 0.71899733 0.69178793 0.73970679 0.69938867 0.71625806 0.71046132
        nan 0.45146351 0.70150531        nan 0.70001163 0.69823439
 0.70136499 0.71098346 0.71254253        nan 0.725142   0.72025135
 0.71824454 0.70886922 0.71223902 0.70977911 0.70448894 0.71297228
 0.72503959 0.63920463 0.73872771 0.70353416 0.7202441         nan
 0.71343136 0.72542972 0.69395394 0.68455405]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 3091.716 seconds
Cross-validation score: 0.743838304485755
Test score: 0.6220657276995305
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 700, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.6}
1093.592282006597
22916.259503368274
53294.881938989056
6402.9410714919395
4162.721111120911
89.38548875431297
68.10400881687747
258.1930776257359
52.64837551904202
2.1475787093841063
71.53226288408041
10.489174458474736
1275.8516503985084
5211.134994565575
193.65865629687323
0.0
323.51600112440065
0.0
100.42128172372031
2.404117385391146
2276.4046482995036
2145.3962225836585
2259.7727478779925
5.137206411163788
5.513226193594164
0.0026912700850516558
33.226880040136166
88.04601579597511
1.0915913135395385
291.95181965170195
0.003305109916254878
10.280000915256096
16.619293733278028
524.8605332146544
45.50810219253617
178.80102243402507
155.88936813602413
31.90206054887676
21.70739455810326
135.6506358869956
48.34597506893624
0.0
0.0
0.0
218.14669073029654
86.8851938283915
65.82620561181102
27.403290457936237
40.206092754146084
35.08855371728714
1.8050759466932504
37.99749725090078
22.242084156605415
255.06265511885795
0.0
138.59586632618448
0.0
0.0
0.0
20.821257426650845
31.581866620646906
154.8363959954877
1.9199653853138443
139.60268984111462
48.88506481068907
0.001575991016579792
109.9024789312025
10.41700841407146
147.01809465208498
70.67155063233804
47.82633540603274
0.004223812109557912
4.661434817498957
28.73356439123745
16.393598433365696
44.22049528376374
1483.9216034355777
22.542537232217
36.742420076996495
161.1289148531796
67.35911494516768
48.24877208592079
10.837877737918461
544.2663055670855
18.026081367235747
1.2646161635930184
19.571206660490134
246.0747715916077
0.0549576606717892
79.76716400821897
63.0294312231963
238.70886678905663
4.1483616386431095
8.219182409637142
54.87826611053606
220.54238818430167
61.13552873735898
271.6496054332383
6.401545527842245
76.55788728575862
108.97328266651311
135.18190592875908
361.78242938491167
270.29561451499103
4.219434730272042
6.543201880587731
20.81776437618828
69.53076410272479
80.4600805950322
56.051675900351256
0.09756268002092838
220.53768746922287
7.349281170128961
333.4034658577584
0.017861427812022157
11.58248485103104
14.514182043909386
0.0012961899628862739
17.698693627957255
2.114391854556743
12.004309721400205
0.0
61.38448403737857
47.37207261491858
0.0
60.78553415508577
23.33471482724417
0.0
0.06815309936064295
169.830183464539
0.0
34.622102925037325
127.48916274827934
0.0
15.837169603997609
5.945551416865783
0.0
0.39608043734915555
121.76030241949775
0.0
16.906312333638198
61.68369723448268
55.795293988609046
10.039611061525648
54.61783881453812
14.534740412054816
102.80754851976235
116.77332202836988
42.310385541313735
664.8309498948292
9.42260348983109
25.46388930082321
17.921383127613808
89.26530994626955
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99619   0.588889  0.630952  0.609195  0.622066  0.596847   

   Average Precision  
0           0.373298  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68864937 0.68589716 0.67254926 0.66458237        nan 0.69171705
 0.67488045 0.66294063 0.6839196  0.66765983 0.64328076        nan
 0.65695536 0.67266207 0.68449959 0.67675048        nan 0.66740065
 0.65581325        nan 0.6655154  0.6823009  0.67606035 0.70189277
 0.68127938 0.66864989        nan 0.67797914 0.68552799        nan
 0.66759574        nan 0.70185506 0.68501921 0.67796748 0.67961567
 0.66920814 0.65435596 0.69597081 0.65973597 0.69806105 0.66338932
 0.67286504 0.68983428        nan 0.67410853 0.69272563 0.67528995
        nan 0.66192386 0.67593553 0.66536043 0.68443996 0.53719694
 0.67319813 0.68002762 0.60563751 0.68820688 0.67861191 0.70616783
 0.6574389  0.68405917 0.67817126 0.67009189 0.69022944 0.6777552
 0.67967109        nan 0.67869437 0.67147184 0.68753006 0.67985106
 0.67523348 0.6601806  0.68734196 0.7135706  0.67314542 0.46001356
 0.65518467 0.66914209 0.68380307 0.70286613 0.70890329 0.66483644
 0.68955336 0.69108226 0.65772188 0.6743948  0.69672284 0.62587564
        nan 0.67365334 0.67521547 0.66097132 0.670259   0.64555838
 0.69063157 0.68321634 0.69316904 0.69943827]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 3055.140 seconds
Cross-validation score: 0.7135705958269687
Test score: 0.7568807339449541
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 600, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.3}
639.1702614238166
43305.155269132534
97347.41002699634
5375.642274763857
5512.245215748211
261.5126391458034
367.9314695214998
144.97417201209464
241.93150413892
166.54292382617132
154.29152092008735
99.50160241717822
1797.0338015809539
8494.83531325472
506.1021898621177
0.0
210.04849822782853
0.0
112.09532866082736
16.45144002897723
2100.9442607742385
214.13299734235625
4203.735355122364
709.2741403214986
6.1234447407478
23.063489029038465
161.99979330991482
87.41239961838437
34.37017429883417
1.6224552542844322
36.86591239160043
0.9542378313199151
163.21042444939667
216.30393579525116
3.378658571615233
551.0738421250426
332.1001850811881
25.021343191925553
89.58407770853955
57.52143365972006
89.66043137123052
0.0
0.0
0.0
425.24237962942425
143.5733518376801
492.3497280556621
77.64995863466174
95.33157405380916
19.897633187283645
141.57177881158714
35.39305528344994
38.019160538333324
307.70375980871177
221.72138092201203
269.4663004568429
0.0
0.0
0.0
164.1345674419572
421.4697734143847
97.16611838740937
20.37317519747012
248.43436497276707
42.609035113986465
63.19724041380687
1250.5422734666208
167.13854742588592
313.63754827879893
80.06143032305408
75.77423262887169
1.0120061386551307
55.83969645306934
51.575456066508195
5.676643050450366
408.1246237137384
1073.172291938754
30.470742100224015
53.11365308723907
687.8059708120854
95.09815629197692
87.89168419118505
399.7926598537888
346.276977785863
64.57408782601124
41.409656714204175
83.996043521518
371.6110192995242
31.048741136561148
19.62485539948102
60.79513553362631
267.2022693786348
111.92874215339543
275.6666908971092
441.65438497402647
16.10149453466147
19.72020401796908
330.51005926565995
94.45540617809456
73.31527459564677
122.53473023904371
223.08578938270512
311.37559558621433
92.14952539431397
11.540021894470556
164.46964218784706
361.7478099463333
122.25458140278351
155.01981191665254
168.8130835223419
8.840233020950109
96.76054718189698
324.43354008295137
100.63889330964594
3.5716248444805387
53.624304541925085
158.13155412612832
13.062290611327626
60.14212501066504
26.794176103314385
76.02901094690606
0.0
197.68800046146498
649.4090931962419
0.0
54.14562043304613
72.9756706457847
0.0
22.985932128038257
241.74538606693386
0.0
274.7672472533741
216.26567121765402
0.0
72.63406346396368
1.3968315274687484
0.0
11.69606526917778
213.75996022391337
0.0
56.29259607309359
35.94116706139175
202.01051483999618
32.95652123194304
51.06842850300018
39.99207749962807
153.19267503306037
2.0603333615290467
204.2958748677429
656.7865889649838
61.84530243938207
85.06532090256223
366.0331603635277
231.23559027603187
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997086       0.66  0.785714  0.717391  0.756881  0.681818   

   Average Precision  
0            0.51958  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.70126181 0.67949381 0.69909469 0.69161796 0.6926027
 0.68753436 0.7067082  0.69623187 0.67817951 0.69579135 0.71134834
 0.69426286 0.69232239 0.69193595 0.71251187 0.68150063 0.71892918
 0.67559471 0.67600739        nan 0.70633584 0.70869619 0.69675957
 0.70356736 0.68378875 0.69032026 0.68151791 0.70901678 0.68790768
 0.69876922 0.70400845        nan 0.70810511 0.69575257 0.71540165
 0.68405782 0.67650394 0.71181439 0.68216307 0.69165372 0.67525332
 0.69143543 0.70415592        nan 0.68614095 0.67076166 0.6907379
 0.71292531 0.69986579 0.70461493 0.68853365        nan 0.68749961
 0.68164387 0.70226467 0.68762793 0.67862378 0.67649041 0.68678739
 0.68560854 0.68265296 0.66115019 0.70493464 0.70801717 0.68624296
 0.67996439 0.68438285 0.6959006  0.66358716 0.70825941        nan
 0.69906422 0.69726318 0.68376195 0.69299333 0.66285373 0.67119669
 0.69334761 0.67437864 0.68850423 0.70482438 0.69388507 0.68357902
 0.67777135 0.69305946        nan 0.67087739 0.68575171 0.6906365
        nan 0.70624497 0.69601761 0.69174731 0.69459248 0.71234023
 0.66154274        nan 0.70857616 0.67145249]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 3245.405 seconds
Cross-validation score: 0.7189291830717466
Test score: 0.7603686635944701
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': 400, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.2}
1279.615401298739
77556.0842593601
121304.29628536291
8818.586007429287
11522.524166543968
140.29836437571794
1041.0267278552055
39.68658720608801
73.6811864823103
51.95907970890403
289.56521427910775
5.190055754035711
4549.536396466196
2400.7435111496598
812.8096073195338
0.0
924.9820347400382
0.0
67.93015277013183
36.189263368956745
3408.7275485601276
1272.1799558484927
4100.4599146666005
50.7146286489442
15.448961632326245
21.11492013093084
70.84122954402119
971.1713294871151
7.930010914802551
21.06611137278378
16.120712906122208
13.366298101842403
2.8000993840396404
185.3421580279246
119.51963203027844
2143.640123689547
345.9585470156744
112.77237209584564
76.89154524169862
149.22851451020688
115.16517474129796
0.0
0.0
0.0
362.7848269203678
353.93230785802007
512.9718422051519
199.37788353022188
343.1579020181671
163.56190260127187
71.24258009158075
98.64470632746816
103.6475460845977
325.884895815514
106.90780258178711
299.70796985551715
0.0
0.0
0.0
463.1665302757174
348.4622831074521
164.77373741101474
0.05689670145511627
795.3596372148022
105.27581730205566
97.49174227192998
3312.3622204586864
272.6602915134281
1014.3246882064268
149.66223804000765
213.48286312259734
118.88753521069884
45.67467201128602
455.2871765354648
28.990145803429186
26.243476539850235
818.7520113382488
8.982070494443178
40.35395147278905
573.9669193280861
28.78674405440688
610.0852091759443
139.12634898815304
1078.2501569576561
115.63054477330297
25.921332988888025
104.77423082571477
53.47107269242406
34.871552826836705
19.255898363888264
54.67580111604184
800.3597408132628
619.1520711388439
145.23895545303822
210.0472466237843
70.00353424157947
344.6670087389648
750.9570804545656
220.28456399962306
17.703881966881454
110.27567170187831
206.62816468253732
1283.8006599657238
345.9752546530217
9.929369397461414
253.19776641204953
201.352090225555
527.1654671542346
282.50448734872043
99.26011249236763
667.7262804955244
89.25971447210759
157.6120828539133
37.8580974098295
16.149325013160706
118.96604357659817
109.43374780938029
11.14005489461124
136.1660077655688
49.33830406609923
197.67312087398022
0.0
130.44771146122366
129.65569230169058
0.0
189.2795193111524
111.31493492983282
0.0
78.83416219055653
979.2368394965306
0.0
293.8939855620265
276.59333039913327
0.0
91.95981078222394
60.378485053777695
0.0
185.14508814364672
715.7050582962111
0.0
26.718782633543015
125.35779305268079
214.71237429976463
61.42530308570713
126.0520970467478
160.07211543619633
902.2440037634224
51.80798772722483
217.43599329330027
100.32093734294176
114.31252826936543
70.15057965368032
422.49184331297874
293.3941021403298
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.997198   0.673469  0.785714  0.725275  0.760369  0.693277   

   Average Precision  
0           0.530163  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68912656 0.68198284 0.69928484 0.66187632 0.68186588 0.67540455
 0.67208815 0.68828076 0.68651255 0.65285202 0.67396778 0.6823932
 0.64245232 0.67175643        nan 0.66977004 0.6840353  0.68080787
 0.69064721 0.66103442 0.67507197 0.68057412        nan 0.67877587
 0.67923127 0.6840675         nan 0.66402125 0.68033833 0.68240016
 0.66689161 0.68553568 0.68913676 0.68089919 0.59986175 0.66289392
 0.68770214 0.6817797  0.67244172 0.67244302 0.68222025 0.64724765
 0.69030394        nan 0.69102216 0.63048171 0.6832043  0.6267512
 0.67418116        nan 0.68956503 0.6898559  0.66564425 0.69020144
 0.65169622 0.67536262 0.68332614 0.66483155 0.68809467 0.68942342
 0.66937365 0.68737472 0.65821926 0.68254597 0.64996424 0.67519837
 0.66743581        nan 0.69521193        nan 0.67251303 0.68254231
 0.6744695  0.65706406 0.70623925 0.70888227 0.67571411 0.67773557
 0.67778056 0.65697068 0.68671837 0.66355217 0.70701297        nan
 0.69243953 0.68939519 0.68145941 0.6696858  0.65557955        nan
 0.68170365 0.67408839 0.69869508 0.66642785 0.651077   0.69780906
 0.70279457 0.68207892 0.67011412        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 3223.246 seconds
Cross-validation score: 0.7088822715352615
Test score: 0.7175398633257403
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 100, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.1}
1113.0834940869877
117693.88327762522
247695.93795362464
17193.467354529465
15321.331155601787
171.72145708053176
169.9482807694003
254.8459675774211
101.73169962503016
149.98204972525127
307.5280125439167
22.414553639362566
8916.234817791032
24168.158284949615
962.5395840298152
0.0
1877.1188390799798
0.0
403.5931451157667
81.46102391404565
8501.213270795706
8882.869702004224
6680.612138768047
451.0599986643392
44.03148028276337
19.816300694132224
112.28886773949489
111.32958736782894
34.40932312564837
839.4294360681342
44.7242623870261
80.62106379121542
94.18534695915878
757.795247080212
81.44353860616684
1001.5675077624619
2037.3465716456994
74.61601865384728
195.46164453882375
628.9153812484583
108.95853294432163
0.0
0.0
0.0
416.8067975976446
336.8019438544288
412.6790245367058
274.2312518185936
706.205019609828
357.1654574959539
64.47926186397672
157.53098964691162
40.01409704145044
1955.734151687473
0.0
2466.1363959741284
0.0
0.0
0.0
134.97536729786952
1848.7142098381883
83.33655504859053
52.9783505871892
571.4575310731016
179.47449522465467
76.12807353238168
935.3333939315344
537.307581793284
813.1319723464549
563.3835445046425
876.8722264766693
31.29766297340393
103.05485995532945
321.61011376883835
161.19359584210906
145.06075267115375
1342.656267442179
148.6608351394534
130.47337593097473
2546.4107560412776
31.47713572412613
237.09764594084118
435.0171164678177
2493.9136421883013
106.07894667051733
36.25046386779286
94.24785429224721
128.53555169398896
26.309781789779663
444.16431305307924
206.52909870321673
1294.9233742882498
481.11048600902177
156.8300566560356
338.86403907000204
1348.4797017354285
253.84187030378962
1603.6778866704553
224.28981387615204
112.34198688098695
239.36803970555775
431.3920938167721
725.8061844431795
1175.6618097550236
108.03582775872201
1154.535605617919
187.5104444772005
318.4304826692678
1263.8690185784653
461.73429346014746
96.45140707027167
1015.3448921777308
401.97881676629186
153.73771644895896
18.483324319124222
110.92350910163077
314.957409535069
5.841321973071899
412.8948416262865
18.60827265807893
208.7407055851072
0.0
29.246621758386027
1535.3057507937774
0.0
128.88743314146996
27.20265445113182
0.0
322.420403720811
1328.1381030010525
0.0
613.7799478703673
536.9007820673287
0.0
378.45889161573723
49.98590816184878
0.0
149.03194766130764
1728.0524132624269
0.0
104.77983349561691
68.64718746906146
438.3033187810797
77.31212267110823
332.2375783661546
165.92626434681006
608.9966060342267
58.65316734299995
767.4385811751126
330.65061259269714
433.7487780377851
503.5778718441725
150.75467468239367
78.29216603934765
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall        F1       F2      F0.5  Average Precision
0  0.996582    0.61165    0.75  0.673797  0.71754  0.635081           0.459915

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.67877145 0.69779215 0.69369822 0.68348474        nan 0.68077439
        nan        nan 0.63684651 0.64993454 0.66678127 0.64467763
        nan 0.65536437        nan 0.68463072        nan        nan
 0.68379743 0.68939806        nan 0.68461507 0.69595351 0.70838969
 0.70023604 0.67435506        nan 0.70251817 0.69069848 0.70581773
 0.68897814 0.69344235 0.69498087 0.6877464  0.70344973 0.68857723
 0.67899979 0.68645641 0.68236468 0.69866042 0.66704742 0.68568189
 0.60736692 0.67837276 0.69026966 0.67533963 0.69176155        nan
 0.69400568 0.68564869 0.65088159 0.68030282 0.68038122 0.70632714
 0.69151433 0.66707753 0.67730449 0.53385395        nan        nan
 0.69892201 0.69935793 0.67645851 0.70267355 0.68373698 0.65306801
 0.67168341 0.68863472        nan 0.66528829 0.67358014 0.68338379
 0.65002468 0.67027973 0.69466492 0.6678041  0.64266336 0.65467196
 0.68105164 0.68178994 0.69856127 0.68392267 0.67225818 0.66075409
        nan 0.70174661        nan 0.66888462 0.69767077 0.71584277
 0.69515528        nan 0.70178249 0.70264538 0.66750759 0.68517745
 0.69156687        nan 0.69680669 0.67835927]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 2977.184 seconds
Cross-validation score: 0.7158427653831854
Test score: 0.6979405034324943
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 100, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.1}
1144.484290421009
154916.7736942172
230096.37327337265
14851.5258538723
27187.92596554756
127.94185036420822
27.83109998703003
188.13029968738556
276.1457920074463
355.4817593097687
444.2500855922699
91.25508999824524
6656.028096795082
3404.860591709614
2826.1469442248344
0.0
1664.9409911632538
0.0
104.7504488825798
75.58067810535431
12642.486809968948
1730.3181618452072
5380.891856789589
669.0563259124756
246.99446541070938
2.5736500024795532
69.21936982870102
45.2988845705986
261.23241662979126
346.0267983675003
195.87309765815735
15.79053020477295
36.4446097612381
288.3739379644394
473.3237053155899
1175.074398458004
370.45270216464996
143.48337864875793
119.36722761392593
1650.4772527217865
210.44815009832382
0.0
0.0
0.0
202.83452859520912
478.10231733322144
611.585134267807
224.2531322836876
415.88462221622467
429.0259773135185
159.15505361557007
151.33461874723434
414.0549930334091
984.6853972673416
55.85060119628906
754.8922764062881
0.0
0.0
0.0
248.79761511087418
177.6191766858101
114.35761737823486
5.668179988861084
464.30495631694794
159.65987956523895
39.02789145708084
2435.2863944768906
250.52969670295715
1239.0992904305458
291.35969430208206
453.4350181221962
25.0246000289917
12.746800422668457
278.4263733625412
49.87977933883667
107.13677048683167
1668.812587916851
232.92428743839264
235.02520588040352
1572.1471737027168
73.11490070819855
203.1348661184311
387.4838121533394
2288.9315995573997
321.5600092411041
8.728551864624023
207.0765923857689
93.43332666158676
121.69407653808594
172.30416703224182
224.61571770906448
770.080860376358
370.4756797552109
221.6646265387535
341.9049847126007
280.32013845443726
392.0037849545479
1405.269824564457
396.6554518342018
163.91111254692078
82.5296162366867
1338.7952123880386
1802.8978152275085
626.7479767799377
61.51330065727234
599.0070066452026
378.61741864681244
242.89235895872116
746.7496002912521
221.92352658510208
132.75508856773376
547.93554854393
215.47025215625763
492.9419963359833
25.845840334892273
125.40395221114159
150.576979637146
14.612490057945251
64.32728922367096
20.042634189128876
177.998659491539
0.0
273.2883031964302
684.6730687618256
0.0
187.11572605371475
121.13003551959991
0.0
18.93199920654297
2023.8019315004349
0.0
237.80288118124008
485.2223414480686
0.0
247.73862344026566
117.19978058338165
0.0
86.98078256845474
2006.7462579011917
0.0
72.78765988349915
3449.8917022943497
200.55072498321533
71.57796013355255
296.5375382900238
56.718935400247574
1249.512434631586
72.8376088142395
472.74542981386185
547.9929803609848
275.2875635623932
334.5074883699417
926.7705980539322
512.5589535236359
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision   Recall        F1        F2   F0.5  Average Precision
0   0.99647    0.60396  0.72619  0.659459  0.697941  0.625           0.439879

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70073543 0.69150043 0.70128445 0.71071476 0.7041013  0.71515733
 0.67429593 0.72472631 0.71737759 0.718431   0.72404062        nan
        nan 0.7102864  0.68059709 0.72467515 0.67906929 0.7045193
 0.69004825 0.69778251        nan 0.71908771 0.70550684 0.70600203
 0.7227357  0.70589419 0.71429277 0.72626668 0.70997617 0.71313357
 0.68359565 0.71981775 0.71928147 0.69923139 0.70877465 0.70873627
        nan 0.68713622 0.70055541 0.69040099 0.69986276 0.56574064
 0.71281661 0.73470884 0.70950334 0.69341541 0.71506417 0.71218457
 0.67924458 0.72355972 0.71954773 0.69913359 0.6747403  0.70898158
 0.71921374        nan 0.71841687 0.71811715 0.71073892 0.69160041
 0.70871402 0.71048313 0.73635078        nan 0.68051653 0.70806871
 0.70401491 0.70174751 0.72149617 0.71284308 0.67163394 0.71525545
 0.69111716 0.7069959         nan 0.72691244        nan 0.70408113
 0.72396298        nan 0.7048583  0.707836   0.71858885 0.70635424
 0.71652456 0.6990982  0.70502338 0.69417598 0.73721604 0.71218083
 0.70390279 0.71060795 0.70488569        nan 0.69090543 0.71004095
 0.72017972 0.70970833 0.6995476  0.72422382]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 3354.078 seconds
Cross-validation score: 0.7372160437788916
Test score: 0.735981308411215
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': -1, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.3}
694.5110489178915
54686.01790108229
86594.86893027788
6501.405582366278
7383.1176382568665
266.19659522059374
60.31993444473483
72.91938304202631
91.63884274638258
2.141825212398544
69.18035515025258
4.691878054756671
2330.0327457843814
2714.1339086510707
1057.868906010408
0.0
363.34981906739995
0.0
84.48054899019189
33.90058714454062
5552.99714056449
741.285690522287
2382.7231580750085
81.94885901315138
22.36593707278371
13.792869377415627
47.858708776533604
38.23451153980568
163.06252336502075
61.63722673943266
11.092487089335918
3.7699109613895416
35.891158788930625
220.20391739951447
36.023461749078706
736.8564344840124
210.26906146109104
45.02478277683258
8.028696658555418
561.9916338270996
69.6995796575211
0.0
0.0
0.0
61.26628553075716
206.5778314685449
287.6711721708998
92.06518996227533
256.3960663364269
212.4481543097645
33.34185476414859
15.92883606441319
70.77558428235352
306.5897895414382
153.21200561523438
836.7405762663111
0.0
0.0
0.0
179.9351859721355
266.13976442907006
34.11601855652407
43.838361943140626
272.6186961578205
178.72875573625788
3.0802139788866043
363.3772937702015
136.57675152737647
314.67947558197193
89.82203777367249
18.559505447745323
1.61559197306633
34.81097800284624
77.6609893576242
5.761543294414878
6.050407628528774
1417.1057691304013
26.298082679510117
27.423156298929825
478.87612453033216
68.39343456272036
58.8435201193206
35.75384056009352
913.4583915006369
75.53152878116816
110.39252584567294
71.6835012533702
22.926702904049307
64.84759929403663
67.69814827851951
16.40785866836086
764.2223304482177
218.03628059104085
169.77979912934825
193.7332603293471
50.14801926538348
44.984149476047605
473.0874873478897
330.0499666403048
12.587976694107056
146.1302077108994
222.81625447282568
248.38584863301367
552.685890105553
29.0034607835114
743.0060269031674
127.62252841610461
305.62863676575944
539.9439709186554
227.8876129128039
13.418010355904698
430.26649320777506
6.068308221641928
76.06715179560706
10.042362454347312
70.75441067758948
105.81371114263311
15.876689723460004
30.879200771450996
7.333619496785104
52.99653064389713
0.0
7.5733367362990975
181.936441807542
0.0
98.45768820680678
6.936128073371947
0.0
126.63416389958002
68.79477711580694
0.0
73.38110331306234
116.674679312855
0.0
50.89121037628502
18.81570073403418
0.0
17.0133193875663
221.24720598896965
0.0
49.47270243591629
4.852095738053322
128.64985454222187
114.12964696786366
26.24967776192352
177.14998220838606
175.73752246890217
1.7900413423776627
286.28830466559157
487.92276499513537
134.33485863544047
79.50127194169909
356.41296459408477
259.8236538171768
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall        F1        F2      F0.5  \
0  0.997198   0.684783    0.75  0.715909  0.735981  0.696903   

   Average Precision  
0           0.514764  

--------------------------------------------------------------------

C:\Users\rande\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\model_selection\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70614325 0.64975845 0.65788785 0.67865817 0.67408814 0.68562025
 0.6703932  0.66784806 0.69903836 0.68651799        nan        nan
 0.68442919 0.67405547 0.70200333        nan        nan 0.64973887
 0.66065519 0.67754214        nan 0.66659677 0.68315962 0.68435735
        nan 0.70609326 0.66266395 0.67750095 0.60378015 0.67471372
 0.67283718 0.68631853 0.68729494 0.65750965 0.65757257 0.67614066
 0.6777597         nan 0.6906386  0.70358463 0.68753533        nan
        nan 0.68799543 0.670898   0.67271878 0.66960394 0.66190764
 0.69230048 0.68085895 0.66134547 0.66244182 0.70433352 0.67407317
        nan 0.69276389 0.68785323 0.66802374 0.69504374 0.66329507
 0.65370961 0.68367833 0.67873937        nan 0.66684966 0.66697589
 0.64067213 0.68174932 0.68681871 0.68897998 0.67338127 0.69214712
 0.68451274 0.69069176 0.66267632 0.68169917 0.67537348 0.68255456
 0.67197455 0.66880424 0.68573754 0.69571566 0.67578287 0.68350622
        nan 0.67021761        nan 0.69710367 0.66157225 0.67474706
        nan 0.65901052 0.66803599 0.70109695 0.6948157  0.65189292
 0.66701226 0.66581533 0.68164853 0.6870899 ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 3023.493 seconds
Cross-validation score: 0.7061432499386642
Test score: 0.7517084282460137
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 100, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.4}
2797.4407410770655
33475.48144707084
70055.4429371506
7028.418218817562
6050.897294770926
61.17191766947508
70.35157069191337
576.9530955031514
16.967416003346443
81.50100197643042
14.839003585278988
0.7321738004684448
1618.6699180826545
6258.836154051125
478.2089421376586
0.0
446.62775549292564
0.0
122.79161985963583
10.907046243548393
2427.204695634544
1169.5595183484256
888.7293920665979
6.956805989146233
4.421830056235194
5.733083210885525
1.4064686745405197
656.7203475534916
45.098209381103516
14.774519920349121
2.618712067604065
2.3414169549942017
10.202645599842072
256.7012773565948
658.3305639401078
262.77820059657097
45.032701186835766
10.908525884151459
17.196449225768447
419.4564586505294
84.32313898950815
0.0
0.0
0.0
376.5918378159404
10.44080263376236
284.785951115191
141.38283944129944
277.49002254009247
115.28649250417948
0.07084880024194717
27.328368574380875
205.18552473187447
195.65020863711834
0.0
140.07815730571747
0.0
0.0
0.0
240.14879763498902
484.2804528865963
31.798229653388262
4.772780478000641
398.92715924978256
48.60140371322632
19.57390022277832
1727.4115779101849
440.51512555219233
508.38800965994596
292.5483275577426
232.97101165354252
0.8194102048873901
0.9338169991970062
143.16960760578513
20.12643862515688
1122.72554282099
183.30395819991827
1.0352035015821457
17.822246205061674
381.8506518676877
15.678902745246887
42.88610527664423
126.65306703001261
856.7061493471265
77.4077560082078
11.334061905741692
31.70287289097905
70.86009023338556
2.806847006082535
122.24909850209951
5.808951295912266
360.81416682899
87.28744421899319
9.300553973764181
12.94854311645031
70.94886077567935
121.09723644331098
334.03769497945905
113.53447764739394
107.56873428821564
103.93778073787689
152.06998582929373
568.6716197580099
25.477811194956303
28.7876432929188
743.0533236563206
29.0000893548131
44.39869236573577
69.69540023803711
9.447520218789577
58.40912055969238
594.4017719179392
76.51436614990234
17.587908897548914
15.328944146633148
14.755250927060843
193.64552038162947
9.019234254956245
56.9858295917511
27.19355600886047
2.0995803251862526
0.0
56.24860913679004
233.69881238043308
0.0
16.117823734879494
34.87160654552281
0.0
9.591060057282448
24.451121136546135
0.0
141.2346878796816
91.79771288484335
0.0
322.5829165801406
1.353868030011654
0.0
0.09126880019903183
59.561241772025824
0.0
24.497699577361345
74.680739749223
85.25309217348695
9.634299121797085
8.703025728464127
40.96735844016075
186.37795010209084
13.89171028137207
102.91803523153067
35.173079535365105
435.8036843948066
127.12121448665857
47.68406940251589
24.412033680826426
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996918   0.640777  0.785714  0.705882  0.751708  0.665323   

   Average Precision  
0           0.504476  

--------------------------------------------------------------------
[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 2687.959 seconds
Cross-validation score: 0.7250620620312199
Test score: 0.7906976744186045
Best Hyperparameters: {'classifier__num_leaves': 26, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': 400, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.4}
2196.04749190266
38023.80559979397
69989.34573807556
4492.695073637296
5114.191339832207
30.43745067541022
86.8881873092032
362.9186037030886
199.16209565126337
107.3074195815716
19.30841445119586
5.493609210476279
1528.6567083400441
6770.146967894456
917.5530773010687
0.0
396.6897316031973
0.0
9.792274321254808
2.424567531910725
2534.987934880948
519.9826013748534
1618.1283240508055
13.059701621881686
51.133208651677705
1.6411661081947386
21.51153748203069
4.340046998986509
54.89720399369253
101.13570172688924
1.2345913155004382
4.741114191012457
13.61169760429766
100.26544292224571
5.899968848796561
845.4909528256394
85.48032806348056
33.189689518127125
67.89784502470866
45.727570712333545
57.190785683575086
0.0
0.0
0.0
332.0028234529309
68.45617019059137
326.5018889035564
72.7423327389406
229.7411457124399
81.16773173579713
4.039620041847229
88.32244067883585
75.0844058439834
145.36141018464696
0.0
361.19467220967636
0.0
0.0
0.0
664.3892895480385
90.92719737568405
11.658113378216513
0.10127769224345684
351.332619107794
72.48508119420148
0.0016300500137731433
1634.53252303513
129.34383468760643
298.6213682538946
171.6009139030939
14.473331419809256
115.78174770623446
43.44700954831205
133.33346824214095
26.17874094285071
1.4819057122804224
155.47673777188174
21.826304717425955
54.770291406603064
106.33913063048385
18.38595188438194
139.63946066214703
39.67986963130534
786.1506937877275
25.52675268240273
6.986487038666382
22.074781487812288
230.13903803098947
6.927510662004352
46.86709675309248
167.38599206507206
166.69543739047367
149.73048064555041
120.6914965286851
51.764188589469995
64.67291585350176
8.16057211207226
428.4370548092993
169.42003633687273
77.77447878359817
8.326103205792606
133.58933335979236
522.068658753531
104.73341835511383
40.94713532924652
171.67447112867376
103.61401912308065
177.86937720770948
408.58889570925385
63.786967849358916
205.2860360816121
53.020476304285694
311.5333303587977
24.78880417527398
0.1798518245923333
65.93656324403128
81.5111215361976
5.816909005865455
98.80028501147171
1.0748468989040703
11.78683764062589
0.0
2.4115407909848727
1025.9721528682276
0.0
30.38315401127329
0.962135853304062
0.0
12.653276893193834
144.64362631377298
0.0
225.46060198207851
102.82380023295991
0.0
109.14934413897572
2.5831690281629562
0.0
2.0691116902744398
669.1443911745446
0.0
26.418399014917668
79.54317950527184
89.17144268954871
16.26551098638447
145.74391811713576
55.193160304799676
388.8109186129295
5.062787478789687
161.28683351536165
126.52924117108341
267.54602653882466
95.44820415938739
86.4838668529992
175.53550045192242
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2     F0.5  \
0  0.997647   0.723404  0.809524  0.764045  0.790698  0.73913   

   Average Precision  
0           0.586509  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.69783989 0.69452997 0.68634695        nan 0.6967224  0.69192136
 0.69697506 0.68479258 0.70117506 0.66030793 0.67023521 0.70055456
 0.67517312 0.70027883 0.69360153 0.69618645 0.70854984 0.67916456
 0.70695069 0.69001262        nan 0.7095359  0.71063519 0.68560923
 0.70046165 0.70395914 0.69509809        nan 0.68370189 0.70141237
 0.66436021 0.64343471 0.70286882        nan        nan 0.67105726
 0.69985898 0.70357993 0.68048312 0.6953699  0.68264211 0.71351696
 0.71019272 0.71971068        nan 0.706672   0.69576377        nan
 0.71911777 0.6649405  0.68754827 0.70180185 0.68292015 0.70007135
 0.70548066 0.6835158  0.70086338 0.69230161 0.69704386 0.70069865
        nan 0.70331271 0.67561466 0.55889715 0.67255948        nan
 0.64191339        nan 0.69892567 0.7124202  0.70289948 0.71781999
        nan 0.72964425 0.68137013 0.68289299 0.70214098 0.55311416
 0.67609585 0.70950191 0.6940394  0.70339381 0.69399353        nan
 0.67794119        nan 0.69427916 0.66574836 0.69023858 0.71502991
 0.70518469 0.68423635 0.71145968 0.71044565 0.64744764 0.70187628
 0.70816959 0.70594758 0.70719198 0.69217641]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 2669.607 seconds
Cross-validation score: 0.7296442547320328
Test score: 0.7403189066059226
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 800, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.2}
678.4494953211397
69243.08261930756
138181.26329369005
7808.040230311453
9369.219588970765
129.38318668678403
38.51954735070467
435.7399943321943
922.9961640741676
138.31241655349731
450.09842817671597
102.30117683112621
4014.1278976928443
1107.0139519562945
519.6653215400875
0.0
208.034572401084
0.0
1850.8322916384786
9.398076394572854
4581.6353493742645
704.01605765149
4586.630247035995
38.52722012810409
2.8061312027275562
20.597538702189922
110.5278076166287
39.36790848150849
53.58715057373047
197.43451007828116
182.0316011440009
89.76729199290276
28.310300215147436
332.5711795631796
473.8437905823812
1103.4100863104686
228.31999071501195
293.725580425933
38.31159529089928
468.1625898052007
183.73007400520146
0.0
0.0
0.0
57.652285072021186
44.15534842014313
244.55036892555654
120.93236593715847
362.2652918025851
108.8251679725945
67.3034261316061
50.1314054094255
236.8174128755927
561.2272995533422
456.18798828125
711.7867438867688
0.0
0.0
0.0
243.64460726082325
174.4070804612711
61.544903011992574
0.43633321300148964
126.10176855698228
50.574227683246136
62.40820151567459
552.0159899741411
214.6098736729473
380.1791642718017
409.43171675596386
222.66310383751988
68.18104146048427
33.196814119815826
135.88080398365855
55.93493679910898
5.745492584072053
512.6277237571776
48.68847318273038
32.85281379148364
815.8065135665238
85.80567321181297
65.48372027464211
98.95884236134589
1272.3841580972075
62.77051621954888
23.33894756063819
166.08738974574953
46.129128992557526
29.122186068445444
12.014973513782024
355.13426854647696
428.31801742874086
707.6558348201215
72.06166709586978
496.3945455234498
104.42985735088587
125.72434779256582
1286.2535544112325
267.23876380734146
36.181190848350525
91.0353275835514
388.0051105860621
579.0222180774435
303.0182974040508
123.22944559156895
381.6318004541099
176.39971644058824
313.22068876959383
295.0690744072199
356.96460907906294
36.45517271757126
440.54838706739247
415.0812338553369
119.90563948079944
6.379768084734678
49.751447182148695
85.34563206881285
20.290414955466986
92.39107867330313
157.32397270202637
126.12490675225854
0.0
254.79721487686038
273.72751297801733
0.0
105.7333265542984
39.79513101093471
0.0
9.022457212209702
576.625430835411
0.0
352.62908904533833
194.04639729112387
0.0
166.88255920633674
71.95975581556559
0.0
98.09128622338176
228.13288106862456
0.0
44.984655098989606
169.06641843542457
232.44153799396008
26.30581791512668
106.87061849143356
41.14788126014173
150.23235026933253
68.46407555043697
363.51258238591254
661.623792828992
863.515073325485
13.37020587362349
386.12611462920904
619.8464914485812
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision   Recall        F1        F2      F0.5  \
0  0.996806   0.631068  0.77381  0.695187  0.740319  0.655242   

   Average Precision  
0           0.489391  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.72906183 0.71350955 0.68943987 0.69881253 0.73510684 0.71566276
 0.70171384 0.68733048 0.71214451 0.68503539 0.69981815 0.7155558
 0.72099497        nan 0.72755198 0.6839426  0.71084139 0.69968779
 0.69854808 0.67730126 0.68242927        nan 0.71771985 0.71871903
 0.71583869        nan 0.70266176 0.71506427 0.53927414 0.68558342
 0.66576109 0.68239585 0.70286913 0.63230273 0.70015923 0.70592467
 0.69807015 0.71464974 0.64373013        nan 0.70287602 0.71285051
 0.69179841 0.71600534 0.57304622        nan 0.72112278 0.69717857
 0.71824077 0.67424577 0.71495593 0.71405711 0.70727568 0.70205076
 0.68967007 0.72576429 0.70584972 0.71225047 0.69286698        nan
 0.6821697  0.70100277 0.67405563 0.71582178 0.68893188 0.37769543
 0.71268357 0.70671207 0.71807711 0.65215122        nan 0.70265489
 0.70346441 0.72015856 0.70992952 0.70996662 0.71099573 0.71421058
 0.70227832 0.67279119 0.69194037        nan 0.72239159        nan
 0.70554003 0.71660864 0.71402233 0.70883538 0.71611561 0.70295302
 0.68381013 0.71734917 0.72812529 0.72167915 0.7099096  0.69570097
 0.71948053        nan        nan 0.70567789]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 2674.664 seconds
Cross-validation score: 0.7351068398847046
Test score: 0.6791569086651054
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 200, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.2}
457.0297999791801
80565.44867082313
124206.06468063593
10405.481289830059
12337.256673205644
115.84724352508783
94.90954004973173
29.29667730629444
701.4838628433645
39.170495577156544
131.92161476239562
32.18627116084099
4759.619914736599
977.9867786355317
981.642529156059
0.0
990.0117797702551
0.0
231.01129093766212
158.83694349229336
6321.8824550919235
580.3405550569296
2205.978142544627
165.10420113801956
114.23027818650007
1.8305220156908035
63.43034382537007
67.46490341424942
36.19603047147393
205.0981604680419
47.82019683718681
15.60070139914751
145.35262437164783
39.63158858567476
141.6899620257318
297.0087743923068
1640.5124426223338
60.04887070134282
34.16784955561161
427.2276424393058
380.39050969854
0.0
0.0
0.0
120.81064238399267
80.53744228929281
592.9953282698989
50.99360414966941
899.4096088930964
143.62254501134157
5.9085550755262375
100.79988920688629
236.23698662221432
591.8749750629067
0.0
263.9634557366371
0.0
0.0
0.0
45.35007166489959
143.11460018530488
123.35200832039118
6.866930935531855
390.3183195106685
33.67755942046642
14.435646891593933
285.83346683532
214.41727339476347
716.8455218449235
142.84990759193897
331.6309028118849
6.682800471782684
12.028794348239899
30.207336015999317
82.6083710193634
10.798643052577972
1471.612950399518
46.640545301139355
32.835951536893845
539.3658925816417
22.82832531630993
36.012501034885645
90.41188151016831
802.1391490399837
266.08306146785617
26.56159718334675
54.458888813853264
114.91020970791578
85.33554971590638
8.67657508701086
150.28982850909233
880.8420313671231
160.6440155878663
102.64055208861828
124.30371459200978
309.17860559746623
40.31580230593681
519.3846877887845
285.07821010425687
52.03718699514866
57.33967876434326
200.50229755416512
836.4385001659393
246.08677995204926
15.131925135850906
598.7359892725945
128.90319753438234
197.6620893701911
790.6067859828472
126.0947627723217
0.08603429794311523
364.9299687743187
27.766566291451454
267.42090802267194
7.901255302131176
140.26217693835497
253.13891451060772
10.712795190513134
204.66286311671138
20.299969043582678
50.99102056026459
0.0
81.39796169102192
449.2465192899108
0.0
219.2737894281745
43.751064240932465
0.0
53.46075617522001
590.2243927195668
0.0
495.9853896833956
326.3202923126519
0.0
151.59375654906034
4.559849999845028
0.0
94.94009237736464
368.9174714460969
0.0
39.34814640879631
81.17793403565884
178.5021804906428
73.4239095710218
271.25920429453254
78.48706928268075
241.02926521003246
12.340825393795967
226.5932220928371
358.0595170930028
245.0523944720626
275.5820404589176
701.3068681061268
294.9180654361844
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.996694   0.637363  0.690476  0.662857  0.679157  0.647321   

   Average Precision  
0           0.441541  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.70930435 0.72010701 0.68514287 0.64042779 0.6865253  0.68575245
 0.70218335 0.68669341 0.70332147 0.71248488 0.68233432 0.69930119
 0.70977996 0.71380468 0.71472758 0.69550934 0.71698272 0.70780098
 0.69177401 0.70849309 0.72286596 0.703521   0.68566924 0.70581996
 0.71300511 0.68105327 0.70858503 0.70327077        nan 0.69303845
 0.70944962 0.69495101 0.69734139        nan 0.68347873 0.71263674
 0.69630132 0.70008429 0.70321665 0.72728635 0.6746027  0.70517306
 0.69115496 0.67687319 0.70454754 0.69101179 0.71002887 0.71667253
 0.71489603 0.69714151        nan 0.70794121 0.70031168 0.67325618
 0.6977368  0.69639773 0.69390475 0.67566226 0.69669783 0.72086155
 0.69440808        nan 0.68666111 0.71033619 0.6836884  0.65922479
 0.71722259 0.68924915 0.72406534 0.67517442 0.62126733 0.69150057
 0.70624632 0.68531426 0.69505276        nan 0.68314811 0.71503854
        nan        nan 0.68414079 0.71627136 0.70422866 0.6979631
 0.70890668 0.70316987 0.69420071 0.69090075        nan 0.69504809
 0.6884762         nan        nan 0.69280282 0.69566666 0.69480105
 0.70813118 0.69702345 0.6784624  0.6919743 ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 2737.440 seconds
Cross-validation score: 0.7272863457746092
Test score: 0.7226107226107227
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 500, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.6}
272.2581668595376
29056.208523151174
56964.59472770628
5798.454409590857
3385.432321787026
7.735774053813657
22.095117458578898
291.755623712801
0.5558531112910714
144.86424525649636
157.42746245412854
3.054078865388874
1273.6857804395258
491.52197488020465
128.3885868427751
0.0
337.78091692598537
0.0
101.53435070290288
1.823273627931485
1990.0919472723472
102.0216957059165
1321.9076663097367
53.25806231106981
0.6244545794324949
0.3520658133784309
49.83607039414346
7.675003539538011
0.004005269962362945
105.63135749445064
0.0
1.240174954291433
1.6739278844033834
77.29973315658572
21.817809242507792
51.184564140450675
264.7162236368167
8.871644060214749
5.587538514577318
66.61092795395234
16.620216426279512
0.0
0.0
0.0
358.6135904339608
0.37794786877930164
111.02491468319204
258.0353225006256
119.84660691767931
54.47363064088859
9.97498715983238
103.41095649957424
2.3376805358566344
511.0349431433133
0.0
186.35613485860813
0.0
0.0
0.0
13.369314106239472
402.6691274929326
30.530556749828975
0.8137740916572511
112.48667974642012
29.226802460558247
65.06000021271757
41.69391431316035
82.15415973030031
259.12449453538284
18.54195443558274
187.77229986366
0.05925200134515762
2.265065673593199
14.679998075429467
16.939097252092324
47.729503243463114
216.03278126525402
57.58614736335585
15.067236527771456
113.19088256257237
11.409851013682783
3.272334778099321
12.266800783479994
834.6812617307805
11.225511073833331
74.87845385476248
81.79878067175741
207.3912597571325
0.0
7.352113752160221
11.613602384750266
393.12417796847876
38.67204838685575
0.14969778564409353
56.40368689515162
84.80146585516195
36.125211982260225
655.0028881619946
10.89711047578021
69.81818447768455
100.63876363635063
22.008505943551427
68.94745521608274
167.47628139486915
0.23624725127592683
46.883462478464935
22.139195002891938
96.00983482366428
252.1142362661485
498.71544087686925
16.6290605820177
187.11294525387348
76.0122891071951
9.637351403478533
0.03274257542216219
1.7664948205056135
257.3733289302327
0.5873889923095703
35.024359196424484
13.424321938859066
10.162756585981697
0.0
70.62172020663274
21.39097883705108
0.0
16.775772988788958
25.980497986631235
0.013332704664207995
0.00050230702618137
689.6906240964308
0.0
115.10044513994944
136.5883322685986
0.0
269.7244992651249
3.8804166755871847
0.0
6.147321114083752
467.25516633014195
0.0
47.48945495717635
11.83307025639806
35.54931597878749
9.573348645863007
376.593458619187
0.4800014519132674
19.49755794252269
55.441631311550736
158.9030056662159
1.6276071453467011
0.026702408271376044
33.39422459900379
36.36974293447565
46.57044218922965
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0   0.99703   0.666667  0.738095  0.700565  0.722611  0.679825   

   Average Precision  
0           0.493296  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.71633451 0.68885422 0.66851842 0.68139229 0.69784051 0.71490971
 0.66738146 0.66184116 0.71390525        nan 0.68741021 0.67389435
        nan 0.67342522 0.70986335 0.65954616 0.68326274 0.71746441
 0.66972811 0.70376528 0.67829671 0.70281192 0.7041641         nan
        nan 0.686137   0.6941213  0.68643049 0.72100965 0.71376352
 0.68981761        nan 0.633647   0.73238059 0.69669372 0.7200336
 0.72248242 0.68986754 0.70665013 0.67664462 0.71404489 0.69489886
 0.67299895 0.70506859 0.6985866  0.72081673        nan 0.69829088
 0.70702639 0.68111643 0.68568321 0.69268316 0.44455458 0.71277205
 0.70891238 0.67610881 0.69758605 0.68542907 0.70308602 0.68643117
 0.71310433 0.720418   0.67691295 0.69975007 0.69866993 0.69821254
 0.70003107 0.67678423 0.70521988 0.66502438 0.70519007 0.70236898
 0.68637274 0.69156276        nan 0.67818168        nan 0.71615358
 0.68141364        nan 0.69270746 0.70288743 0.70387543        nan
 0.7175542  0.70818926 0.67733551 0.71623138 0.69566027 0.69736811
 0.7290485  0.69106391        nan 0.72408122 0.71556852        nan
 0.66236026 0.66830959 0.7071057  0.70325799]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 2602.577 seconds
Cross-validation score: 0.7323805943949001
Test score: 0.6704545454545454
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 100, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.1}
695.2244842275977
121603.64893057686
254714.2983130738
20234.801933332114
13113.083657211842
141.5156825184822
317.10070960223675
172.41990587115288
100.4318390712142
122.08776316046715
214.0921826697886
22.537787906825542
7534.64361038222
16585.802329757556
1014.4003780335188
0.0
1700.9645563685917
0.0
282.58299737097695
67.23916639387608
8451.971032650676
5433.489869582467
12181.954331509016
178.67335744202137
22.60443154722452
13.118148572742939
136.59684740006924
278.119603343308
84.3410147355462
343.72932218853384
40.312720984220505
32.15381655469537
167.9450507229194
547.8117003273073
48.86459725801251
645.2055510431528
216.08951516542584
179.44109339267015
97.01498479023576
1098.2314558029175
73.92091440595686
0.0
0.0
0.0
232.59174306021305
1380.7469234392047
309.5829120259732
588.0293538086116
2145.026023622602
389.7257445268333
140.41059193736874
368.3387407101691
111.35865368694067
861.5354973003268
6.266825929284096
1274.2582525461912
0.0
0.0
0.0
506.3642376822536
827.6948302984238
194.7267826255411
6.275676809251308
468.0536104515195
150.59322912991047
114.40201531164348
4603.223973785993
616.5239204572263
927.7153753712773
292.21476180350874
158.55892127007246
48.05405640602112
40.540899991989136
257.2427658289671
280.0914674811065
35.886460207402706
673.7573520839214
130.57104716077447
57.18689514033031
902.5144748464227
94.81444040685892
118.6385041475296
262.14432518184185
2099.158707531169
113.80230811610818
342.2970298528671
164.8033785894513
155.70788297057152
23.93868151307106
710.9093206822872
349.989376497746
1113.5997856929898
417.4093081410974
65.28224968910217
204.15830314718187
478.45349092781544
190.71393002569675
1354.5750073101372
276.6041217446327
367.57980620861053
110.7477223277092
723.929548215121
808.9024826802779
909.3993950337172
148.65669952874305
817.8871716306894
278.1352281868458
356.8273593261838
677.1628855911295
496.6210057635908
81.84834066033363
1305.984418425709
333.8076755106449
491.6019020136446
12.346267491579056
251.2230458408594
529.4213880375028
37.803287379443645
398.12287655472755
23.045466870069504
124.06045761704445
0.0
204.73420953564346
891.8625660650432
0.0
135.2374687581323
109.81000138120726
0.0
223.6007963269949
985.5628027189523
0.0
689.4989585606381
429.49342178506777
0.0
185.85017527639866
68.5367342773825
0.0
181.62851417064667
747.9926198981702
0.0
436.1273923292756
96.04132506763563
445.96549255400896
66.82881619288491
143.7865433394909
145.17829673737288
449.06291242688894
127.23427802324295
352.31837343081133
849.4744828343391
1883.313597034663
317.72738184034824
467.43591399118304
390.4639343768358
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall       F1        F2  F0.5  Average Precision
0  0.996078   0.567308  0.702381  0.62766  0.670455  0.59           0.399867

--------------------------------------------------------------------

