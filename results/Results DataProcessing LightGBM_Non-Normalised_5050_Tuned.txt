[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 879.283 seconds
Cross-validation score: 0.7814286348132576
Test score: 0.7975986277873072
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': -1, 'classifier__max_bin': 50, 'classifier__learning_rate': 0.1}
1213.39317715168
26997.069630920887
17906.728093594313
24495.591392695904
5379.941841244698
124.45589908957481
148.02451825141907
40.751213520765305
332.5148393213749
430.9603454172611
244.66785126924515
242.53717836737633
8777.046210676432
921.162948846817
114.05102676153183
0.0
526.8431482613087
0.0
33768.80926927924
135.90630331635475
3902.5613399744034
26.192888140678406
201.9742319881916
94.54100978374481
220.3240571320057
92.61530229449272
48.5585335791111
1620.2393243014812
1252.7473176121712
157.87144443392754
797.8581437170506
43.91488242149353
20.707197695970535
574.0216395556927
13.492151737213135
355.17851808667183
568.7243593633175
102.11073738336563
200.51858142018318
423.25336572527885
71.3647458255291
0.0
0.0
0.0
195.5210590660572
0.0
451.59654811024666
391.6962191462517
1.0780400037765503
426.9615992605686
176.1001238822937
7.384890079498291
105.69459500908852
596.1911076307297
0.0
475.96464443206787
0.0
0.0
0.0
70.03490555286407
256.57624819874763
176.836517482996
55.63998532295227
18.965412974357605
28.2261620759964
20.450324952602386
13.796210825443268
339.50454545021057
678.6270787417889
93.25052976608276
147.33474987745285
227.23852199316025
129.60905027389526
330.5182099044323
310.8514029979706
106.8359876871109
545.7736800909042
42.586173325777054
226.10864448547363
769.681931078434
8.023411244153976
33.60598301887512
110.6455345749855
1799.6736212670803
62.478946566581726
40.389021039009094
14.113299012184143
668.13982629776
145.0986350774765
208.2329257428646
290.7842875421047
344.6600036621094
572.9538337290287
130.81610196828842
469.7884643673897
38.85467833280563
143.27194094657898
147.03668749332428
458.53314739465714
86.90420687198639
44.830895602703094
253.1175411939621
59.199173390865326
0.0
55.27138739824295
1183.794668585062
2152.072569966316
69.81740894913673
11.937222838401794
676.4644948840141
454.05864104628563
107.46976438164711
139.88475421071053
71.82451620697975
133.7808437347412
94.57581520080566
869.7972110807896
200.80209398269653
104.68330800533295
31.932930678129196
103.13530626893044
0.0
139.17453902959824
295.4229262173176
0.0
329.2226570248604
84.43354871869087
0.0
15.846224129199982
415.5433557033539
0.0
998.4582412838936
483.5471242964268
0.0
41.09040427207947
218.25496032834053
0.0
4.210209965705872
236.65793526172638
0.0
236.59018033742905
272.01974841952324
411.4146182537079
73.27607908844948
106.87123119831085
159.10279950499535
489.819865912199
0.0
143.50772523880005
68.23084795475006
209.45190718770027
17.196069717407227
503.9027276337147
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.987956    0.65035  0.845455  0.735178  0.797599  0.681818   

   Average Precision  
0           0.552897  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.77224905 0.77846229 0.74834648 0.76328142 0.74973219 0.78733426
 0.76652417 0.77109941 0.75873928 0.76419524 0.77326859 0.76028373
 0.76911073 0.74666997        nan 0.75571998 0.77432574 0.78707975
 0.73152673 0.7653675  0.76158366 0.77626918 0.77489306 0.76861478
 0.74952767 0.74685221 0.77185874 0.77014712 0.77860864 0.75203788
 0.76616214 0.7513198  0.74861749 0.7712037  0.75519552 0.76076305
 0.75720115 0.75117637 0.7620563  0.76082109 0.75633227 0.77702722
 0.76372874 0.76467944 0.76016906 0.75218132 0.77172086        nan
 0.77322509 0.75669309 0.75704281 0.7482372         nan 0.76248847
 0.75607273 0.75899019 0.75903166 0.76884074 0.76238672 0.7605324
 0.77324069 0.7575133  0.77119175 0.7578264  0.75368433 0.77895437
        nan 0.77564418 0.75720459 0.75603831 0.76635069 0.77863716
 0.75312931 0.76936364 0.781051   0.77861444        nan 0.77050295
 0.75986242 0.76500277 0.74893461 0.76124513 0.76475379 0.75469216
 0.7580888  0.77790469 0.7432134  0.77179894 0.74590156 0.74934122
 0.76303829 0.78444079 0.77283353 0.7818903  0.73983478 0.73234235
 0.77635267 0.77426634 0.76878378        nan]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 890.414 seconds
Cross-validation score: 0.7873342559396972
Test score: 0.7271095152603231
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 300, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.3}
208.8900812373904
8759.448344987914
9924.000157934053
15196.486814637583
3886.011316799705
143.50787309376756
163.38226875662804
15.651291981339455
112.99196627736092
41.430720537900925
47.99491564184427
95.47817144775763
1568.83130543679
79.01672537534614
100.69294612109661
0.0
257.23950372624677
0.0
3949.3360662193404
24.638351000377043
1637.468039125204
19.821598201990128
11.278695094075374
3.7226328998804092
92.3970159292221
44.12878741323948
87.66369221988134
1321.8568279030733
48.19396413748982
5.310489892959595
616.3876065015793
3.855360984802246
2.3891650438308716
111.36095505952835
5.013662099838257
17.938069138675928
1034.1389654953964
50.83807613700628
9.86442020535469
65.43649835884571
28.07329072803259
0.0
0.0
0.0
6.568609952926636
0.0
194.38555871101562
50.902306980628055
0.0
100.09010781347752
15.882237255573273
5.048270225524902
30.96052972092275
110.02333849016577
0.0
360.9822887182236
0.0
0.0
0.0
37.00574171543121
33.369501046836376
37.221997544169426
1.0435669869184494
15.654859781265259
19.37075257115066
1.23471999168396
0.526045024394989
159.08264272660017
614.5806210946757
10.319390833377838
12.777471981011331
49.15274953842163
32.54333981871605
57.085385584272444
86.25916463136673
0.0
172.41176527123466
3.970724940299988
135.81066888570786
213.23529696464539
33.040779024362564
12.068730679457076
38.41540342150256
401.7356086075306
37.45271906436392
2.3231000900268555
26.460090381617192
319.83355045318604
53.535194873809814
11.853934779763222
28.037725865840912
6.177619934082031
249.06757003068924
1.2619610130786896
72.55470734834671
3.6903300285339355
9.82838785648346
9.00021979212761
49.01532752811909
6.7168780863285065
1.668188065290451
7.818437933921814
9.487959718797356
0.0
20.805742993135937
495.75334426388144
685.3959654867649
57.58047844469547
15.751530170440674
258.95533859729767
212.89757958054543
24.60915869474411
489.6940587460995
36.670872416347265
13.53675851225853
96.83658801022102
661.3473794609308
78.87701028538868
63.31401840309263
4.845508873462677
74.7817595154047
0.0
10.166056364774704
13.396242236660328
0.0
82.66773652145639
83.90500547736883
0.0
5.044846147298813
68.73971984535456
0.0
88.89459071325291
88.02560923993587
0.0
57.72625306248665
176.73521350324154
0.0
267.52421021461487
14.523684468678823
0.0
48.212370654568076
221.68426007032394
78.38944892585278
37.98441497187332
10.933631986379623
40.76023706793785
59.37140816450119
0.0
88.10256203528843
19.96000748872757
66.77638509264216
3.7334600687026978
93.69165223836899
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1       F2      F0.5  \
0  0.988316   0.692308  0.736364  0.713656  0.72711  0.700692   

   Average Precision  
0           0.515003  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.75498579 0.73303231 0.77237151 0.74978345
 0.74444267 0.75130887 0.7403543  0.74554016 0.76603262 0.75274976
        nan 0.74576619 0.72948643 0.72949678 0.75775958 0.74537626
        nan        nan 0.74567298 0.73728554        nan 0.76021627
 0.75652651 0.76786885 0.73459337        nan 0.75632642        nan
 0.74839    0.7678483         nan 0.75849923 0.74032298 0.76741667
 0.73629196 0.76781036 0.76710775 0.74759621 0.75491502 0.74643948
 0.75567755 0.73696755 0.75393003 0.75487001        nan 0.73970739
 0.73178322 0.74738165 0.75282561 0.74300085 0.77583569 0.76648374
 0.73607944 0.74274156 0.74041279 0.75460291        nan 0.76968436
 0.74620162 0.75306052 0.75273497 0.75713431 0.76908163 0.74895181
 0.73429105 0.7350005  0.74140069 0.73286138 0.75575153 0.74767056
 0.75239272 0.76790656 0.73721707        nan 0.75883004 0.74420157
 0.75289697 0.75208714        nan 0.76070269 0.75133618 0.74842243
 0.75937641 0.75036171 0.75992401 0.7641972  0.74314265 0.7291546
 0.7223883  0.73467075 0.76465887 0.73256191 0.77075603        nan
 0.72043455 0.73063701 0.76258338 0.74365485]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 835.949 seconds
Cross-validation score: 0.7758356938077826
Test score: 0.7705779334500877
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 500, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.2}
437.4078097005224
13683.342124753395
14373.329519384395
17581.694888220376
6686.786129209765
90.68064451796755
12.295300369441975
39.47899302448877
307.03408411494456
32.896619379520416
92.20253783464432
2.15519442781806
3271.639014533721
599.9275119171361
47.16321964375675
0.0
364.15284788794816
0.0
7874.717080283113
240.14627236919478
1295.8847458810833
37.39264973340323
88.99557985924184
30.55581135619174
15.592559681273997
41.03320342488587
194.94922436773777
1283.972116365847
27.93376411497593
26.085559176193783
1.8608770817518234
13.603793919086456
11.349898742646474
477.24622074000825
20.543039153330028
28.047275036131396
412.09012715518475
78.24231635463576
55.09693146774589
484.64904759798446
58.173174159078144
0.0
0.0
0.0
19.209617042914033
0.014511900022625923
80.48052637279034
101.31071550177148
0.0
233.47188125550747
26.570165425539017
0.0
126.21423001605945
60.79736949902872
0.0
799.7408246382755
0.0
0.0
0.0
59.6417241524905
266.1759038715728
123.63499151829343
10.022866223977218
4.1307239681482315
15.966229438781738
6.923979043960571
0.0
269.575862596801
363.1943093485015
5.868667930364609
17.99919036682695
61.96293501276523
89.0881385441171
51.17927530477755
125.63301723613196
18.525815978236096
267.7409219067847
10.563415237236768
45.452883861557325
636.9625507295132
19.794536769390106
42.13718145963503
108.75886376202106
487.9407267719507
156.4033939409419
1.2881906162947416
19.308367592049763
396.82281473279
2.755088061094284
42.42724135018102
6.436744499675115
101.77512949705124
66.56287035831701
16.608327097259462
189.71013040022808
6.134524881839752
13.108482378913322
12.357080197893083
147.11862271645805
44.48088812828064
48.69660717784427
63.22742464713701
39.51389394234866
0.0
18.286902487452608
512.1832818626426
960.8125700938981
74.35435854410753
0.0
483.0509200586239
116.95489205420017
117.98225278453901
229.95768914883956
57.4281832203269
45.41886292397976
26.426639758283272
1306.2382738139713
45.99481158837443
16.8124303273637
20.289998084306717
39.35172349845734
0.0
54.16083398461342
140.82119309995323
0.0
232.4589674770832
65.46934656094712
0.0
28.86855326127261
165.56157205393538
0.0
571.2704068571329
56.662991389632225
0.0
10.935319242998958
30.413931531548428
0.0
599.9376794099808
42.45163007680094
0.0
77.30210293980781
350.2190006971359
158.32277245697333
16.1455208347179
41.4080585692318
38.9069155305624
33.364762008190155
0.0
63.329989156998636
45.13150471448898
70.64122504586066
0.0
240.79253276219788
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall       F1        F2      F0.5  Average Precision
0  0.988316   0.671756     0.8  0.73029  0.770578  0.694006           0.541359

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.76527498 0.75544465 0.76083544 0.77586773 0.75471641 0.7587137
 0.74917042 0.76320398 0.77220302 0.76705374 0.75902412 0.75366602
 0.77205337 0.75969533 0.73956546 0.73758944 0.76429925 0.75998657
 0.72468889 0.76035612 0.73577124 0.77339866 0.73356345        nan
 0.74512724 0.75638675 0.76462422        nan 0.75098974 0.75605804
        nan 0.75606557 0.76362862 0.76595191 0.77275631 0.73952723
 0.75145873        nan 0.74023094 0.75197177 0.7586498         nan
        nan 0.73959595 0.74426485 0.74597737 0.74013708 0.75708873
 0.75378536 0.75999029 0.75596368 0.76104646 0.74886049 0.76412979
 0.75572652 0.74977298 0.76296012 0.75357245 0.77364435 0.76235443
 0.74816983 0.74736775 0.75130745 0.7620792  0.77207692 0.76052677
 0.75255999 0.76973562 0.76080849 0.75662608 0.76100296 0.75344715
 0.7406198         nan 0.75972819 0.75636135 0.75405412 0.74065592
 0.74929612 0.76325758 0.75322032 0.7577277  0.75836001 0.74520824
 0.75855063 0.74842588        nan        nan 0.75089379 0.75589816
 0.7618932         nan 0.74616188 0.74600341 0.75252175 0.7818478
 0.76516234 0.76547114 0.74433308 0.77308047]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 932.677 seconds
Cross-validation score: 0.7818478045884102
Test score: 0.769927536231884
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 100, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.4}
96.14736380268005
7492.131719322671
6927.983220898022
12525.471074936679
1675.2658437853388
34.346410844125785
25.592778718622867
0.18167191231623292
55.609625588636845
54.740527662681416
40.39976832258981
70.53361812443472
1139.2303529275814
844.6614735638141
44.19110430055298
0.0
26.127658756740857
0.0
4342.905245242437
25.80670340033248
1836.8624754110933
27.394496629131027
50.99248191213701
9.382692507118918
103.3103955576662
50.65247195138363
14.942692206939682
651.6221300528268
23.979282000975218
4.533037088112906
0.4316742685623467
0.0595591189339757
6.187436678883387
133.81936742344988
10.99693842313718
105.79759138764348
196.0937076114642
111.82791338497191
16.936916721228044
418.0006172560388
14.041180281783454
0.0
0.0
0.0
23.914320132229477
0.0
5.530822654487565
48.253603579010814
0.11304688034579158
59.18274496408412
3.4751451464835554
0.03001430071890354
50.01040407572873
92.67632773588412
0.0
261.5544058405794
0.0
0.0
0.0
30.142509090830572
114.07711085362826
14.664781222498277
4.4762056628242135
0.4171557517256588
4.329472284647636
4.103641696507111
0.11774399876594543
19.14305887190858
110.43391397339292
8.019848799711326
238.5795500142849
15.942853959801141
24.34101327433018
7.961553671630099
9.096957553643733
2.700925854034722
193.88904099800857
4.150529827282298
17.29075689613819
471.21336999488994
8.995285880868323
21.544020257075317
73.15913986793021
436.59482444351306
19.15060944276047
7.829160786175635
15.811872851394583
258.0256918966188
22.663167059479747
60.81250246649142
88.89928548253374
7.1658141342923045
121.12523568799952
0.006640045787207782
39.839634780888446
1.0033359294757247
3.061598453205079
13.704522137879394
127.18662885960657
41.39147718681488
6.172365500999149
42.70528684137389
40.823209930444136
0.0
9.401621950237313
212.01032636564923
228.0748165335972
66.71515080216341
21.50830078125
209.67784041090636
113.52220937825041
18.08301042020321
19.94552088563796
17.50924663982005
5.028526185196824
179.46276383288205
945.6082425299101
80.58886023855302
24.845991492271423
25.040856566018192
41.45766990998527
0.0
24.953397927922197
74.41448929009493
0.0
121.86547773447819
46.70979225717019
0.0
168.8635723197949
35.318263645574916
0.0
447.6106885140762
34.578336003702134
0.0
38.27060748060467
29.28032393025933
0.0
30.083088888786733
88.3217117076274
0.0
24.798926104529528
198.34692264080513
62.675419261911884
20.602509206917603
18.39779667381663
67.37918694096152
29.541311568580568
0.0
154.6853427552851
2.8193481261841953
82.7030230780365
7.36064808210358
26.539653678191826
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.990653   0.758929  0.772727  0.765766  0.769928  0.761649   

   Average Precision  
0           0.590939  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75483742 0.77715309 0.76681183 0.7608965  0.76751216 0.78044012
 0.76954735 0.77458603 0.7733118  0.76228885 0.79202062 0.77573422
 0.76136391 0.77869246 0.77372236 0.7650974  0.76894868 0.77240627
 0.75268652 0.76936346        nan 0.76093397 0.77488601 0.76082147
 0.77757361 0.77602806 0.7798865  0.79081099 0.76487863 0.77645556
 0.77837574 0.77923784 0.74912602 0.77036922        nan 0.74917243
 0.78253503 0.76101661 0.77983757 0.77397461 0.76649489 0.76866489
 0.7783499  0.77627683 0.76653347 0.76786353 0.75122958 0.76762636
 0.79377249 0.79336866 0.75699726 0.77651374        nan 0.75903346
 0.78905169        nan 0.76242234 0.77196132 0.76749016 0.7776859
        nan 0.77270079 0.78800784 0.77643596        nan 0.7737032
 0.75877962 0.77072946 0.76403287 0.76961423 0.78176624 0.77443973
        nan 0.76124095 0.76232745 0.75618774 0.76397704 0.77768156
 0.77758909 0.78127819 0.77400165 0.7544845  0.77807213        nan
 0.77923704 0.76713114 0.78759872 0.78114349 0.75802249 0.75558408
 0.78655448 0.76686454 0.78498981 0.79489704 0.77659583 0.76497099
 0.791741   0.75176651        nan 0.76079464]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 1013.773 seconds
Cross-validation score: 0.7948970370428553
Test score: 0.6846846846846846
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 900, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.3}
166.4638839774749
9576.768986521189
9291.652037984251
15180.798840757578
3718.5263052217397
51.83296190536923
47.45570190285798
10.239702866392818
213.02574149452266
65.82252050895477
98.78382941204109
43.34986568929162
2419.2000312196033
562.5604439228025
8.331665728241205
0.0
274.83539564824264
0.0
2464.2635159359525
302.11338630653324
3903.184316426614
13.305205887882039
19.251061575370443
2.6090690416140205
22.60350317737334
9.614495752083414
28.69924793217797
523.3670306141794
714.7258940706961
67.8177731507476
248.9809883888811
14.525442361831665
12.079333174508065
145.09331607169838
17.07827037847892
52.866179664335505
474.08605244301725
47.36704234741046
28.932962941238657
364.45070843514986
19.956540073399083
0.0
0.0
0.0
26.441597748758795
0.0
115.18184313318488
50.887587627525136
0.5179011039435863
156.148801695148
35.17882842570543
0.1635259985923767
44.92761353991091
69.05680966906675
0.0
309.1787232243522
0.0
0.0
0.0
79.6238320319149
376.4861746074216
18.11873920116574
0.05821559934520337
5.590410486287525
1.0193056450225413
2.7385754722090496
4.168189445445023
52.284474648709875
545.8369310753851
6.859947890538024
76.60762297653628
9.04778327094391
29.129311731627013
121.8916058645118
36.09580183457547
21.83506396156372
173.34172717283946
7.304092198610306
129.47934144409373
437.773034656272
8.684117306955159
12.27557453000918
57.81066761121474
189.61439253347905
17.02426350222595
8.378220543789212
30.595236940524742
442.93273568805307
16.47041660925788
28.163799668582215
6.917096379680743
68.543581828475
87.53616778180003
0.7644469914957881
201.15978063992225
2.214068964123726
12.198488545138389
27.84162688193919
75.74481601410821
14.106771241873503
7.461534932779614
8.093478680588305
75.56036745547317
0.0
30.475080269705842
487.9590735840011
101.08616212400375
48.110070958297
13.29119236767292
102.81875266565476
137.36459283947903
2.5506029715761542
13.526039866164865
40.04690736186242
30.793637282018302
78.09640943400154
728.2069339775203
42.573503724301645
83.12710974924266
7.96765457931906
157.5971774897771
0.0
67.73527442080376
89.14491812458698
0.0
59.43001544044819
35.81953299281304
0.0
21.272636727429926
127.00360644913417
0.0
119.18880489328876
95.82588944495126
0.0
15.29331146585173
16.907569799368503
0.0
1.0420402511954308
31.711136948153893
0.0
26.435448773932876
147.21029821443827
46.1726092377794
25.660979746828218
5.285007613214274
16.897594610810756
73.39562578173354
0.0
14.209243766512373
4.456434587482363
80.21945200771032
11.176991179585457
6.88788381424547
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.986878    0.66087  0.690909  0.675556  0.684685  0.666667   

   Average Precision  
0           0.462713  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.7790781  0.78553316        nan 0.76733835 0.7710075  0.77234366
        nan 0.76375418 0.76853889 0.76428481 0.77139872 0.78078
 0.7800795  0.78111    0.78576665 0.77839548 0.77223568 0.77237198
 0.78100535 0.7809942  0.79221274 0.77115678 0.77588472 0.76065022
 0.77958413        nan 0.78805745 0.75630422 0.78064389 0.7943632
 0.78725696 0.76824781 0.75380515 0.77563878 0.77915308 0.78382627
 0.78914329 0.79935763 0.77550484 0.75657174 0.78079236 0.7773749
 0.74144096 0.77800682        nan 0.77053015 0.76973736 0.80465322
 0.77174046        nan 0.78193629 0.77445307 0.76112152 0.78163428
 0.78390556 0.78978384 0.77183494 0.77381309 0.78059822 0.78959916
 0.78162006        nan 0.78627967 0.78696132 0.78378938 0.77277439
 0.77657961 0.77912472 0.77633654 0.76381967 0.78321117 0.7689387
 0.77108389 0.77004257 0.79311186 0.78578791 0.78433488 0.78694684
 0.77002665 0.76821358 0.78892484 0.76500418 0.76930358 0.75722711
 0.77337749 0.77421109 0.78179464 0.76873875 0.77457229 0.78573912
 0.78578798 0.77333915 0.78357607 0.76329018 0.77549443 0.77375811
 0.78023862        nan 0.77939819 0.77639021]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 1053.071 seconds
Cross-validation score: 0.8046532218831622
Test score: 0.7028469750889679
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': -1, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.2}
376.9907620359983
14429.477025060663
12621.943725323777
24118.14212539369
3166.1119661001353
24.050017292812342
16.43089232816419
12.491749637581961
161.69162974398836
19.576026316040952
97.78434350923635
23.342634698208535
2857.652432979768
898.2223222544515
23.751635909080505
0.0
147.38916932114523
0.0
5918.947401409353
28.29647973888683
2421.323597266714
12.126504818480592
94.65129347286302
8.951446843886515
43.010453375344696
62.42888450986152
208.95275771836168
907.15203105408
225.04263411890133
7.253185873928729
53.20050285913749
8.192770646951004
11.718039732963973
164.7416754656756
25.73057447402971
141.7017215313856
616.3356932343304
115.60187067463994
44.70503202751752
716.934071673887
38.46917959847042
0.0
0.0
0.0
23.161263820182285
0.00114465004298836
106.83940810497006
78.54284274342187
0.06254170089960098
175.71306974126492
132.7965040371637
7.707849979400635
120.26787113957107
121.37763991686475
0.0
332.95357024588156
0.0
0.0
0.0
58.888744883241806
101.79195888340473
69.71551034157835
17.62379850447178
26.255561469122767
42.34537721518427
94.56290804594755
37.27894442155957
137.62173579560204
211.20707782722826
23.271370886269224
16.188718995894305
13.011006777806188
91.18278037588246
51.165664658574315
162.61619075781527
32.799584683263674
58.04088357669191
32.368712958454125
203.0885725011176
315.00738670141436
18.273801275587175
19.932005423010196
45.754922926449126
772.711800181083
55.569808812955074
2.7950723786889435
28.492557518315152
181.63132998195942
38.43800485929006
202.4224586089258
358.3317764209549
31.13428020477295
167.45744593341078
2.125395141189074
109.82641533512282
6.5267818961292505
3.608850210905075
10.32918966939883
26.88947778835427
38.69687318822292
6.817440441634972
25.267841209089056
51.56006805651441
0.0
18.26850354974158
624.3780326212245
463.04885049575296
92.34950981459315
1.2472760081291199
410.6785200316335
278.2182216227575
49.33310131917824
47.70428720893558
78.35867638513446
57.20033036917448
220.0837504086012
2069.7381933178704
139.40037776288227
178.77906870143488
149.6262853553053
105.86586222357607
0.0
33.00127684023073
106.48702182775865
0.0
136.17708340659738
96.82904071960947
0.0
12.15397249290254
11.118282313233067
0.0
891.7486854662129
135.44359885438627
0.0
21.193444688808995
27.879291597065503
0.0
26.970184460282326
22.487572756384907
0.0
60.458404327457174
191.83597294588344
68.81918478809803
22.389403496551537
29.565319538500034
20.426057263797702
123.73418853396288
0.0
75.14855582641725
16.2610755674541
227.20274339969865
2.252137588059803
338.6088163544008
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.986698   0.647541  0.718182  0.681034  0.702847  0.660535   

   Average Precision  
0           0.470625  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75019572 0.76706125 0.75507336 0.7415376  0.74669928 0.75977357
 0.75090957 0.76446252 0.7815675  0.76764165 0.7701232  0.7638034
 0.75913862 0.74271819 0.75130475 0.76011757 0.76235186 0.76592603
 0.75951355        nan 0.77065684 0.76002175 0.75175075 0.76569842
 0.75530918 0.74009685 0.77800792 0.76150599 0.78495247 0.74151769
 0.76618714 0.77113427 0.77126273 0.77141955 0.759828   0.74576305
 0.75681083 0.77583932 0.78380576 0.7566967  0.7561457  0.74134461
 0.76762609 0.7871022         nan 0.74853654 0.7724859  0.77806118
 0.74080363 0.74696115 0.7723508  0.7637138  0.75864417 0.77177042
 0.76869576 0.76965615        nan 0.7608085  0.74735164 0.76994966
 0.76038679 0.76765657        nan 0.73608856 0.75817772 0.77216932
 0.73987996        nan 0.7507572  0.77403587 0.75333185 0.7438714
 0.73833692 0.74623655 0.775109   0.76247928 0.73084104 0.75607451
 0.7398157  0.767108          nan 0.75093256 0.74117629 0.76454695
 0.75732124 0.77081723 0.76508858 0.74709751 0.76109834 0.76346429
 0.75961501 0.7521964  0.74845765 0.74018036        nan        nan
 0.76211528 0.74391602 0.76214646 0.76096648]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 968.951 seconds
Cross-validation score: 0.7871022047264102
Test score: 0.7820738137082601
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 900, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.2}
1077.607349516824
12056.552529036067
14497.803803544492
28878.110363721848
2542.6954378392547
112.91373035311699
152.67117173597217
14.767864923924208
765.0444461284205
179.7418981641531
158.2922867294401
116.98973322100937
1736.9449051097035
957.6175382435322
2.429675469174981
0.0
263.6113236621022
0.0
3164.1337330210954
80.94888558518142
2248.1392678134143
74.5530890673399
94.28584098536521
21.01811423152685
84.71369900926948
23.592732465825975
60.24085346516222
332.8337686881423
74.28867502696812
56.27073913812637
31.337041083723307
12.173397509381175
12.513899197801948
195.24956951104105
22.49619505740702
223.81865218654275
219.10755672119558
184.39302855730057
14.901113238185644
584.6494747586548
51.63114302046597
0.0
0.0
0.0
37.29478666186333
0.0
120.46665504574776
58.22356885485351
1.06427001953125
203.49378555640578
22.944804338738322
0.0
62.46694039180875
242.98760374076664
0.0
597.5086034834385
0.0
0.0
0.0
69.48571917507797
298.1232952438295
140.6557841207832
12.616365015506744
6.154854213818908
1.7971697598695755
70.21482361480594
91.79235517978668
52.52915710024536
402.9511151853949
12.421728990972042
93.45252677798271
38.076946061104536
36.270847810432315
29.336568869650364
51.50376661866903
54.92904580011964
200.91813006065786
6.27581936866045
120.24149826169014
301.7741867341101
1.2062822822481394
30.644007781520486
51.15928688086569
402.59644639492035
89.46929742861539
2.2527793254703283
22.96496764663607
695.3020109143108
6.429977931082249
5.054005114361644
37.91048749163747
0.9492219835519791
296.15637204423547
61.868000477552414
29.635167858563364
0.7158709913492203
36.49940012767911
68.54157908260822
377.54238887503743
1.4105112720280886
34.415560860186815
64.30535252206028
98.51490499451756
0.019638899713754654
42.76466138102114
565.2965249540284
581.2119616186246
69.82543587870896
103.54740142822266
148.657282570377
130.59677230380476
46.34301704540849
240.2694187052548
32.04180054925382
31.700013171881437
6.835527095943689
1892.5740460827947
66.34914433397353
556.2094193976372
47.475275099277496
51.718914249911904
0.0
32.50191569700837
161.87637015990913
0.0
151.57288353145123
72.86126637458801
0.0
50.528370186686516
140.79317409545183
0.0
166.4344501644373
51.91393499262631
0.0
59.01516764611006
83.63823951780796
0.0
221.3094431180507
59.175821201875806
0.0
82.81493189558387
73.35407886002213
89.31560474541038
35.82252874970436
81.14978403784335
8.927247609943151
96.22386616654694
0.0
30.48143471777439
22.3485570512712
32.54499582760036
0.1635340005159378
39.48228404298425
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall       F1        F2      F0.5  \
0  0.989035   0.689922  0.809091  0.74477  0.782074  0.710863   

   Average Precision  
0           0.561985  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\metrics\_plot\precision_recall_curve.py:125: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.77149738 0.74425623 0.74467028 0.76116523 0.76425558 0.76846254
 0.74498085 0.75777673 0.77477456 0.75719951 0.77133205        nan
 0.75963521 0.74259179 0.7617556  0.76566793 0.72745001 0.7542924
 0.75735797 0.74796488 0.75183519 0.74093638 0.7570007  0.74241572
        nan 0.74024458 0.77582924 0.77805584 0.76672581 0.73195137
 0.76185465 0.76279009 0.75217852 0.74867288 0.76918748 0.76397947
 0.74890407 0.7711074  0.74885526 0.75197409 0.75615959 0.74979339
 0.75442378 0.74979074 0.76013293 0.75599632 0.76174101        nan
 0.75382074 0.74025843 0.75759186 0.75702138 0.75660413 0.76482586
 0.76137732 0.74657281        nan 0.75118369 0.77698749 0.73021685
 0.77319618        nan        nan 0.75004276 0.74867628 0.75004146
 0.74832602 0.7510281         nan 0.76402059 0.75737798 0.75593216
 0.75025532 0.74150529 0.75657907 0.76674355 0.74426821 0.76786487
 0.75526422 0.75858384 0.77609435 0.7424844  0.75632416 0.75011517
        nan 0.74699403 0.73649317 0.75849532        nan 0.73819086
 0.73445178 0.73624335 0.76154571 0.7435489         nan 0.75877022
 0.73456331 0.75895692 0.75658605 0.77082053]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 876.255 seconds
Cross-validation score: 0.7780558435680385
Test score: 0.7547169811320755
Best Hyperparameters: {'classifier__num_leaves': 11, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 500, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.2}
197.64527070522308
14062.36917078495
11594.313123524189
17622.28824532032
3263.6240199804306
81.7969981431961
11.311389923095703
8.922329902648926
547.0805417895317
147.4926302433014
121.42718696594238
54.83826971054077
5373.063508987427
495.36785769462585
148.2415030002594
0.0
74.06311845779419
0.0
5937.942844152451
438.39974343776703
4162.33373105526
5.217050075531006
161.24945092201233
23.732150077819824
21.157360076904297
15.154250144958496
87.25894927978516
1914.0027627944946
5.531919956207275
16.14579963684082
0.0
15.047599792480469
12.973440170288086
146.37322276830673
4.9203901290893555
36.94768047332764
1601.016005039215
32.228800773620605
21.533549904823303
501.45321464538574
20.543220043182373
0.0
0.0
0.0
6.68011999130249
0.0
214.09813952445984
185.27005910873413
0.0
253.01546001434326
0.0
18.25790023803711
37.58693027496338
227.83960342407227
0.0
756.6871747970581
0.0
0.0
0.0
114.02446842193604
177.28280425071716
12.610980033874512
4.162179946899414
34.18498957157135
14.711930274963379
41.61240005493164
62.7588996887207
36.181570529937744
113.55576801300049
2.096590042114258
35.86185073852539
137.27131128311157
130.67918825149536
188.8803629875183
69.82905077934265
4.391019821166992
10.52616000175476
23.6658992767334
6.038780093193054
286.7010567188263
0.0
11.691749811172485
23.10919976234436
554.6090515851974
23.079229950904846
3.007920026779175
59.150620222091675
977.8733286857605
3.9888399839401245
2.340290069580078
5.146739959716797
233.52899169921875
525.1463885307312
0.0
148.49505233764648
0.0
13.677169799804688
42.11832904815674
254.99224984645844
35.075379848480225
40.74732971191406
9.261679887771606
40.72673010826111
0.0
6.483160018920898
704.2843487262726
461.0054965019226
66.5307092666626
10.621119618415833
644.7320601940155
259.9906471967697
365.5641518831253
125.49140286445618
55.296050786972046
161.15199661254883
95.83080291748047
1727.567302942276
73.20667123794556
643.0008952617645
57.53379821777344
92.33031022548676
0.0
22.83212971687317
40.434279561042786
0.0
109.54389095306396
73.39645147323608
0.0
45.65879821777344
54.72724008560181
0.0
265.1759948730469
114.35240459442139
0.0
45.55579948425293
91.50219774246216
0.0
8.580809831619263
169.56929874420166
0.0
73.09754073619843
203.63810324668884
120.70039105415344
149.96229648590088
34.223740220069885
15.129570007324219
0.0
0.0
70.02230894565582
0.0
95.4113107919693
0.0
232.76561212539673
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall        F1        F2      F0.5  \
0  0.986159   0.615385     0.8  0.695652  0.754717  0.645161   

   Average Precision  
0           0.496262  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73539861 0.75754958 0.75334162 0.73963652 0.76056047 0.74015877
 0.75718295 0.76479644 0.73994801 0.74647932        nan 0.74599309
 0.76727949 0.76028015 0.75468309 0.76731062 0.77434167 0.75937875
 0.75897381 0.73685842 0.75708421 0.76005111 0.75530031        nan
 0.75482572 0.75919353 0.76844559 0.76929728 0.75869435        nan
 0.77304543 0.75888361 0.75283517 0.76929124 0.7476864  0.77103255
 0.76843415        nan 0.73926242 0.76239062 0.75863906 0.75774208
 0.75913636 0.76726124 0.76323244 0.77748888 0.75450748 0.76680471
 0.75540701 0.75010556 0.75057809 0.75295913 0.75099548        nan
 0.7563166  0.75021883 0.76116225        nan 0.75930093 0.77083849
 0.77035824 0.758632   0.76953152 0.75456473 0.75897203 0.76170902
 0.7662421  0.75923824 0.76922916 0.76415866        nan 0.76025947
 0.76457987 0.7499142         nan 0.74713181 0.73986963 0.7726928
        nan 0.7640842  0.75899853 0.75748095 0.7649878         nan
 0.77287413 0.76410506        nan 0.77530169 0.74374483 0.7426397
 0.75015076 0.76669329 0.75680495 0.75897434 0.76742278 0.7580725
 0.76956369 0.76335524 0.75498957 0.75797231]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 805.816 seconds
Cross-validation score: 0.7774888750046622
Test score: 0.72992700729927
Best Hyperparameters: {'classifier__num_leaves': 16, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': -1, 'classifier__max_bin': 500, 'classifier__learning_rate': 0.3}
48.02828149497509
9155.051841527224
8385.008100777864
13315.189372062683
4128.574685424566
37.095245108008385
5.127729117870331
20.644091427326202
346.5490528643131
47.79309257864952
154.40984892845154
6.5067970007658005
2438.296104684472
146.7084041684866
11.004799842834473
0.0
216.11519028246403
0.0
2337.8282098025084
13.238457903265953
5136.154465660453
5.724570870399475
350.1395900994539
20.437212124466896
33.45570167899132
82.36834713816643
61.98269322514534
746.4719740748405
0.3414050042629242
16.374878525733948
25.48240089416504
0.0
1.5825459957122803
272.9891876578331
13.670609086751938
24.382623434066772
1071.28558370471
9.00777006149292
35.20889690518379
803.666381239891
28.979851827025414
0.0
0.0
0.0
84.53084874153137
0.0
104.08838021755219
15.502795845270157
0.0
134.51113887131214
84.06793466210365
0.6809899806976318
67.53766143321991
108.7903703302145
0.0
248.70607201755047
0.0
0.0
0.0
40.40105426311493
63.19612427055836
23.896011501550674
20.44290065765381
28.26357924938202
7.615514099597931
7.821287140250206
2.0252199470996857
146.38350361585617
174.22624045610428
73.19730377197266
1.4481170177459717
15.444076389074326
45.9779272377491
47.021200299263
46.86080211400986
10.161388039588928
174.59178605675697
10.819000244140625
24.755799025297165
288.31815734505653
7.680363178253174
5.961118042469025
24.929668977856636
603.4970977008343
9.44380909204483
0.23045499622821808
40.08610548079014
252.10366097092628
15.96111285686493
21.881806790828705
47.19868874549866
0.0
424.6250118613243
6.530439853668213
124.4536383152008
5.94556999206543
8.002939909696579
15.607375264167786
185.13995614647865
43.413964092731476
10.899721950292587
39.27698639035225
30.179588943719864
0.0
15.430374085903168
585.1300505697727
173.56241139769554
72.14544668793678
0.0
244.80492860078812
148.79054337739944
57.92934423685074
82.48850913345814
47.76310381293297
0.0
40.939674973487854
1270.3437358736992
47.295323103666306
10.095509767532349
6.630329966545105
14.456123888492584
0.0
37.29584014415741
26.91761614382267
0.0
114.10505640506744
204.02886146306992
0.0
67.17804534733295
98.67943236231804
0.0
428.7109221816063
54.07316327095032
0.0
1.431888997554779
19.84435424208641
0.0
241.97843000292778
18.971689820289612
0.0
116.67248895764351
415.9429611861706
134.15723483264446
9.643424034118652
11.027812123298645
49.757409662008286
14.193836897611618
0.0
43.03148630261421
10.636131167411804
63.387948989868164
13.834760084748268
173.74803599715233
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.989574   0.740741  0.727273  0.733945  0.729927  0.738007   

   Average Precision  
0           0.544113  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.7378994  0.77072514 0.75199809 0.75264233        nan 0.74228311
 0.73574378 0.75011559 0.75003162 0.7729956  0.74933292        nan
 0.7522963  0.76216049 0.75689806 0.73177149 0.7388999  0.77262312
 0.7605678  0.76553926 0.76553003        nan 0.75527526 0.76008875
 0.75727234 0.76343601 0.74289154 0.76595884 0.75017174 0.74995685
        nan 0.73601893 0.74669937 0.75336031 0.75546513 0.75830978
 0.73915121 0.74225822 0.74231505 0.75479064 0.75548463 0.75596476
 0.7526695  0.75465426 0.73642237 0.75536587 0.75939473 0.7539486
 0.75442925 0.77736001 0.74784035 0.74534614 0.75999226 0.74248042
 0.75576073 0.75341873 0.76424302        nan 0.76710265 0.71860967
 0.75867974 0.76784024 0.75361749        nan 0.75563253 0.7362753
 0.75736363 0.74915483 0.75052147 0.73114907 0.75183602 0.75659599
 0.75760008 0.7493396  0.75406115 0.75278582 0.74862428 0.75573296
        nan 0.74820411 0.76904463 0.74235641 0.76305085        nan
 0.75051384 0.76846057        nan 0.75620078 0.76852726 0.75518061
 0.73840413 0.74662985 0.76517264 0.75210485        nan        nan
 0.76085929 0.74691026 0.74617715 0.7774295 ]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100
Elapsed time to compute best fit: 828.334 seconds
Cross-validation score: 0.777429500758221
Test score: 0.7586837294332723
Best Hyperparameters: {'classifier__num_leaves': 41, 'classifier__min_data_in_leaf': 100, 'classifier__max_depth': 400, 'classifier__max_bin': 500, 'classifier__learning_rate': 0.5}
627.2221272352544
5842.312042072677
5200.611689481968
9914.920710918177
3197.4818468494122
54.56064917366166
83.52520897761133
42.26871597102581
570.4783386382478
32.464966096536955
127.97937034722418
78.96503173752717
1910.9802142938788
167.23399837708712
48.40625486720819
0.0
33.17976716245903
0.0
2678.0080966348687
106.82178502858005
1081.1201487250873
22.837410850268498
14.739861303503858
0.674999861366814
11.419575652500498
19.290774788474664
174.32666921883356
43.40705737355893
0.06528391037136316
26.923855748493224
150.669128576621
21.752348113805056
4.704775582482398
330.6750035676123
14.40639446440764
67.19526256083918
221.74881672751872
17.61371929591769
15.521176282985834
58.952381782757584
68.75666015274692
0.0
0.0
0.0
0.06990767949901056
0.0
39.703259559420985
8.47533691348508
30.411113502806984
185.9759302698294
56.274223513988545
0.0
23.94423385122718
149.78264150808536
0.0
83.82599412976924
0.0
0.0
0.0
29.02623697937088
28.116901019129728
34.92789207473834
45.12904863724543
10.093756295740604
11.200733601115644
10.111310758220498
0.21143199503421783
110.6877144500395
148.25583002978237
2.3136023795304936
5.000837445593788
6.273815366766939
0.7011824993387563
48.757897068389866
18.424393770634197
1.1483546725357883
109.6372728195056
0.7804912222563871
24.701801615592558
391.285748178605
11.028773018879292
37.491286023258
19.316846322799393
200.3368841617048
14.696058451176214
26.057230854908994
17.5887167260953
133.72565226179722
5.7075495950412005
47.94170224331174
22.739721149526304
344.8940124511719
28.54327645851299
4.429208750516409
16.14677320435294
0.000910595990717411
6.1988597902382025
17.421910892728192
91.66228807822336
0.23339577536535216
2.0669129055313533
4.846994073610404
66.10988758872554
0.18790000677108765
34.528997641471506
491.2320456929883
432.53581970270534
82.06690330704441
54.32099890708923
80.39249841027777
41.690344893759175
25.980521172867157
49.565714515720174
14.212265529531578
1.456381935538957
0.0023035259728203528
660.2228586477431
59.05735825430747
164.50852004451008
17.4431177515944
19.694339340723673
0.0
8.838234620921867
63.49436690895163
0.0
42.848120456401375
71.5746595397577
0.0
42.336504489183426
7.19144942011917
0.0
8.855806316481903
59.00962381303543
0.0
25.45280402812932
258.27731817376844
0.0
103.16265424195444
42.785538664335036
0.0
75.69683193729725
40.99546988999646
63.63778731216189
15.795771942968713
0.40792468791187275
26.19922358252734
187.27298018429428
0.0
101.8572972954862
7.786322165578895
36.5593103209394
0.0
43.0127927181602
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.990832   0.775701  0.754545  0.764977  0.758684  0.771375   

   Average Precision  
0           0.590155  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.74693462 0.7389811  0.75906263 0.74383734 0.71663268
 0.7373769  0.74474038 0.74041184 0.75306432 0.74881102 0.72936608
 0.74904967 0.71786649 0.74473798 0.75765346 0.74184606 0.72465601
 0.7136674  0.73321687 0.73152563 0.73655282 0.71563223 0.75294743
 0.73990951 0.72733559 0.70097013 0.75943926 0.75709453 0.71633893
 0.75589417 0.72560581 0.74445111 0.73359621 0.74619043 0.74661481
 0.7406721  0.74209557 0.72456817 0.73407471 0.73934671 0.73387906
 0.74448982 0.74323289 0.73758721 0.75172552 0.72644945 0.72874695
 0.76302057 0.70825539 0.73771478 0.71890518 0.74859674 0.75087897
 0.73490243        nan 0.74204801 0.72361366 0.72738185 0.72796224
        nan 0.74014855 0.72242415 0.74567356        nan 0.73236976
 0.73707892 0.72807835 0.74110731 0.74042777 0.74745338 0.7535175
 0.73800307 0.74372431 0.74154774        nan 0.75535428 0.75851339
 0.67636287 0.72146079 0.74009316 0.75114672 0.74814796 0.74217826
 0.74221221 0.74982082        nan 0.72050112 0.73460642 0.75497768
 0.7376478  0.75099147 0.74861981        nan 0.71713368 0.76822184
 0.74789188 0.74374615        nan 0.74085596]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 927.316 seconds
Cross-validation score: 0.768221844547316
Test score: 0.7982456140350878
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 800, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.1}
707.9303468118887
25525.744244309142
23086.249973788857
39555.50927916728
6361.738433107908
84.36851947568357
88.50636911392212
79.14168447256088
640.2453429661691
55.09505753964186
18.619122879579663
130.91822761297226
5189.992319360375
914.8600475788116
247.70913553237915
0.0
973.0449738129973
0.0
17688.553661460057
150.85047306865454
2590.445499755442
71.0497495830059
171.95514551573433
149.12577728927135
50.98988148570061
19.452203527092934
136.04056047694758
1543.5915262810886
659.0917045976967
108.02983843930997
154.58854484558105
44.323498487472534
21.94195020198822
308.849565343
12.048778288066387
199.39217386767268
2470.4188659116626
67.04159137606621
131.0326742529869
1765.3757100440562
59.041619434952736
0.0
0.0
0.0
88.83663523197174
0.0
288.812473282218
144.25367361307144
0.0
451.41797280311584
144.4154325723648
17.42579972743988
445.9945882856846
447.566325891763
0.0
1005.7307226210833
0.0
0.0
0.0
117.40676672756672
636.0230907350779
172.7040893305093
5.560670912265778
41.58971881866455
7.840691238641739
173.17655897140503
0.0
361.2043221630156
494.1477266922593
2.7989299297332764
37.86691343039274
127.83104623388499
224.65273897349834
218.35063821077347
823.8432419300079
32.93771366775036
421.6807787306607
11.304624564945698
160.6448915489018
618.8684124350548
17.82431684434414
30.55291773378849
166.4025597218424
940.3348204195499
86.2717846184969
10.831381022930145
55.20762025937438
737.6376039385796
137.6686550527811
89.9568462818861
80.47777604684234
46.813010692596436
280.67772233486176
91.03564774990082
240.6347268819809
20.633080005645752
46.656699419021606
50.70491969585419
271.96191127598286
45.68534756451845
65.28977847099304
200.39567977190018
75.68385002017021
0.1337449997663498
207.2955539226532
1527.2749350667
1323.5145170241594
343.14457738399506
10.669024288654327
487.1869123876095
366.06500166654587
35.03053115308285
82.52140909433365
156.13966053538024
99.05732890963554
306.24286330491304
2592.2533960677683
115.15329615864903
191.8346363902092
37.90911877155304
164.81832334399223
0.0
22.853609949350357
259.2744301687926
0.0
230.910543885082
103.97661921195686
0.0
48.65901041030884
189.19696226716042
0.0
1309.1638807356358
172.98294135183096
0.0
49.89784222841263
44.68182224035263
0.0
209.25410449504852
142.08083020336926
0.0
329.79939080774784
427.2160112336278
439.1203186735511
54.06425692886114
52.8181798607111
147.25057762861252
87.12190890312195
0.0
425.81741416454315
92.4002725481987
146.77778905630112
3.324860095977783
445.11111683398485
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.989574        0.7  0.827273  0.758333  0.798246  0.722222   

   Average Precision  
0           0.582506  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73737918 0.75051136 0.74879953 0.76725111 0.75978514        nan
 0.76210978 0.75005377 0.75765296 0.74191311 0.75095016 0.74687244
 0.74074954 0.72965649 0.74889509 0.76611147 0.75183946 0.73068665
 0.75720963 0.74780151 0.73961112        nan 0.75804165 0.75876996
 0.75452679        nan 0.75079639 0.75386777 0.73647527 0.7473734
 0.75709774 0.73960699 0.76641766 0.74764495 0.73079883 0.75567282
 0.75222981 0.74686468 0.74410739 0.729973   0.75477234 0.7431852
 0.75648857        nan        nan 0.75436161 0.7401287  0.75543242
 0.75694393 0.72312841 0.7084894  0.7411908  0.76712901 0.74388379
 0.76051151 0.74727977 0.77133839 0.74620662 0.7560974         nan
 0.75980881        nan 0.74025415 0.74736068 0.73366809 0.7612644
 0.74372565 0.7587733  0.73096503 0.75676284 0.75347453 0.7585547
 0.74566293 0.76192513        nan        nan 0.77079391 0.75997266
 0.75479459 0.74738067 0.77017846 0.74396632 0.75786606 0.75504306
 0.74944186 0.74854306 0.74433483 0.7160415         nan 0.74365697
        nan 0.75253037 0.74907338 0.75049092 0.76468501 0.7518692
 0.7306421  0.7531828  0.76572394 0.75980718]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 857.981 seconds
Cross-validation score: 0.7713383863198754
Test score: 0.7837837837837838
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 100, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.6}
53.77477713013832
3986.7453015338106
7518.217432206206
9147.400111228897
3026.52361257295
12.926664299418917
11.51329070702308
0.1233185505552683
35.91507577570155
0.3744950865908771
0.674387888982892
31.79958775729756
1460.3957630846521
148.50899052899535
5.287124045426026
0.0
29.75947341363781
0.0
1989.790193385486
20.086264463035832
629.2615212001838
1.8733691868110327
6.406178067773851
16.62460665742401
86.20368092936087
3.4988007287029177
1.2123443817836232
303.0028800875298
48.663021818734705
2.3069761847145855
0.23986114754551124
17.202802636427805
4.6828459311509505
102.7335796414518
22.88302213238785
37.33693404518999
379.14161065861117
67.83423122277416
22.46451422991934
427.9291414370367
9.46308211737778
0.0
0.0
0.0
10.855659955181181
0.0
17.010996611265092
33.11463548294377
0.0
18.47114685070119
4.416575026121358
0.0
78.49667107784535
46.10916880951845
0.0
246.824355348479
0.0
0.0
0.0
58.76378941691483
16.364125546941068
50.317296279797155
9.927780942060053
0.0
1.6939954289700836
1.4892161758616567
0.03679279983043671
46.79024107893929
385.1463642863144
3.8797172376301834
4.021486260998946
1.103033807605243
7.030819102656096
6.158033245883416
26.422593347811926
0.08263868209905922
208.66566619742662
0.15237172943307087
5.2561713125578535
232.58094533716212
2.144804558949545
33.56744308802355
46.27669045999595
396.01428586170266
33.33653414707094
10.152097396387035
7.87650296324864
173.1379199780058
14.622427418245934
43.83776855643373
114.33373584708897
11.89853423833847
51.50584722503845
4.733185756835155
133.52914488622946
1.910542757192161
0.9233612287425785
14.888406440193345
119.0831843215509
25.028645768295974
14.72077600855846
12.282726302917581
29.039256145257713
2.2498277369886637
9.044614571426973
194.89188512510736
166.1747339887952
20.43027331313351
5.989902484929189
161.27649629715597
69.65502610226395
32.171006385270225
49.742209910051315
31.576452318229713
0.008151787620590767
35.152045438606365
4.763440459965137
23.303579609608278
550.8712436400901
1.278099142597057
25.408071068664604
0.0
31.16437654432616
36.37931233686077
0.0
53.571286982041784
21.61100048365096
0.0
5.48020366957644
18.115548671106808
0.0
50.493770802165955
17.834597652303874
0.0
1.4216446655627806
65.71081880135871
0.0
31.68357247413951
53.87615556548826
0.0
89.84273560531437
5.130665989243425
28.133941994719475
172.96691810891934
9.02168047777377
10.55194792302791
19.157185110052524
0.0
6.842091182596531
26.715465755638434
92.88718709384557
0.07963749766349792
47.37160320795374
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.990832   0.756522  0.790909  0.773333  0.783784  0.763158   

   Average Precision  
0           0.602474  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73181427 0.75706473 0.74989199 0.75117967 0.74559514 0.75059999
 0.74613936        nan 0.75861925 0.74624563 0.74983869        nan
        nan 0.73857054 0.75177484 0.75677265 0.74268204        nan
 0.75373903 0.76143429 0.74569271 0.74847611 0.75764301 0.7624621
 0.76684187 0.7482018  0.75917338 0.77661066 0.75181272        nan
 0.74861415        nan 0.75489866 0.75807113 0.73678666 0.75671334
 0.76542639 0.74847501 0.74744498 0.7373095  0.75037409        nan
 0.75139799 0.75265855        nan 0.74609595        nan 0.7542225
 0.75749481 0.76260086 0.74817283 0.76177779 0.72814073 0.76101158
 0.75400382 0.75776505 0.74841507 0.75867737 0.74710952 0.74859652
 0.74892511        nan 0.75672286 0.73305276        nan        nan
 0.73582179 0.75875441 0.76539964 0.75847017 0.75196206 0.7413161
        nan 0.75124578 0.76253041 0.74993168 0.76005791 0.75105871
 0.73741587 0.74731718 0.74699666 0.74837397 0.75449083 0.7503466
 0.74891285 0.75539339 0.73911024 0.75020001 0.77070612 0.76888108
 0.75428556 0.7458322  0.74524721 0.76257152 0.75344388 0.7574371
 0.74862331        nan 0.7361187  0.75378851]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 873.574 seconds
Cross-validation score: 0.7766106611268546
Test score: 0.7946428571428571
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 300, 'classifier__max_bin': 300, 'classifier__learning_rate': 0.2}
500.3793072551489
15503.951076120138
10882.284945197403
15918.203206788749
4656.7188623435795
57.89630661532283
306.694844994694
23.397386759519577
670.0567840076983
35.763499442487955
98.26627999916673
19.03212121129036
1731.2651956230402
915.4595254920423
18.66629609465599
0.0
356.34723261743784
0.0
11634.852064434439
101.34619060531259
3452.5364047549665
19.34706872701645
20.52829896658659
38.47786822915077
42.33350884914398
27.184328380972147
207.64482226967812
621.9785247221589
711.0631678029895
15.701473653316498
449.42057529091835
6.756701111793518
5.66920605301857
270.9045175090432
15.100745782256126
153.04160154610872
833.4679723791778
77.76819097623229
100.17131838947535
1098.5159680582583
54.75449598580599
0.0
0.0
0.0
71.85277388244867
0.0
109.98346674442291
85.35264310240746
6.767570145428181
160.8591673821211
39.05536338314414
1.7305699586868286
169.8457683995366
60.26480096951127
1.8092399835586548
323.13212190568447
0.0
0.0
0.0
46.506816163659096
314.02599335461855
72.92079659178853
5.4722216203808784
11.828801810741425
34.15568217635155
132.8917609527707
0.2282940000295639
82.06288383901119
373.78044844046235
16.970577776432037
26.8869793638587
15.353984989225864
51.9689754396677
218.3602963089943
36.980264618992805
28.924362152814865
228.80764019489288
4.493351519107819
31.701012410223484
694.9494744427502
28.86005722731352
26.403024069964886
68.49582232162356
669.9800455830991
43.25366826727986
1.6230060532689095
11.08315448090434
541.3949926942587
11.469342298805714
67.26678927242756
65.94946404546499
192.99789810180664
301.28356371819973
2.5193631052970886
84.45154286548495
0.0
17.49620071798563
13.63860010355711
127.35455410182476
5.758127048611641
138.48671043291688
84.68402958661318
100.17203718423843
0.6733629703521729
37.96259928867221
285.4303892850876
624.8395599611104
141.7517738044262
7.4109601974487305
250.76126046478748
157.5514186695218
75.82241919636726
171.57378667593002
101.06016463413835
16.60733860731125
131.67329590022564
851.2047844827175
19.20159736648202
103.38158981502056
23.271387197077274
88.07943432033062
0.0
12.998788081109524
55.573256358504295
0.0
172.56437576562166
134.63690416887403
0.0
18.74017819017172
97.43020022287965
0.0
1078.682202078402
34.55277567729354
0.0
18.265690576285124
40.92367820441723
0.0
31.97842176258564
36.701715383678675
0.0
49.34859801456332
77.40703245997429
100.42506112158298
36.70334830880165
65.26383624970913
29.286064378917217
226.9810445792973
0.0
8.105172388255596
130.43884994834661
227.83065398037434
0.0
172.4137014374137
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.990653   0.741667  0.809091  0.773913  0.794643  0.754237   

   Average Precision  
0           0.603851  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.7399398  0.7393779  0.75181973 0.73611157 0.73088595 0.75183508
 0.7400553  0.71743636 0.75621425        nan 0.7506604  0.74981567
 0.72807317 0.74894263 0.72856498 0.73163309 0.73327246 0.74872676
 0.74983191 0.75148302        nan 0.73898609 0.74753595 0.73658596
 0.71859876 0.74622552 0.74055329 0.72862487 0.7385386  0.73156559
 0.74611283 0.75493665 0.75313663 0.72000566 0.7453987         nan
 0.75078534 0.73657856 0.760859   0.75875702 0.75466648 0.74790472
 0.7576954  0.74452774 0.75840884 0.74557624 0.72572158 0.75317981
 0.75218168 0.74376272        nan 0.75276564 0.75132894 0.75976135
 0.75080961 0.75146201 0.74331297 0.73905237 0.7272676  0.7454194
 0.74514887 0.745175   0.74840239 0.74146335 0.7622233  0.73413225
 0.74394754 0.76177228 0.75335018 0.74595537 0.74313151 0.73981983
 0.74813418 0.73113241 0.73768857 0.74050236 0.73081804 0.72492778
 0.7438382  0.74892275 0.7539133  0.73429969 0.74219471 0.7438636
 0.74449263 0.74011937 0.74116733 0.73732862 0.73797339 0.74973581
        nan 0.74502386 0.74883988 0.73353507 0.74081355 0.74775162
 0.74050306 0.74292697 0.74680132 0.73234983]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=900
Elapsed time to compute best fit: 960.982 seconds
Cross-validation score: 0.7622232963533995
Test score: 0.8078231292517007
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 900, 'classifier__max_depth': 200, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.1}
415.66060596704483
21583.77308790802
28323.001241094666
38968.67736136052
7571.20852402132
12.125515542924404
61.31546548753977
11.781280040740967
650.704852104187
90.07192993164062
187.19873070716858
24.71340411901474
6348.407913200557
1131.4086582208984
125.66016656160355
0.0
497.6633353829384
0.0
9904.829173996288
14.974050968885422
6210.60165688589
26.59670801833272
121.33102063741535
38.0199918076396
99.9158426199574
15.12837028503418
110.83813481032848
3702.798429493414
1.7627587541937828
7.53587007522583
307.365634560585
44.49211025238037
12.166947962716222
414.18322637910023
11.766909122467041
143.40917635895312
769.916128131561
102.38637065887451
35.90668606385589
2294.230988264084
78.72597980499268
0.0
0.0
0.0
74.3503882746445
0.0
390.2185983574018
57.762279987335205
0.4495700001716614
699.2243438959122
79.07432970753871
2.1768100261688232
97.2572138607502
112.65444219112396
0.0
835.6486795000237
0.0
0.0
0.0
82.596519947052
453.59508991241455
73.98060300946236
95.67771816253662
171.98739886283875
2.7628400325775146
7.792300224304199
4.188910007476807
200.96923327445984
288.1021873950958
27.965060234069824
23.062320709228516
105.42477050423622
130.79670244455338
39.15014307573438
419.21572284423746
26.67539882659912
358.2877886016213
4.555340051651001
124.99049913883209
992.9771746397018
0.02049499936401844
21.84983991086483
157.57697081565857
879.6983861923218
24.15776002407074
0.009860680438578129
12.464977905154228
935.0579253435135
14.401931315660477
128.52807168662548
147.83551993966103
1418.5346908569336
620.7327248416841
36.9702804684639
372.11860275268555
9.461638395674527
5.108320236206055
87.5582823753357
328.4620305299759
131.9012894630432
60.72720259428024
36.889786064624786
31.934619426727295
0.0
30.17577163130045
1154.4788772976026
1790.7980318069458
244.80187940597534
12.606190204620361
687.6213068366051
311.12205159664154
123.64055967330933
372.52446937561035
103.53916249796748
53.90751075744629
19.100677602924407
2654.927269935608
117.08089995384216
332.6154317855835
44.471320152282715
80.0076106581837
0.0
19.08584314584732
217.88663366436958
0.0
240.5552613735199
121.84560549259186
0.0
436.81680488586426
50.593585819005966
0.0
533.7755556106567
535.8218979979865
0.0
9.571900717914104
198.36371651478112
0.0
38.27277970314026
152.30896067619324
0.0
125.51375806331635
322.4420803785324
165.54649186134338
313.38333401083946
201.41646206378937
21.802445948123932
122.17728900909424
0.0
91.02583885192871
143.62877917289734
85.65374799072742
0.0
232.00411595203332
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.987776   0.641892  0.863636  0.736434  0.807823  0.676638   

   Average Precision  
0           0.557058  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.76962944 0.77749499 0.75903434 0.74308397 0.76979416 0.76489429
 0.74562908 0.76776487 0.77977387 0.76290008 0.75322143 0.77635449
 0.76941681 0.741539   0.76876733 0.76212395 0.77055769 0.75869788
 0.74087644 0.77037851 0.77368878 0.77530117 0.78198438 0.75350163
 0.76536712 0.77053242 0.77505639 0.768974          nan 0.75527924
        nan 0.74568295 0.76947454 0.77225437 0.78724631 0.74993713
 0.77129444        nan 0.78343883        nan 0.77759964 0.76247972
 0.77756854 0.76654012 0.76444383 0.76239282 0.77762953 0.74372391
        nan 0.76441605 0.78761064 0.77084976 0.74983243 0.7724092
 0.7370087         nan 0.75516722 0.7813062  0.74481323 0.75929327
 0.74435466 0.76840631 0.75229796 0.76752395 0.77017415 0.77396781
 0.784835   0.75400996 0.75508215 0.7751486  0.73943943 0.77500281
 0.7633358  0.74441644 0.76973796 0.74886247 0.75659931 0.75404447
 0.73961893 0.77611353 0.76871718 0.73143771 0.77938868 0.76522658
 0.77088661 0.75670367 0.75247235 0.76385491 0.75091651 0.75280124
 0.76796957 0.75328741 0.75590596        nan 0.73436315 0.77578759
 0.76601259 0.75451964 0.78122266 0.75238163]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 951.752 seconds
Cross-validation score: 0.7876106362979539
Test score: 0.7246376811594204
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 300, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.2}
237.57835959918646
16098.559991507425
9164.9984589416
21581.381829275135
1520.3637174285063
238.80193851169335
51.09901468617318
20.651214899896786
165.9593024100891
39.2026789165684
270.44877740734955
24.75783678146672
1568.178229789055
771.8461761699523
16.927052582709848
0.0
232.9727816280556
0.0
6255.759051982327
152.64634448019072
3355.2582621100287
19.38570076595423
327.5460296016063
10.62862127457629
52.29905150413106
25.531013589773465
34.61474611603444
4301.1791995211415
326.185866951615
205.81592639953305
209.77266995771788
21.699762702010048
10.123864922177745
332.8587100780272
36.360595224861754
163.46854926960077
1061.9686394019882
36.70919202968969
80.14660239017189
165.17383861466044
83.94293194852798
0.0
0.0
0.0
63.59088343754411
0.0
140.7368594840977
190.62451664814625
0.09012492746114731
290.8088814103976
51.60437980780989
0.009042209945619106
99.6402091318887
92.86508248617935
0.0
233.49306535521555
0.0
0.0
0.0
62.238153152030364
265.69049111804475
183.44636231652566
32.4531242554076
11.73529629866276
12.047148267258308
5.820342811755836
57.02141756564379
129.57469459543174
268.3822194912139
22.486463991894183
14.770631408022382
92.45435157520414
95.39643610485655
33.30342465269223
172.98718685509016
20.830579660372173
281.16192763347465
22.716561501543197
59.69798107891084
229.19854355195775
22.796518117189407
64.63202944174384
112.93133466526122
745.7428918911244
48.2052405407851
20.238489010814604
27.488951061340984
284.2225958858051
50.19034866836744
28.57098577337638
188.57081686146557
0.041831371490843594
209.11766633269144
34.66165022018686
132.3916026877414
1.5274258508579805
12.030621714890003
14.853269220589937
285.7602342697792
48.14479336282966
19.04399246170538
125.45541593444068
445.8139722646447
0.5784760117530823
104.57495676120743
310.70377407816704
595.4611194823183
182.5992232763674
22.765404943009344
281.2621937538497
168.13208650511027
108.3902091011405
210.25989797990417
111.75282756515548
65.49703132569266
185.15309648886313
2523.1280122079365
52.639187191955806
98.57772865261722
248.33953684396784
118.4996473843421
0.0
17.50664222074556
113.12762171977533
0.0
199.83106193332716
86.89220939237205
0.0
11.425671700168015
72.32551712587883
0.0
391.51616999460384
55.48143127366802
0.0
20.125029945108388
67.06259006477558
0.0
614.3382567905355
42.09557975362986
0.0
75.01162567197676
273.78491313280387
172.42107747772707
43.316873585791654
84.97599916462786
20.436573764313607
52.58589388968903
0.0
26.349508422555914
62.73536915771274
184.66568345624748
0.0001963319955393672
167.4180512154163
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.988855   0.714286  0.727273  0.720721  0.724638  0.716846   

   Average Precision  
0           0.524873  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73679658 0.7524584  0.73198395 0.75873861 0.7669097  0.75550973
 0.74594265 0.73955442 0.73113674 0.75399348 0.75763426 0.75934827
 0.75727175 0.75163108        nan        nan 0.73196327 0.76292135
 0.75472224 0.75170286 0.76013181 0.750553          nan 0.7631209
 0.74435737 0.74828053 0.74663784 0.73536509 0.75991126 0.75462986
 0.72452455 0.73990876 0.75914563 0.74067643 0.75631036 0.72970526
 0.74665516 0.76201671 0.74364217        nan 0.7291117  0.75102153
 0.7674009  0.72318613 0.75052265 0.75536856 0.75704964 0.73949532
 0.74275198        nan 0.7665182  0.73319896 0.7399253  0.76933666
 0.75222654 0.75299283 0.73759123 0.75827698 0.73764497 0.75963436
 0.75115746 0.7494728  0.74933181 0.74322139 0.75017176        nan
 0.75679436        nan 0.73635683 0.75640238 0.75338977        nan
 0.73642733 0.76028354 0.74652882 0.74832345 0.73648431 0.73568356
 0.75752067 0.74751426 0.75688798 0.75195308 0.75575359        nan
 0.76062679 0.74968571 0.73316893        nan 0.74838081 0.75273032
 0.75519489 0.7443941         nan 0.7352345  0.75729862 0.73012318
 0.7601824  0.72596757 0.75373428 0.75971078]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=700
Elapsed time to compute best fit: 930.494 seconds
Cross-validation score: 0.7693366589019909
Test score: 0.8053097345132743
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 700, 'classifier__max_depth': 100, 'classifier__max_bin': 200, 'classifier__learning_rate': 0.4}
151.40734034962952
7880.68117122259
6057.1869440395385
10376.945997229777
3859.335512764752
44.65993099659681
66.78186300955713
11.732788145542145
545.2131765764207
14.828307460993528
150.2508474048227
7.069125443696976
2727.1228758785874
139.155800960958
135.26461688056588
0.0
270.882425468415
0.0
2220.048476483673
26.915326841175556
1418.4414214994758
9.15337105281651
25.467364843934774
10.994369560852647
10.001342937350273
11.02942705154419
249.54428645968437
606.1510393358767
13.328171983361244
102.58114916086197
305.2755402736366
3.1918965354561806
4.144144851714373
146.39423115924
40.447892390191555
52.74311938509345
590.6010641567409
19.187746062874794
96.86607506498694
125.42698384821415
17.67463525943458
0.0
0.0
0.0
11.342143081128597
0.0
97.97217720653862
34.31700777634978
0.0
148.1810300387442
22.018977995961905
0.0
23.380438916385174
75.35939607769251
0.0
447.8637636639178
0.0
0.0
0.0
22.92541044205427
176.8587296064943
54.347917733713984
6.223251208662987
5.356225274503231
26.08713493309915
19.388070162385702
0.07401499897241592
112.58570504188538
54.23915881104767
0.40339699387550354
2.9193366169929504
6.047308713197708
169.5874179005623
90.64678306877613
10.375586554408073
33.919325299561024
170.53160908119753
11.043887663632631
21.547082537785172
178.6433094367385
1.4266400337219238
21.670484993606806
35.787654999643564
489.77155677601695
40.447127716615796
14.45666791126132
17.465124437585473
259.74334824830294
3.5452558994293213
2.377339992672205
52.13049554824829
499.19000244140625
56.48798218369484
2.2368500232696533
57.39245643094182
0.19892629981040955
1.506160020828247
3.2713868524879217
87.70548941195011
5.293485024943948
9.47605799138546
20.75269539654255
16.514942528679967
1.0578577164560556
23.582101445645094
202.568702545017
314.48053527995944
103.27592760324478
21.606541097164154
178.11209416016936
201.0167453289032
57.875796331092715
306.3291867263615
9.92970410734415
15.279598340392113
1.4505907781422138
1189.7282160576433
107.91634712740779
161.67683577351272
13.946248538792133
42.78781193867326
0.0
8.18575163371861
65.91532519087195
0.0
159.17427656799555
32.69199698232114
0.0
138.92327028512955
12.67180441506207
0.0
68.55417622253299
124.44200512580574
0.0
13.689640402793884
147.28331807814538
0.0
20.227257563732564
22.2400319352746
0.0
5.832377523183823
200.8742548264563
86.94649628549814
26.554420821368694
55.747086556628346
12.240263350307941
2.1552440226078033
0.0
41.07597226649523
35.93071210384369
67.41464263945818
2.382152244448662
9.725231548771262
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1       F2      F0.5  \
0  0.990473      0.728  0.827273  0.774468  0.80531  0.745902   

   Average Precision  
0            0.60567  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.74603018 0.74320953        nan 0.7338543  0.74576459 0.77388344
 0.769935   0.74248808 0.74928824 0.74729614 0.75599542 0.74101435
 0.74993981 0.75783803 0.72479244 0.76409472 0.74992942        nan
 0.76305583 0.74329741 0.75588939 0.75688538 0.75634529 0.75111104
 0.75487141 0.72634723 0.75916991 0.75414277 0.76165051 0.76672022
 0.76079165 0.75469016 0.74867204 0.74766502 0.76282932 0.75761401
 0.75049396 0.76116423 0.74186481 0.7621185  0.76317011        nan
 0.74481522 0.75835767 0.74393351 0.75518675 0.7578066         nan
 0.75401024 0.76574383 0.76033653 0.76133992 0.74218131 0.74374223
        nan 0.73616227        nan 0.75013399 0.76519091 0.76096012
 0.74332757 0.77799782 0.7606033  0.73872914 0.75692087 0.73569425
 0.76000348 0.75321285 0.73873966 0.74670927 0.76081956 0.75393943
 0.76365439 0.77461815 0.74297448 0.73352664 0.74916449        nan
 0.73791041        nan 0.75080854 0.76130793 0.75044548 0.7668685
 0.74108508 0.77373622 0.74133389 0.74067072 0.74847467 0.74760089
 0.75126647 0.76847017 0.75981511 0.75077215 0.75075707 0.7424524
 0.75209883        nan 0.74003207 0.77083124]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 844.190 seconds
Cross-validation score: 0.7779978185742117
Test score: 0.737410071942446
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': 700, 'classifier__max_bin': 400, 'classifier__learning_rate': 0.5}
557.4604992446257
6657.666198436345
6343.500815698295
8994.095472802117
494.56898554298095
40.94426728494
3.574464501347393
17.004347468027845
44.21422635274939
47.82824148226064
192.38930371869355
9.213593470165506
1205.2896079054917
510.15160298586125
34.255071519874036
0.0
80.41936054709367
0.0
3439.1148183771875
16.296975343488157
1695.365697096102
4.077788443537429
13.200447410810739
57.13143818755634
3.433830439229496
3.7217356838518754
21.16249776945915
1719.1220119028585
324.6771603364614
11.198846295592375
36.31756476312876
0.004616810008883476
1.3450192334130406
171.984565751045
13.79023194720503
71.13695869891671
468.8768991282559
0.3647953304462135
22.892067558481358
229.45372305589262
28.232127589406446
0.0
0.0
0.0
54.12533444131259
0.0
161.35010214499198
15.225509082200006
0.09121730364859104
36.56351105408976
5.269847726623993
0.0460209995508194
70.74072288098978
94.56313588563353
0.0
140.8100606170483
0.0
0.0
0.0
19.445368129992858
4.5333503182046115
14.836110481701326
8.704755153506994
4.596179962158203
0.008299140259623528
98.04291738430038
9.92179012298584
10.247336191881914
72.00666993681807
0.0
39.52693120483309
47.874982772860676
5.612545948941261
60.1986564877443
68.1095209174091
17.734255004441366
42.46135320828762
2.58614088955801
9.738812816969585
330.23502674512565
13.988256948417984
50.08049151231535
21.837151634681504
187.80434355768375
25.22956437501125
5.67118311766535
20.15562309557572
190.08292438136414
18.975299835205078
18.13807102292776
72.29492291016504
13.41800091508776
27.74647876783274
6.394509971141815
33.26674068288412
0.0
3.3589250296354294
0.18803296540863812
88.40110653475858
1.1667736095259897
6.39460457698442
10.092782971507404
0.9238743329187855
0.0
0.13123231218196452
375.45184680679813
269.23860601260094
89.25804414320737
19.994344890117645
36.87285560625605
141.03184486064129
10.400118584278971
7.3586181295104325
13.974321533809416
6.189340114593506
1.9333576378412545
1433.513671523775
46.91076183319092
2.8498990358784795
17.862187694758177
15.985627456801012
0.0
10.457460508914664
30.191205273382366
0.0
128.0845225385856
34.36051670671441
0.0
30.515619358513504
18.138586563407443
0.0
57.11126112192869
3.8156211115419865
0.0
3.907058674842119
0.17882589250802994
0.0
259.0959953921847
30.123044413980097
0.0
99.58384401560761
11.497281560441479
118.87623021914624
46.82979462342337
0.8667649670969695
16.127092353242915
20.902885496616364
0.0
18.86542454175651
30.21619175170781
124.9001391705242
0.8466827748343349
7.602262565400451
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1       F2      F0.5  \
0  0.988855   0.706897  0.745455  0.725664  0.73741  0.714286   

   Average Precision  
0           0.531993  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.7432669  0.73174776 0.73553661 0.75042843        nan        nan
 0.72955806 0.75058183 0.7531832  0.75268364 0.76854225 0.74785002
 0.76505011 0.74180749 0.72216909 0.73149015 0.75991356 0.76348826
        nan 0.74741889        nan 0.74506741        nan 0.75031489
 0.73263271        nan 0.74994514 0.75104954 0.75274592 0.73971431
 0.74501964 0.76534342 0.73322658 0.7632481  0.76600491 0.74841751
 0.73784126 0.75399037 0.7572354  0.76206037 0.76607411 0.76522391
 0.73318781 0.76823367 0.74852894        nan 0.75271902 0.75533842
 0.76699612 0.7436909         nan 0.74238238 0.73633381 0.76025196
        nan        nan 0.74831825 0.72646753 0.73623309 0.72238491
 0.75919967 0.73796247        nan 0.76145684 0.75131726 0.74406007
 0.7599616  0.76693304 0.75251237 0.76966641 0.7478074  0.75935859
 0.76914426 0.75739982 0.7229084  0.74799728 0.73677373 0.74474135
 0.73684571 0.7504563         nan        nan 0.756893   0.74753516
 0.74195909 0.7343185         nan 0.7525914         nan 0.7491592
 0.76038868 0.74021153 0.74805755 0.73863762 0.75892823 0.73582007
 0.76007841 0.75681986 0.7527538  0.75517358]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500
Elapsed time to compute best fit: 827.076 seconds
Cross-validation score: 0.7696664149046689
Test score: 0.7651245551601424
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 500, 'classifier__max_depth': 300, 'classifier__max_bin': 350, 'classifier__learning_rate': 0.4}
67.22259841696359
6128.416219544684
8704.019675131189
10011.379788229242
3358.032578983868
10.002024161629379
29.832964400295168
1.767660197801888
137.68109029816696
3.9070545935537666
120.20594093226828
21.763805378926918
1354.383172732545
268.71246091410285
12.766718034632504
0.0
297.7219767834322
0.0
4667.625929210102
21.566144762560725
747.7747323174626
35.477709426311776
21.005709485849366
12.465545498766005
10.770027806749567
24.248493973980658
15.858722422737628
1110.7484261116479
343.39757251087576
19.202373525477014
0.7017127759754658
6.190093015087768
6.940194139257073
135.40895243082196
16.36058725649491
35.3134457282722
493.2170056090981
15.38866749254521
42.92229077225784
528.4458734907676
11.652768392581493
0.0
0.0
0.0
186.42366347671486
0.0
75.28661113698035
20.870015962980688
11.115569829940796
160.5125450186897
159.2220934573561
4.884638950228691
36.47631460730918
81.74366882140748
0.0
309.5832405099645
0.0
0.0
0.0
36.896731186658144
285.02208784455433
50.99360315682134
2.8530200745444745
0.008675339631736279
2.282769054174423
19.480068352422677
1.5878734737634659
85.07929005706683
146.09329034620896
0.9540826864540577
21.301613147696557
19.400875533232465
87.67136670369655
23.460974670946598
126.39616118650883
9.685346252284944
158.90406299964525
13.898814655549359
32.04168286427557
272.7884383443088
6.4831430396588985
21.97982883316581
17.277453676157165
461.37229150743224
30.09146843018243
2.6218472220934927
13.19428617702215
126.40831787977368
25.65624694013968
43.58726011053659
32.480592019390315
0.0
10.229303595027886
1.5948175601661205
53.245959546417
0.008766310289502144
0.06713171117007732
0.6518749645911157
65.99085723864846
4.672136551700532
3.401561724022031
6.73285902896896
6.855900329304859
0.0038049800787121058
12.346934592351317
256.3463248250773
751.2761542579392
83.92905342939775
0.051988501101732254
162.56448003975675
106.83576906635426
33.0090922084637
9.475476510939188
19.541215060977265
10.013703839154914
17.187557979719713
1069.5707833704655
47.37683864077553
10.338971088640392
3.979696772992611
15.224954533623531
0.0
10.316850038128905
74.6449425049359
0.0
111.07436381431762
41.2572525634605
0.0
45.29946345370263
73.18291952447908
0.0
230.57161061558872
63.08875240269117
0.0
9.079489864991046
10.970098805380985
0.0
16.106933856848627
67.93876532427385
0.0
291.87882754177554
97.5027615763247
108.19390135724097
54.95037218858488
17.40532593068201
27.011088826046034
134.80536517431028
0.0
44.742213307879865
16.467061960604042
58.29349953972269
9.822144214063883
114.48922210838646
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.989214   0.704918  0.781818  0.741379  0.765125  0.719064   

   Average Precision  
0           0.555432  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75736868 0.7602934  0.75166523 0.76851691 0.76493311 0.7539111
 0.75856256 0.74499242 0.75811461 0.75712218 0.7525106  0.76408079
 0.77255102 0.76697609 0.74559639        nan 0.75381191 0.76974013
 0.74567131        nan 0.7438461  0.73859576 0.75618295 0.75485608
 0.73928291 0.75448374 0.75981634 0.75820009 0.7256605  0.7761932
 0.76390428 0.75505809 0.7485368  0.74804273 0.7486219  0.73274858
 0.76313313 0.76956909 0.73630073        nan 0.77017752 0.77204931
        nan 0.76370848 0.77261349 0.77029135 0.75231654 0.76414824
 0.74676753 0.74016397 0.74498801 0.75680901 0.76415461 0.76082479
 0.75710142 0.74973996 0.75383347 0.73959968 0.77086527 0.75316677
 0.74863761 0.73013545 0.75813343 0.7618159  0.76911069 0.75424067
 0.75305543 0.75185084 0.75894225 0.74887432 0.74125399 0.77739811
 0.7638766  0.77702921        nan 0.72872688 0.75671904 0.75191547
 0.76710604        nan 0.76405508 0.75293213 0.75379447 0.75364733
 0.77525317 0.75078564 0.75009594 0.76226737        nan 0.74610869
 0.76356155 0.74258459 0.74232057 0.76253529 0.74926595        nan
 0.73598383 0.75645428 0.761156   0.76125125]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=600, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=600
Elapsed time to compute best fit: 943.350 seconds
Cross-validation score: 0.7773981113945647
Test score: 0.7456140350877193
Best Hyperparameters: {'classifier__num_leaves': 21, 'classifier__min_data_in_leaf': 600, 'classifier__max_depth': 300, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.1}
146.37356609106064
24338.679717570543
23948.474011957645
40548.634773254395
6379.105147600174
104.31111907958984
84.17657041549683
43.311930418014526
309.0399570465088
60.79490041732788
956.1552734375
162.71448040008545
7038.08465886116
1018.1741373538971
106.2070426940918
0.0
533.382763504982
0.0
9889.03406715393
326.4426894187927
4538.056665658951
22.594080209732056
61.17169165611267
0.0
243.2334268093109
151.65923929214478
262.0335810184479
3069.260249376297
742.4539794921875
34.311949729919434
488.9989929199219
57.49809980392456
7.989739894866943
591.2985054254532
9.701419830322266
66.10248041152954
2077.1971006393433
39.43873977661133
13.791860103607178
2205.6185507774353
28.2782301902771
0.0
0.0
0.0
222.82157349586487
0.0
312.06712913513184
91.07712090015411
0.0
382.24802231788635
50.747411131858826
42.63084936141968
266.51202952861786
379.8254964351654
0.0
272.447301864624
0.0
0.0
0.0
177.64584112167358
311.20153045654297
99.68369632959366
12.37332010269165
30.334219455718994
18.424389839172363
401.7669982910156
0.0
198.7571403980255
1386.4077327251434
20.402920246124268
55.01288741827011
7.17253777384758
181.79997372627258
127.65358781814575
28.66939926147461
3.5432300567626953
416.3802378177643
7.748879909515381
132.09476971626282
582.8612057715654
37.38497066497803
11.84430980682373
128.4893012046814
2279.4150264263153
48.763070821762085
56.409860014915466
22.683290481567383
1064.0383967161179
190.27261233329773
271.1260966062546
135.94890928268433
248.33216738700867
389.1735100746155
13.01550006866455
293.7206244468689
14.10890007019043
4.140439987182617
14.614720344543457
521.1505756378174
66.38401985168457
79.73556900024414
25.792479395866394
50.12111032009125
0.0
138.89309787750244
527.7468154430389
1065.136142730713
236.4570791721344
58.74368095397949
623.4448335170746
1222.3264071941376
373.6147131919861
198.70370244979858
121.76728820800781
195.69585990905762
162.3727389574051
3351.102025270462
165.7298904657364
167.64045989513397
15.657690048217773
29.675050497055054
0.0
16.64744997024536
129.68005084991455
0.0
413.36224937438965
67.4286698102951
0.0
322.42304372787476
71.13016986846924
0.0
619.7826908230782
145.9662003517151
0.0
63.09070014953613
258.47557306289673
0.0
109.91677904129028
267.3668427467346
0.0
289.4808156490326
406.6571309566498
166.84998965263367
99.85358810424805
62.45170849561691
43.0097599029541
12.575519800186157
0.0
83.21744167804718
74.01228141784668
124.44131922721863
2.7800700664520264
339.53508949279785
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.987417   0.653846  0.772727  0.708333  0.745614  0.674603   

   Average Precision  
0           0.509739  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.76045142 0.74336915 0.76360551        nan 0.77848611 0.76763528
 0.75577653 0.76732381 0.7569326  0.7580235  0.76784021        nan
 0.74915703 0.76339869 0.76661345 0.75923551 0.75130365 0.76465394
 0.761108   0.7613359  0.77308382 0.75534435 0.76508244 0.76465904
 0.75368268 0.75387    0.7366248  0.76789008        nan 0.76404308
 0.74158371 0.76334074 0.74975124 0.75104858 0.73793785 0.75616472
 0.75266908 0.72617014 0.77315168 0.76053698 0.75932865 0.7483294
 0.77513057 0.74950826 0.7637727  0.74856203 0.76614085 0.73966399
 0.77059509        nan        nan 0.75102981 0.76645217 0.76006238
 0.75053798 0.74664263 0.75173814 0.75141522 0.77374341 0.75707914
 0.75977656 0.75578802 0.75835011 0.76564902 0.76274507        nan
 0.72563503        nan 0.76464399 0.74467459        nan 0.76708814
 0.78299131 0.7722763  0.75992294 0.75958792 0.75271321 0.75032428
 0.74590976 0.75205771 0.77606594        nan 0.77082162        nan
 0.75205255 0.76298976 0.74023997 0.76724937 0.74530094 0.76053656
 0.76730663 0.76494127 0.76213813 0.74984682 0.7510089  0.75073246
 0.76921023 0.75530162 0.77473328 0.75828343]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 932.235 seconds
Cross-validation score: 0.7829913129432652
Test score: 0.7705779334500877
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 800, 'classifier__max_bin': 200, 'classifier__learning_rate': 0.2}
503.53654643148184
16048.137849993072
11453.626001955941
19440.131915293634
4539.810506455688
181.70229610428214
103.62627160549164
12.396848261356354
258.84066320210695
17.72942914068699
150.32912646397017
89.60138416290283
3387.454396562651
462.87103541754186
98.6601151227951
0.0
320.07138638943434
0.0
7365.072204310447
116.1487614698708
1584.6007845001295
85.62439227104187
28.435487017035484
6.317295283079147
46.69336250424385
122.22686462849379
119.09024128317833
1843.742940178141
9.450595065951347
14.004234954714775
183.0853254236281
8.89461188018322
14.582324296236038
407.86565317213535
19.00422975420952
321.327411419712
455.94561112392694
21.55126489326358
80.4371457695961
456.0037360880524
39.45417867600918
0.0
0.0
0.0
26.896610306575894
0.0
69.47345727682114
13.725480392575264
0.0
153.08511590279522
39.49210737645626
0.0
163.4650929942727
90.77798734232783
0.0
463.9681033939123
0.0
0.0
0.0
54.136728174984455
373.6673659849912
53.701126804575324
86.91673216223717
14.85139948129654
30.616103410720825
208.72749203443527
24.648714035749435
113.23773506656289
124.89445981010795
16.37186063826084
6.6483679711818695
13.590522650629282
70.04321677237749
52.71196918934584
316.518048517406
39.157653108239174
150.91429361980408
28.429734759032726
42.21120967436582
375.4148604273796
6.889445915818214
43.56842484751178
91.36969318985939
672.6255097016692
93.07380486000329
0.6740340851247311
17.485801681876183
328.1128901988268
24.27279931306839
164.6881795451045
51.72864302992821
0.1584170013666153
254.35652375221252
222.7428458929062
167.42685825750232
12.350767649710178
15.096667904406786
69.12850791215897
178.86914072185755
42.47155425325036
31.045399636030197
51.800717290490866
55.12510247249156
0.0762040987610817
48.29346834868193
747.7407828718424
334.7956305369735
425.39442877471447
68.61389642208815
212.85606690496206
236.35123218595982
44.30609002709389
32.736859443597496
198.96385594829917
94.81056389212608
112.90689964219928
1592.942161526531
99.73619796335697
69.61238326132298
19.119533821940422
18.96544793288922
0.0
28.20112881064415
193.67412536777556
0.0
159.83058730512857
133.73602213338017
0.0
5.632377907633781
108.95600980520248
0.0
400.9283349364996
94.14864005893469
0.0
33.56608907878399
192.99005794897676
0.0
550.2409070581198
69.7969271466136
0.0
76.46328973397613
166.0099638029933
80.83672469109297
226.24736988916993
48.82139960490167
29.476898251101375
41.42467795684934
0.0
135.9801781028509
9.38417787477374
103.03876948356628
2.717639923095703
173.66803320124745
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision  Recall       F1        F2      F0.5  Average Precision
0  0.988316   0.671756     0.8  0.73029  0.770578  0.694006           0.541359

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75763064 0.76997392 0.76097212 0.74865977 0.71988523 0.74804668
 0.75448593 0.75949545 0.74366396 0.73686625 0.73614323 0.78126283
        nan 0.77736787 0.7527206  0.75313591 0.75751933 0.76824155
 0.74821995 0.74921831 0.7509811  0.75853194 0.7602257         nan
        nan 0.75254093 0.76482451 0.77288604 0.74813209 0.76412381
 0.76623188 0.76140372 0.74688701 0.74344713 0.74424878 0.74685294
 0.76424225 0.75554036        nan 0.74009701 0.74189519 0.73864597
 0.75824908 0.76709418 0.767143   0.75734707 0.74478438 0.767115
 0.75339976 0.75312912        nan 0.75216317 0.74901934 0.77688075
 0.72080057 0.7357736         nan 0.73520982 0.76063748 0.76238415
 0.76830856 0.7522646  0.75672813        nan 0.75468103        nan
 0.74675786 0.75517149 0.76836826        nan 0.73537244 0.74985069
 0.76136907 0.74879721 0.774404   0.7587943  0.7410265  0.74086103
 0.76599601 0.73206307 0.73390157 0.75329466 0.75829335 0.76067695
 0.77089416 0.75523124 0.76474039 0.74005574 0.76719711 0.76257786
 0.76174763 0.78005024 0.7403876  0.77463933 0.73596992 0.76205245
 0.76122831        nan 0.7674997  0.75879302]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200
Elapsed time to compute best fit: 905.065 seconds
Cross-validation score: 0.7812628317616569
Test score: 0.7221206581352833
Best Hyperparameters: {'classifier__num_leaves': 56, 'classifier__min_data_in_leaf': 200, 'classifier__max_depth': -1, 'classifier__max_bin': 100, 'classifier__learning_rate': 0.3}
162.96444511413574
10540.782673723887
7162.5938025249925
13882.543243227701
2971.706573280967
33.726721579398145
49.48802498287114
2.4321694544923957
520.2000410679684
134.32921996922232
124.05421520984964
75.52892464473553
1290.814959144278
364.30253788143
77.6040327619412
0.0
157.41150365948852
0.0
7711.612631807131
68.26020494528348
1019.5938938914624
13.219444651185768
366.1636675575355
47.917915521829855
16.37469039001735
28.772616833937818
218.40551375672635
812.8476083164569
475.73446515845717
5.0216157586546615
305.3446593476401
0.4403645112470258
5.925599049078301
443.5206131763666
3.1648249715508427
61.29881717808894
1004.5088586202182
266.4344183906651
21.68753591357381
394.8875380592508
24.519017655340576
0.0
0.0
0.0
102.51698589322041
0.0
67.4081896740172
58.439388990649604
0.994706392113585
208.68773540464463
23.333652551926207
0.7529128827154636
42.5831015065196
164.5336272457207
0.0
105.61839417612646
0.0
0.0
0.0
49.023715167917544
81.63736579489058
30.393670317265787
4.264398650062503
1.9109593499451876
18.760583922834485
9.00019139671349
15.60775674879551
59.622848296014126
252.75431819858204
11.234338804366416
29.856344093335792
33.3839547088719
31.388481716567185
28.28927960351575
9.322107659245376
34.68527200014796
138.45250211394523
4.118509529423357
45.82751544349594
520.0760696782088
2.7982253021855286
18.30383027686912
102.34972558438312
375.75981168056023
33.36262669823191
0.7149126223521307
12.615705612348393
154.23646774343797
46.1811242660624
35.33900435719988
86.27272707159864
3.3438855931162834
122.23034039309277
122.27863630407955
91.11722814469249
5.106984876998467
10.154572302039014
4.889233366178814
118.0007027433312
7.903045544633642
7.458563683263492
4.802635735762353
68.58490715539665
0.5430838972824859
24.076780600998973
623.2564688043783
536.2167115153425
44.86573245584077
24.79108846699819
271.41176598111633
134.6939227935177
15.896169098821701
19.946500244404888
79.2499688782409
21.938584890885977
88.92772347974824
622.4251938027248
108.53678343637148
136.35066808181
19.025552875711583
84.2067905812728
0.0
11.566920314260642
95.15163057719474
0.0
94.19178028905299
32.96858317344959
0.0
36.5948997225496
47.693585579574574
0.0
126.20179440669017
160.0146728194959
0.0
18.555607083028008
30.55751917183006
0.0
36.03179328580154
29.30803959138575
0.0
275.0251782392297
19.332628482807195
93.18253406431866
6.957794212823501
12.66527804022553
7.76650204314501
83.36907454850734
0.0
71.6053050593473
14.523937328784086
126.27221113981795
0.12861827705637552
33.59711104625603
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.989394   0.738318  0.718182  0.728111  0.722121  0.734201   

   Average Precision  
0           0.535819  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.74610609 0.74369057 0.72932524 0.74432121 0.74687397 0.72131478
 0.74587115 0.73533998 0.74082359 0.70709406 0.70889925 0.73953191
 0.74844317 0.76353254 0.72257998        nan 0.7435666  0.73996033
 0.73137383 0.72264991 0.73339342 0.74230082 0.77223152 0.75967129
 0.75513174 0.73966454 0.75321179 0.75855173        nan 0.74324484
 0.73210098 0.74285199 0.75616212 0.74860872 0.73606087 0.74183231
 0.75487762        nan 0.73506683 0.75116275 0.75030972 0.73612127
 0.72953732 0.75772155 0.7480425  0.73967716 0.73194605 0.73602113
        nan 0.73832415 0.74255782 0.74451151 0.75679679 0.73260811
 0.73738757 0.73899139 0.73270835 0.72212447 0.73317121 0.74991654
 0.74147517 0.71487052 0.74596292 0.75079923 0.73557059 0.75079119
 0.73558367 0.76925766 0.75114915 0.75112711 0.7422879  0.71800357
 0.7617573  0.75165043 0.7365227  0.7562343         nan 0.76157858
 0.75006524 0.74082377 0.74763855 0.76411221 0.76080078 0.73622541
 0.72034312 0.74838266        nan 0.74488705 0.74098877 0.7528942
 0.73898321 0.76700959        nan 0.75370745 0.75344844 0.73595556
 0.73946178        nan 0.7650089  0.74903953]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 872.540 seconds
Cross-validation score: 0.7722315233235488
Test score: 0.7739130434782608
Best Hyperparameters: {'classifier__num_leaves': 46, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 100, 'classifier__max_bin': 50, 'classifier__learning_rate': 0.1}
534.3319840431213
23337.792853754712
24408.33907363462
29756.95706719905
3991.199960277654
23.331463368900586
218.4750943183899
40.2221457362175
452.6656093597412
81.23849004507065
455.24046446965076
57.417394177173264
5147.409094715724
2434.4520490668947
58.98808014392853
0.0
212.50525146350265
0.0
20002.8417787869
80.48475002621126
12654.076304226706
3.598970018327236
265.82238456606865
145.24173665046692
13.42490005493164
10.721800697036088
219.825603723526
1998.0075796842575
690.9032459688606
53.812899589538574
127.1395697593689
36.10281676054001
5.059866115450859
605.4565598219633
3.65067595243454
145.5760726928711
2458.646164244972
95.39705371767195
287.3012121319771
1138.7660572230816
94.64531302452087
0.0
0.0
0.0
116.43958115577698
0.0
166.91746770590544
6.588760957121849
0.0
879.7344259917736
48.42647036910057
7.011109828948975
446.49242314323783
347.4934711623937
0.0
725.7234658561647
0.0
0.0
0.0
83.62634119391441
346.83992497622967
100.69778574258089
48.600759506225586
215.09660819848068
0.0
232.27612972259521
96.71530771255493
148.31288838386536
500.98942003399134
1.863070011138916
12.337101296172477
151.47331674396992
200.4399555325508
66.08312218356878
18.857446685433388
126.4716248512268
277.27102349465713
0.0
98.07604065537453
815.3730758428574
16.226934731006622
14.424850963056087
32.390762627124786
1099.977297782898
36.38177680969238
8.499949157238007
77.18904995918274
2630.7224967037037
74.50956609845161
21.537220239639282
35.565929532051086
1093.9670991897583
154.86895944178104
2.0603299140930176
604.9636739492416
0.0
2.3039009869098663
200.24576389428694
233.5736480925989
172.6612982749939
164.05431838566437
32.44169447493914
26.085943818092346
0.0
20.33663691363472
557.0890408303821
1138.0638715680689
141.27496945858002
40.95246036350727
429.4273474216461
212.58835578744765
53.20858097076416
288.13084581820294
36.31095981691033
190.2176103591919
76.56661014235578
642.7675161361694
141.32299156473528
1194.4070927236316
13.661660194396973
64.15275025367737
0.0
51.377926141023636
110.202701151371
0.0
336.3349567204714
238.40614652633667
0.0
23.207282166928053
262.4381347000599
0.0
986.0690584514523
199.73416832089424
0.0
51.47139197587967
142.1916402578354
0.0
24.152900099754333
168.19686025381088
0.0
211.31047005951405
36.061888605356216
195.41305449418724
644.0824954509735
0.0
11.565340042114258
61.286613404750824
0.0
100.18651056289673
71.21387034654617
154.3387216398478
0.003029612849786645
304.304710149765
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.987956   0.659259  0.809091  0.726531  0.773913  0.684615   

   Average Precision  
0           0.537176  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73102754 0.73507104 0.75260619 0.73533689        nan 0.75104324
 0.72983255 0.75522612 0.74167686 0.75248931 0.7238301  0.75290832
 0.75119322 0.74611299 0.74134289 0.76003039 0.71149821 0.72312942
 0.7492979  0.74776214 0.74904247 0.71840994 0.75890688 0.73463401
 0.75741949 0.75998386 0.74866576 0.71502433 0.73296749 0.73780961
 0.72905677 0.75285843 0.72780085 0.74773994 0.75592494 0.73407602
 0.74728371 0.74496361        nan 0.75650359        nan 0.74602235
 0.7541806         nan 0.7325344  0.74279803        nan 0.72475925
 0.75017765 0.74135331 0.73431481 0.74377618 0.73974266 0.73564126
 0.73595938 0.75067057 0.75436879 0.75480936 0.72313764 0.72700736
 0.75126039 0.7490294  0.74502669 0.72886623 0.76508454 0.74158726
 0.74374481 0.75063256 0.71796648 0.75284118 0.75863551 0.74450321
 0.726667   0.75287004 0.73271502 0.74205455 0.7446479  0.74158818
 0.7243647  0.74990203 0.75128013 0.77167837 0.7306291  0.74606954
 0.74200274 0.73154521 0.73934204 0.74113225 0.74477771 0.73228317
 0.73963862 0.75025853 0.7368688  0.75481012 0.72302769        nan
 0.73791841 0.74787185 0.72846749 0.74326852]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300
Elapsed time to compute best fit: 978.793 seconds
Cross-validation score: 0.7716783651934767
Test score: 0.7685352622061483
Best Hyperparameters: {'classifier__num_leaves': 51, 'classifier__min_data_in_leaf': 300, 'classifier__max_depth': 900, 'classifier__max_bin': 150, 'classifier__learning_rate': 0.7}
44.81870571184254
5014.790834666281
5161.156623364239
8918.14819537025
783.119802035516
21.517961912650208
3.4451529067009687
7.390113345537295
205.4456715383858
4.98583595733362
14.935732104349881
22.344279461045517
2017.6477696850816
241.94162259714176
2.9019745793775655
0.0
27.557030340314668
0.0
1836.3574792986335
32.61439404964572
568.6750815631112
18.101117892896582
43.77879728757398
4.210015357684212
44.92854250573873
75.41076135216281
24.675557479189592
341.09829105504133
375.9015862948727
1.0245671230432583
1.0955468740652445
2.029954448022181
5.072701270067959
81.27610775803805
35.25985952312476
7.808601544793021
188.26948319853545
3.6402414658177804
33.476733933794094
248.51016789173627
13.910182385327062
0.0
0.0
0.0
4.067678117629839
0.0
9.22932407380199
93.36419540022871
0.0
134.77656324406615
53.65257133215346
0.0
31.363465349943
33.36074848158091
0.0
327.8075416802458
0.0
0.0
0.0
8.556662760696781
239.5528072679183
18.851313091626707
30.12789058929775
0.11106603349617217
4.203175536356866
3.3643821024415956
5.175180262158392
65.00825371382598
37.07544034129023
4.374812481692637
32.634929418850895
5.932863580477715
35.84795516826853
13.162208794381513
29.062088048201986
13.765829279727768
31.73425327660516
12.186281246074941
59.50475113866116
131.88761328262626
10.325318148598765
3.453989969431859
13.515637831826098
195.81712068043998
24.41394154041489
0.12920442613540217
28.88063840022383
140.2475974341305
1.578518516740587
1.3892837187158875
12.881143513364805
0.0
43.595023687783396
3.2136847608198877
142.23496794016864
0.07690899819135666
1.2952593571462785
23.154939604173705
46.77832625402995
0.0006957181867619511
21.634858821894454
25.56921676240745
0.21484200614941074
0.013870489550754428
14.721436835619897
136.04747669108474
187.59248624110126
169.46889593657679
2.9887049389071763
117.09228666643321
90.61901170255442
14.981801980917226
12.321930456015252
26.751162613283668
3.9920322751277126
3.6873202859424055
757.5446512353675
0.29894291586242616
16.90401698090136
6.559508095851925
19.303204721920718
0.0
13.608631047209201
28.351149938342132
0.0
80.26843939490037
1.7293646388738524
0.0
2.397773659824452
11.269321075102198
0.0
74.50153658131603
31.948443961425596
0.0
50.67066673413137
22.075430673721712
0.0
426.2620012490079
17.018376414918748
0.0
25.049174483715433
5.94076253586536
35.37038443313941
33.33735675088246
32.20521248462683
16.414169047086034
189.6902140476741
0.0
17.273369783396117
27.773431815832737
130.97149744191628
4.262183444341645
0.3993866240725765
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.990473   0.752212  0.772727  0.762332  0.768535  0.756228   

   Average Precision  
0           0.585749  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75101603 0.77077132 0.76779264        nan 0.76750497 0.758647
 0.74991739 0.7492754  0.76499193 0.76484034 0.76104246 0.74319229
 0.74963714 0.75163337 0.72724916 0.75199703        nan 0.758175
 0.7575891  0.75794711 0.75991876 0.74921119 0.75999557 0.75205657
 0.74512235 0.75311767 0.74920506        nan 0.75420927 0.74176971
 0.76336029 0.75546599        nan        nan 0.74534841 0.74617267
 0.74769676 0.74396791        nan 0.72713931 0.7568923  0.76530911
 0.742299   0.76397808 0.75758188        nan 0.74286188 0.74542344
 0.74350842 0.76024514 0.75507123 0.7413921  0.7412407  0.75701223
 0.76791564 0.75095861 0.75922732 0.76288936 0.75615674 0.76339307
 0.75213083 0.75448613 0.74923404 0.74767156 0.76333567 0.73875311
 0.77266581        nan 0.74118427 0.75145593 0.74985207 0.73962636
 0.75730146        nan 0.77098718 0.73999089        nan 0.76545988
 0.7633589  0.75853073 0.76022754 0.75821987 0.75577898 0.73129598
 0.73630052 0.73156142 0.74683746 0.7652798         nan 0.74904619
 0.75509589        nan 0.74512598        nan 0.76799001        nan
 0.75076831 0.75817368 0.7540902  0.73726105]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=800
Elapsed time to compute best fit: 878.747 seconds
Cross-validation score: 0.7726658130034163
Test score: 0.7820738137082601
Best Hyperparameters: {'classifier__num_leaves': 31, 'classifier__min_data_in_leaf': 800, 'classifier__max_depth': 300, 'classifier__max_bin': 450, 'classifier__learning_rate': 0.5}
99.21370006378726
8598.302451690666
4979.48184949592
9304.621138786431
824.4615184426366
14.772912584529026
117.0483863465488
0.6563238531380193
138.932950027287
14.269986575469375
32.38454471661498
7.8154350109398365
1994.8033443388995
343.73663495046276
54.52133001666516
0.0
285.4784834894183
0.0
2798.628490309562
33.156799004156326
1349.925377108535
37.095785724930465
19.277890728786588
6.763407564954832
2.6823850007331203
9.509763296227902
10.737453968657064
457.3898952871541
142.42755010630935
9.503255712799728
0.2966718152165413
0.36329851392656565
7.576026672497392
110.76894188777078
12.688162432983518
0.5908361107576638
387.5140306081157
3.3653541214298457
18.421617625320444
500.2405519395252
35.64018878537834
0.0
0.0
0.0
19.027746435254812
0.0
24.68626072726113
26.168687891917216
0.0
149.6539537962526
5.312714271247387
0.0
72.55277446750551
6.539601271040738
0.0
108.89334568235506
0.0
0.0
0.0
75.55920639587566
27.76671968214214
11.044013615231961
0.0008177920244634151
7.161819696426392
2.298716539517045
0.3455985754044377
0.6681060194969177
76.12798056285828
219.8224854301828
4.832059979438782
4.101143308915198
26.687473085476086
19.16461160965264
17.700044394194265
31.32973132887855
1.372791975736618
60.78624734902405
2.7463276125490665
2.4608260920258545
562.9121280359104
12.734292071312666
13.550306317396462
5.951881864345205
373.77081878995523
8.733572488920943
13.819869256578386
4.953723073005676
555.8418129831553
1.1752632246352732
30.49681975357987
74.79330492205918
2.3717913702130318
95.74249876756221
0.23189427610486746
93.76220497954637
5.156589984893799
0.9957943148910999
0.502032885706285
33.21007095184177
0.18148519843816757
3.7630464546382427
45.657285056076944
24.796818130649626
0.01351420022547245
18.943410111649428
137.6039104484953
440.4090025611222
56.10586657654494
8.81407231837511
114.64946193760261
228.68027257733047
76.25115323625505
46.78800176085588
20.912344864103943
54.24242277862504
9.795340587152168
963.3692723829299
32.759909069747664
195.83965682564303
20.1316543912437
24.17498565511778
0.0
14.099347329311392
88.91833357505936
0.0
110.25776925776154
51.43523054634179
1.8616998533252627
10.46061384677887
40.86170635937823
0.0
221.75125986618514
32.02669863216579
0.0
10.53995364671573
122.83269455004483
0.0
240.74883703840896
21.608098771114783
0.0
28.235773034160957
39.82527968287468
57.52850378098083
11.05112651888976
11.099546529352665
25.753946665455487
3.6788601614534855
0.0
5.169134401716292
2.1037312038242817
40.152739852666855
0.0
30.64168160222315
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall       F1        F2      F0.5  \
0  0.989035   0.689922  0.809091  0.74477  0.782074  0.710863   

   Average Precision  
0           0.561985  

--------------------------------------------------------------------

C:\Python39\lib\site-packages\sklearn\model_selection\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.75756186 0.74984843 0.7575389  0.76644952 0.75208218 0.77338573
 0.76519209 0.76268199 0.76561948 0.76765737        nan 0.76584255
 0.75939827 0.76223977 0.77003277 0.76470944 0.75333568 0.76197934
 0.75014401        nan 0.77639008 0.75042843 0.74948864 0.75949861
 0.74373491 0.74835777 0.74834221 0.76389288 0.73911105 0.74011844
 0.75913125        nan 0.7645013  0.76704848 0.76482047 0.77536014
 0.75093961 0.76621034 0.78030596 0.74257072 0.76911975 0.75141727
 0.76542831 0.77259753 0.74212091 0.75617638 0.77479799 0.75792039
 0.75986167 0.75266908 0.76121221 0.75299125 0.75003794        nan
 0.76906126        nan 0.75691815 0.76406161 0.75581594 0.75193593
 0.73645491 0.77564523 0.76422924 0.75935478 0.7447673  0.76330049
 0.7661556  0.74910413 0.7432342  0.75954382 0.77583445 0.75861924
 0.76450701 0.76719455        nan 0.77392474 0.7540698  0.77348778
 0.76100762 0.74120124 0.75624188 0.74114509 0.75991291 0.75683308
 0.75664764 0.74520943 0.74027667 0.77694877        nan 0.76479891
 0.77261358 0.73344841 0.75440366 0.74490376 0.74862055 0.76061666
 0.75534335 0.76788132 0.75610579 0.76301334]
  warnings.warn(

[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400
Elapsed time to compute best fit: 1075.996 seconds
Cross-validation score: 0.7803059587732422
Test score: 0.7285974499089253
Best Hyperparameters: {'classifier__num_leaves': 36, 'classifier__min_data_in_leaf': 400, 'classifier__max_depth': 200, 'classifier__max_bin': 255, 'classifier__learning_rate': 0.4}
190.41596286802087
6963.7470687527675
8586.5989392458
10746.505219795625
3416.02747660951
57.30233304394642
6.274009449465666
12.623805646551773
106.27017224291922
15.555212188162841
169.8066391829052
27.131027572439052
1616.5975226374285
813.2386455859232
15.133764481724938
0.0
186.61579812680964
0.0
2463.2915879023494
128.9691487060336
553.6049354015558
5.336721514584497
11.798216977069387
15.494573932897765
77.43684025484254
21.523845654621255
18.818942208694352
1265.0903323665261
13.947829740820453
48.14905363321304
122.11779621220194
6.789760312414728
1.905498721433105
321.56517373828683
19.941429154132493
54.15586762392195
326.5910553471185
34.04269359796308
30.33252644794993
370.4669024263858
41.15870091022225
0.0
0.0
0.0
8.879532729566563
0.0
169.98758118005935
102.00664126337506
0.0
125.98635870683938
20.662543860496953
0.0
23.052836314134765
39.13593385339482
0.0
159.160751460935
0.0
0.0
0.0
43.942859428498195
209.47835871984717
28.002430936699966
4.160502908285707
2.2372899055480957
18.19434854228166
1.7513632136397064
13.372784408391453
80.36596200778149
118.84493993478827
9.626303491648287
5.450103317038156
1.1342486918729264
36.97538849443663
16.96849438024219
274.3866240090574
0.25485004810616374
77.9643158269464
5.576684868894517
46.54563824046636
178.49026011954993
7.290332532138564
11.178410205058753
74.63823139370652
329.0899342666671
23.255366906581912
0.05260233546141535
24.102848934475332
143.38790306448936
16.87950927298516
51.670271536830114
15.397169198840857
0.8346067979000509
95.76941168832127
43.10301146857091
19.967439734202344
5.498967042192817
3.8536639824451413
17.929625268356176
156.531304275617
5.520162508357316
33.10379828087753
4.585040183970705
22.202866303981864
0.006865799892693758
11.682948348694481
362.4343875446066
339.5822935505712
63.65571606508456
0.900917663355358
34.19940778822638
170.29991294140927
20.298514923080802
42.88613510574214
38.14139798149699
17.89119480550289
30.995661409629975
1084.9258960010047
44.51348084965139
424.40454724495066
1.418474383535795
83.25433348392266
0.0
14.242928344724987
78.33334607223514
0.0
63.00257962694741
219.52851501395344
1.1293257400393486
0.542893313919194
29.14611446391791
0.0
62.1273129832698
46.32482215937671
0.0
3.8212324313353747
139.79817009779254
0.0
663.6053450885229
42.30145086237462
0.0
26.85105794016272
77.98566668271087
38.76860233588377
145.10914971840248
15.759583511506207
12.334253523928055
6.466489446349442
0.0
185.21280137333088
0.9262977440957911
53.66079401060415
0.38561028987169266
52.01887622859795
0.0
Legend
Recall -  measures the fraction of relevant links that are retrieved
Precision - measures the fraction of retrieved links that are relevant
F-measure - measures the harmonic mean of recall and precision
F2-measure - favors recall
F0.5-measure - favors precision
Average Precision - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold

   Accuracy  Precision    Recall        F1        F2      F0.5  \
0  0.989394   0.733945  0.727273  0.730594  0.728597  0.732601   

   Average Precision  
0           0.539171  

--------------------------------------------------------------------
