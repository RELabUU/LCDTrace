{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Import Self-written Functions\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from d00_utils.calculateTimeDifference import calculateTimeDifference #Function to calc time difference\n",
    "from d01_data.loadCommits import loadCommits #Function to load SVN data\n",
    "from d02_intermediate.cleanCommitData import cleanCommitData #Function to clean commit data\n",
    "from d02_intermediate.cleanJiraData import cleanJiraData #Function to clean JIRA data\n",
    "\n",
    "from d03_processing.createFittedTF_IDF import createFittedTF_IDF #Function to see if a trace is valid\n",
    "from d03_processing.createCorpusFromDocumentList import createCorpusFromDocumentList #Function to create a corpus\n",
    "from d03_processing.checkValidityTrace import checkValidityTrace #Function to see if a trace is valid\n",
    "from d03_processing.calculateTimeDif import calculateTimeDif #Calculate the time difference between 2 dates in seconds\n",
    "from d03_processing.checkFullnameEqualsEmail import checkFullnameEqualsEmail #Check if fullName is equal to the email\n",
    "from d03_processing.calculateCosineSimilarity import calculateCosineSimilarity #Calculate the cos similarity\n",
    "from d03_processing.calculateDocumentStatistics import *\n",
    "\n",
    "from d03_processing.calculateQueryQuality import *\n",
    "from d03_processing.normalize_data import *\n",
    "\n",
    "from d04_model_evaluation.model_evaluation import *\n",
    "\n",
    "#Display full value of a column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-sellers",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw JIRA data as a pandas dataframe\n",
    "jira_df_raw = pd.read_excel('../data/01_raw/jira_example.xlsx')\n",
    "\n",
    "#Import raw svn data as a pandas dataframe\n",
    "svn_df_raw = loadCommits('../data/01_raw/svn_example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-utility",
   "metadata": {},
   "source": [
    "# 2. Clean Raw Data\n",
    "## 2.1 Clean Raw Data - SVN Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning after 0 minutes and 1.3027262687683105 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "svn_df_clean = cleanCommitData(svn_df_raw)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "svn_df_clean.to_excel(excel_writer = \"../data/02_intermediate/svn_df_clean.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "svn_df_clean.to_pickle(path= \"../data/02_intermediate/svn_df_clean.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished cleaning after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-campbell",
   "metadata": {},
   "source": [
    "## 2.2 Clean Raw Data - JIRA Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loaded-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3) Finished Cleaning Summaries after 0.020943403244018555 sec\n",
      "2/3) Finished Cleaning Description after 0.11174774169921875 sec\n",
      "3/3) Finished Cleaning Dates after 0.11373233795166016 sec\n"
     ]
    }
   ],
   "source": [
    "#Rename key to Issue key\n",
    "jira_df_raw = jira_df_raw.rename({'Key': 'Issue key'}, axis=1)\n",
    "\n",
    "#Clean Data sets\n",
    "jira_df_clean = cleanJiraData(dataFrame = jira_df_raw, cleanComments = False, commentAmount = 39)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "jira_df_clean.to_excel(excel_writer = \"../data/02_intermediate/jira_df_clean.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "jira_df_clean.to_pickle(path= \"../data/02_intermediate/jira_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-bankruptcy",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create Corpora\n",
    "Create the corpora for JIRA Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confused-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "jira_corpus_summary  = createCorpusFromDocumentList(jira_df_clean.Summary)\n",
    "jira_corpus_description = createCorpusFromDocumentList(jira_df_clean.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "jira_corpus_all = [i+\" \"+j for i,j in zip(jira_corpus_summary,\n",
    "                                          jira_corpus_description)]\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/jira_corpus_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_summary, f)\n",
    "\n",
    "with open('../data/02_intermediate/jira_corpus_description.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_description, f)\n",
    "\n",
    "with open('../data/02_intermediate/jira_corpus_all.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-syntax",
   "metadata": {},
   "source": [
    "Create the corpora for SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "modern-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corpus for log messages\n",
    "svn_corpus_log = createCorpusFromDocumentList(svn_df_clean.Logs)\n",
    "\n",
    "#Create corpus for unit names\n",
    "svn_corpus_unitname = createCorpusFromDocumentList(svn_df_clean.Unit_names)\n",
    "\n",
    "#Create corpus for entire commit (log message + model)\n",
    "svn_corpus_all = createCorpusFromDocumentList(svn_df_clean.Logs + svn_df_clean.Unit_names)\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/svn_corpus_log.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_log, f)\n",
    "\n",
    "with open('../data/02_intermediate/svn_corpus_unitname.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_unitname, f)\n",
    "\n",
    "with open('../data/02_intermediate/svn_corpus_all.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-collaboration",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raising-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained results.\n",
    "#Load JIRA Corpora\n",
    "jira_corpus_summary = pd.read_pickle(\"../data/02_intermediate/jira_corpus_summary.pkl\")\n",
    "jira_corpus_description = pd.read_pickle(\"../data/02_intermediate/jira_corpus_description.pkl\")\n",
    "jira_corpus_all = pd.read_pickle(\"../data/02_intermediate/jira_corpus_all.pkl\")\n",
    "\n",
    "#Load SVN corora\n",
    "svn_corpus_log = pd.read_pickle(\"../data/02_intermediate/svn_corpus_log.pkl\")\n",
    "svn_corpus_unitname = pd.read_pickle(\"../data/02_intermediate/svn_corpus_unitname.pkl\")\n",
    "svn_corpus_all = pd.read_pickle(\"../data/02_intermediate/svn_corpus_all.pkl\")\n",
    "\n",
    "#Load clean datasets\n",
    "jira_df_clean = pd.read_pickle(\"../data/02_intermediate/jira_df_clean.pkl\")\n",
    "svn_df_clean = pd.read_pickle(\"../data/02_intermediate/svn_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-mouse",
   "metadata": {},
   "source": [
    "## 3.0 Preprocess Data - Create cartesian product JIRA x Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "marked-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_df = jira_df_clean.merge(svn_df_clean, how='cross')\n",
    "\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_df = cartesian_df.drop(cartesian_df[cartesian_df.Jira_created_date > cartesian_df.Commit_date].index)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "cartesian_df.to_pickle(path= \"../data/03_processed/cartesian_df.pkl\")\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "cartesian_df.to_excel(excel_writer = \"../data/02_intermediate/cartesian_df.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modified-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_key_jira</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Jira_created_date</th>\n",
       "      <th>Jira_updated_date</th>\n",
       "      <th>Jira_resolved_date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary_2grams</th>\n",
       "      <th>Summary_3grams</th>\n",
       "      <th>Description</th>\n",
       "      <th>Description_2grams</th>\n",
       "      <th>Description_3grams</th>\n",
       "      <th>Jira_natural_text</th>\n",
       "      <th>Jira_natural_text_2grams</th>\n",
       "      <th>Jira_natural_text_3grams</th>\n",
       "      <th>verbs</th>\n",
       "      <th>Revision</th>\n",
       "      <th>Email</th>\n",
       "      <th>Commit_date</th>\n",
       "      <th>Issue_key_commit</th>\n",
       "      <th>Logs</th>\n",
       "      <th>Logs_2grams</th>\n",
       "      <th>Logs_3grams</th>\n",
       "      <th>Unit_names</th>\n",
       "      <th>Unit_names_2grams</th>\n",
       "      <th>Unit_names_3grams</th>\n",
       "      <th>Commit_natural_text</th>\n",
       "      <th>Commit_natural_text_2grams</th>\n",
       "      <th>Commit_natural_text_3grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>AFM-3224</td>\n",
       "      <td>Alan Lawrence</td>\n",
       "      <td>2021-02-04 10:19:00</td>\n",
       "      <td>2021-02-15 17:46:00</td>\n",
       "      <td>2021-02-11 15:54:00</td>\n",
       "      <td>[cant, close, sidebar, situat]</td>\n",
       "      <td>[[cant, close], [close, sidebar]]</td>\n",
       "      <td>[[cant, close, sidebar]]</td>\n",
       "      <td>[left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>637</td>\n",
       "      <td>fabian.davison@mendix.com</td>\n",
       "      <td>2021-02-04 13:31:38.352422</td>\n",
       "      <td>[AFM-2518]</td>\n",
       "      <td>[chang, titl, properti, page, titl, report, processingactivitiesreportoverview, page]</td>\n",
       "      <td>[[page, titl], [processingactivitiesreportoverview, page]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[processingact, report, overview]</td>\n",
       "      <td>[[processingact, report], [report, overview]]</td>\n",
       "      <td>[[processingact, report, overview]]</td>\n",
       "      <td>[chang, titl, properti, page, titl, report, processingactivitiesreportoverview, page, processingact, report, overview]</td>\n",
       "      <td>[[page, titl], [processingactivitiesreportoverview, page], [processingact, report], [report, overview]]</td>\n",
       "      <td>[[processingact, report, overview]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>AFM-3224</td>\n",
       "      <td>Alan Lawrence</td>\n",
       "      <td>2021-02-04 10:19:00</td>\n",
       "      <td>2021-02-15 17:46:00</td>\n",
       "      <td>2021-02-11 15:54:00</td>\n",
       "      <td>[cant, close, sidebar, situat]</td>\n",
       "      <td>[[cant, close], [close, sidebar]]</td>\n",
       "      <td>[[cant, close, sidebar]]</td>\n",
       "      <td>[left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>639</td>\n",
       "      <td>daisy.rogers@mendix.com</td>\n",
       "      <td>2021-02-04 13:36:47.388304</td>\n",
       "      <td>[AFM-3159]</td>\n",
       "      <td>[creat, branch, line, updat, notif, dropdown, revis, 610, main, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, updat], [updat, notif], [notif, dropdown], [revis, 610], [main, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, updat], [line, updat, notif], [updat, notif, dropdown]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[creat, branch, line, updat, notif, dropdown, revis, 610, main, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, updat], [updat, notif], [notif, dropdown], [revis, 610], [main, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, updat], [line, updat, notif], [updat, notif, dropdown]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>AFM-3224</td>\n",
       "      <td>Alan Lawrence</td>\n",
       "      <td>2021-02-04 10:19:00</td>\n",
       "      <td>2021-02-15 17:46:00</td>\n",
       "      <td>2021-02-11 15:54:00</td>\n",
       "      <td>[cant, close, sidebar, situat]</td>\n",
       "      <td>[[cant, close], [close, sidebar]]</td>\n",
       "      <td>[[cant, close, sidebar]]</td>\n",
       "      <td>[left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>640</td>\n",
       "      <td>edwin.harrison@mendix.com</td>\n",
       "      <td>2021-02-04 13:46:01.485825</td>\n",
       "      <td>[AFM-3112]</td>\n",
       "      <td>[creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>AFM-3224</td>\n",
       "      <td>Alan Lawrence</td>\n",
       "      <td>2021-02-04 10:19:00</td>\n",
       "      <td>2021-02-15 17:46:00</td>\n",
       "      <td>2021-02-11 15:54:00</td>\n",
       "      <td>[cant, close, sidebar, situat]</td>\n",
       "      <td>[[cant, close], [close, sidebar]]</td>\n",
       "      <td>[[cant, close, sidebar]]</td>\n",
       "      <td>[left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>641</td>\n",
       "      <td>edwin.harrison@mendix.com</td>\n",
       "      <td>2021-02-04 13:46:20.919449</td>\n",
       "      <td>[AFM-3112]</td>\n",
       "      <td>[delet, branch, recent, updat]</td>\n",
       "      <td>[[delet, branch], [branch, recent], [recent, updat]]</td>\n",
       "      <td>[[delet, branch, recent], [branch, recent, updat]]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>AFM-3224</td>\n",
       "      <td>Alan Lawrence</td>\n",
       "      <td>2021-02-04 10:19:00</td>\n",
       "      <td>2021-02-15 17:46:00</td>\n",
       "      <td>2021-02-11 15:54:00</td>\n",
       "      <td>[cant, close, sidebar, situat]</td>\n",
       "      <td>[[cant, close], [close, sidebar]]</td>\n",
       "      <td>[[cant, close, sidebar]]</td>\n",
       "      <td>[left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]</td>\n",
       "      <td>[[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>642</td>\n",
       "      <td>edwin.harrison@mendix.com</td>\n",
       "      <td>2021-02-04 13:52:41.145653</td>\n",
       "      <td>[AFM-3112]</td>\n",
       "      <td>[creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]</td>\n",
       "      <td>[[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]</td>\n",
       "      <td>[[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Issue_key_jira       Assignee   Jira_created_date   Jira_updated_date  \\\n",
       "166       AFM-3224  Alan Lawrence 2021-02-04 10:19:00 2021-02-15 17:46:00   \n",
       "167       AFM-3224  Alan Lawrence 2021-02-04 10:19:00 2021-02-15 17:46:00   \n",
       "168       AFM-3224  Alan Lawrence 2021-02-04 10:19:00 2021-02-15 17:46:00   \n",
       "169       AFM-3224  Alan Lawrence 2021-02-04 10:19:00 2021-02-15 17:46:00   \n",
       "170       AFM-3224  Alan Lawrence 2021-02-04 10:19:00 2021-02-15 17:46:00   \n",
       "\n",
       "     Jira_resolved_date                         Summary  \\\n",
       "166 2021-02-11 15:54:00  [cant, close, sidebar, situat]   \n",
       "167 2021-02-11 15:54:00  [cant, close, sidebar, situat]   \n",
       "168 2021-02-11 15:54:00  [cant, close, sidebar, situat]   \n",
       "169 2021-02-11 15:54:00  [cant, close, sidebar, situat]   \n",
       "170 2021-02-11 15:54:00  [cant, close, sidebar, situat]   \n",
       "\n",
       "                        Summary_2grams            Summary_3grams  \\\n",
       "166  [[cant, close], [close, sidebar]]  [[cant, close, sidebar]]   \n",
       "167  [[cant, close], [close, sidebar]]  [[cant, close, sidebar]]   \n",
       "168  [[cant, close], [close, sidebar]]  [[cant, close, sidebar]]   \n",
       "169  [[cant, close], [close, sidebar]]  [[cant, close, sidebar]]   \n",
       "170  [[cant, close], [close, sidebar]]  [[cant, close, sidebar]]   \n",
       "\n",
       "                                                                                                    Description  \\\n",
       "166  [left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "167  [left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "168  [left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "169  [left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "170  [left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "\n",
       "                                                                                                                      Description_2grams  \\\n",
       "166  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "167  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "168  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "169  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "170  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "\n",
       "                                                                                                                      Description_3grams  \\\n",
       "166  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "167  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "168  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "169  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "170  [[left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "\n",
       "                                                                                                                            Jira_natural_text  \\\n",
       "166  [cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "167  [cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "168  [cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "169  [cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "170  [cant, close, sidebar, situat, left, navig, panel, open, smaller, screen, size, bug, current, use, version, side, panel, design, system]   \n",
       "\n",
       "                                                                                                                                                 Jira_natural_text_2grams  \\\n",
       "166  [[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "167  [[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "168  [[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "169  [[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "170  [[cant, close], [close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "\n",
       "                                                                                                                                        Jira_natural_text_3grams  \\\n",
       "166  [[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "167  [[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "168  [[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "169  [[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "170  [[cant, close, sidebar], [left, navig], [navig, panel], [smaller, screen], [screen, size], [current, use], [use, version], [side, panel], [design, system]]   \n",
       "\n",
       "    verbs Revision                      Email                Commit_date  \\\n",
       "166    []      637  fabian.davison@mendix.com 2021-02-04 13:31:38.352422   \n",
       "167    []      639    daisy.rogers@mendix.com 2021-02-04 13:36:47.388304   \n",
       "168    []      640  edwin.harrison@mendix.com 2021-02-04 13:46:01.485825   \n",
       "169    []      641  edwin.harrison@mendix.com 2021-02-04 13:46:20.919449   \n",
       "170    []      642  edwin.harrison@mendix.com 2021-02-04 13:52:41.145653   \n",
       "\n",
       "    Issue_key_commit  \\\n",
       "166       [AFM-2518]   \n",
       "167       [AFM-3159]   \n",
       "168       [AFM-3112]   \n",
       "169       [AFM-3112]   \n",
       "170       [AFM-3112]   \n",
       "\n",
       "                                                                                      Logs  \\\n",
       "166  [chang, titl, properti, page, titl, report, processingactivitiesreportoverview, page]   \n",
       "167                  [creat, branch, line, updat, notif, dropdown, revis, 610, main, line]   \n",
       "168          [creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]   \n",
       "169                                                         [delet, branch, recent, updat]   \n",
       "170          [creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]   \n",
       "\n",
       "                                                                                                                            Logs_2grams  \\\n",
       "166                                                                          [[page, titl], [processingactivitiesreportoverview, page]]   \n",
       "167                     [[creat, branch], [branch, line], [line, updat], [updat, notif], [notif, dropdown], [revis, 610], [main, line]]   \n",
       "168  [[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]   \n",
       "169                                                                                [[delet, branch], [branch, recent], [recent, updat]]   \n",
       "170  [[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]   \n",
       "\n",
       "                                                                                                                Logs_3grams  \\\n",
       "166                                                                                                                      []   \n",
       "167                          [[creat, branch, line], [branch, line, updat], [line, updat, notif], [updat, notif, dropdown]]   \n",
       "168  [[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]   \n",
       "169                                                                      [[delet, branch, recent], [branch, recent, updat]]   \n",
       "170  [[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]   \n",
       "\n",
       "                            Unit_names  \\\n",
       "166  [processingact, report, overview]   \n",
       "167                                 []   \n",
       "168                                 []   \n",
       "169                               None   \n",
       "170                                 []   \n",
       "\n",
       "                                 Unit_names_2grams  \\\n",
       "166  [[processingact, report], [report, overview]]   \n",
       "167                                             []   \n",
       "168                                             []   \n",
       "169                                           None   \n",
       "170                                             []   \n",
       "\n",
       "                       Unit_names_3grams  \\\n",
       "166  [[processingact, report, overview]]   \n",
       "167                                   []   \n",
       "168                                   []   \n",
       "169                                 None   \n",
       "170                                   []   \n",
       "\n",
       "                                                                                                        Commit_natural_text  \\\n",
       "166  [chang, titl, properti, page, titl, report, processingactivitiesreportoverview, page, processingact, report, overview]   \n",
       "167                                                   [creat, branch, line, updat, notif, dropdown, revis, 610, main, line]   \n",
       "168                                           [creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]   \n",
       "169                                                                                                                     NaN   \n",
       "170                                           [creat, branch, line, recent, updat, revis, 636, branch, line, develop, line]   \n",
       "\n",
       "                                                                                                             Commit_natural_text_2grams  \\\n",
       "166                             [[page, titl], [processingactivitiesreportoverview, page], [processingact, report], [report, overview]]   \n",
       "167                     [[creat, branch], [branch, line], [line, updat], [updat, notif], [notif, dropdown], [revis, 610], [main, line]]   \n",
       "168  [[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]   \n",
       "169                                                                                                                                 NaN   \n",
       "170  [[creat, branch], [branch, line], [line, recent], [recent, updat], [revis, 636], [branch, line], [line, develop], [develop, line]]   \n",
       "\n",
       "                                                                                                 Commit_natural_text_3grams  \n",
       "166                                                                                     [[processingact, report, overview]]  \n",
       "167                          [[creat, branch, line], [branch, line, updat], [line, updat, notif], [updat, notif, dropdown]]  \n",
       "168  [[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]  \n",
       "169                                                                                                                     NaN  \n",
       "170  [[creat, branch, line], [branch, line, recent], [line, recent, updat], [branch, line, develop], [line, develop, line]]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesian_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-texas",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess Data - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "romance-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating labels for dataProcessing\n"
     ]
    }
   ],
   "source": [
    "#Create new dataFrames for the time features\n",
    "labels_df = pd.DataFrame() \n",
    "\n",
    "#Create a column, which indicates which traces are valid.\n",
    "labels_df[\"is_valid\"] = cartesian_df.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for dataProcessing\")\n",
    "\n",
    "#Save intermediate results\n",
    "labels_df.to_pickle(path= \"../data/03_processed/labels_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caring-choir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4313</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_valid\n",
       "192       True\n",
       "194       True\n",
       "205       True\n",
       "211       True\n",
       "213       True\n",
       "216       True\n",
       "219       True\n",
       "221       True\n",
       "222       True\n",
       "223       True\n",
       "224       True\n",
       "225       True\n",
       "226       True\n",
       "227       True\n",
       "228       True\n",
       "410       True\n",
       "427       True\n",
       "432       True\n",
       "466       True\n",
       "643       True\n",
       "646       True\n",
       "647       True\n",
       "648       True\n",
       "649       True\n",
       "650       True\n",
       "652       True\n",
       "656       True\n",
       "657       True\n",
       "664       True\n",
       "701       True\n",
       "875       True\n",
       "876       True\n",
       "885       True\n",
       "889       True\n",
       "1135      True\n",
       "1140      True\n",
       "1142      True\n",
       "1143      True\n",
       "1144      True\n",
       "1148      True\n",
       "1150      True\n",
       "1156      True\n",
       "1166      True\n",
       "1337      True\n",
       "1347      True\n",
       "1355      True\n",
       "1356      True\n",
       "1361      True\n",
       "1367      True\n",
       "1370      True\n",
       "1372      True\n",
       "1379      True\n",
       "1380      True\n",
       "1401      True\n",
       "1572      True\n",
       "1573      True\n",
       "1574      True\n",
       "1594      True\n",
       "1599      True\n",
       "1605      True\n",
       "1607      True\n",
       "1619      True\n",
       "1621      True\n",
       "1622      True\n",
       "1633      True\n",
       "1788      True\n",
       "1789      True\n",
       "1790      True\n",
       "1791      True\n",
       "1792      True\n",
       "1793      True\n",
       "1797      True\n",
       "1872      True\n",
       "1873      True\n",
       "1875      True\n",
       "1876      True\n",
       "1877      True\n",
       "1878      True\n",
       "1879      True\n",
       "1880      True\n",
       "1881      True\n",
       "1882      True\n",
       "1884      True\n",
       "1892      True\n",
       "1893      True\n",
       "1894      True\n",
       "1897      True\n",
       "1898      True\n",
       "2153      True\n",
       "2158      True\n",
       "2160      True\n",
       "2161      True\n",
       "2162      True\n",
       "2163      True\n",
       "2172      True\n",
       "2174      True\n",
       "2175      True\n",
       "2179      True\n",
       "2185      True\n",
       "2380      True\n",
       "2382      True\n",
       "2385      True\n",
       "2389      True\n",
       "2391      True\n",
       "2398      True\n",
       "2399      True\n",
       "2401      True\n",
       "2402      True\n",
       "2405      True\n",
       "2407      True\n",
       "2410      True\n",
       "2411      True\n",
       "2412      True\n",
       "2414      True\n",
       "2417      True\n",
       "2418      True\n",
       "2420      True\n",
       "2421      True\n",
       "2422      True\n",
       "2423      True\n",
       "2424      True\n",
       "2425      True\n",
       "2426      True\n",
       "2427      True\n",
       "2428      True\n",
       "2444      True\n",
       "2449      True\n",
       "2456      True\n",
       "2496      True\n",
       "2666      True\n",
       "2667      True\n",
       "2668      True\n",
       "2669      True\n",
       "2670      True\n",
       "2671      True\n",
       "2672      True\n",
       "2682      True\n",
       "2849      True\n",
       "2851      True\n",
       "2852      True\n",
       "2854      True\n",
       "2856      True\n",
       "2858      True\n",
       "2861      True\n",
       "2868      True\n",
       "2871      True\n",
       "2872      True\n",
       "2883      True\n",
       "2884      True\n",
       "2918      True\n",
       "3153      True\n",
       "3155      True\n",
       "3156      True\n",
       "3157      True\n",
       "3159      True\n",
       "3160      True\n",
       "3164      True\n",
       "3165      True\n",
       "3169      True\n",
       "3170      True\n",
       "3171      True\n",
       "3179      True\n",
       "3180      True\n",
       "3181      True\n",
       "3184      True\n",
       "3199      True\n",
       "3388      True\n",
       "3395      True\n",
       "3396      True\n",
       "3397      True\n",
       "3400      True\n",
       "3401      True\n",
       "3402      True\n",
       "3406      True\n",
       "3407      True\n",
       "3408      True\n",
       "3409      True\n",
       "3410      True\n",
       "3411      True\n",
       "3412      True\n",
       "3416      True\n",
       "3417      True\n",
       "3419      True\n",
       "3420      True\n",
       "3421      True\n",
       "3422      True\n",
       "3423      True\n",
       "3424      True\n",
       "3425      True\n",
       "3434      True\n",
       "3670      True\n",
       "3671      True\n",
       "3672      True\n",
       "3673      True\n",
       "3674      True\n",
       "3675      True\n",
       "3676      True\n",
       "3681      True\n",
       "3682      True\n",
       "3771      True\n",
       "3772      True\n",
       "3773      True\n",
       "3774      True\n",
       "3775      True\n",
       "3776      True\n",
       "3777      True\n",
       "3778      True\n",
       "3779      True\n",
       "3780      True\n",
       "3781      True\n",
       "3782      True\n",
       "3783      True\n",
       "3980      True\n",
       "3989      True\n",
       "3991      True\n",
       "3992      True\n",
       "3993      True\n",
       "3994      True\n",
       "3995      True\n",
       "3996      True\n",
       "3997      True\n",
       "4001      True\n",
       "4002      True\n",
       "4301      True\n",
       "4302      True\n",
       "4303      True\n",
       "4311      True\n",
       "4312      True\n",
       "4313      True\n",
       "4314      True\n",
       "4315      True\n",
       "4317      True\n",
       "4318      True\n",
       "4319      True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df[labels_df[\"is_valid\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-helicopter",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess Data - Create Process-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "emerging-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data Processing\n"
     ]
    }
   ],
   "source": [
    "#Create new dataFrames for the time features\n",
    "features_process_related = pd.DataFrame() \n",
    "\n",
    "#Calculate the time features for data Processing Dataset\n",
    "features_process_related['f1_assignee_is_commiter'] = cartesian_df.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "features_process_related['f2_timedif_issuecreation_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "features_process_related['f3_timedif_issueupdated_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "features_process_related['f4_timedif_issueresolved_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished data Processing\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "features_process_related.to_pickle(path= \"../data/03_processed/features_process_related.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-associate",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Document Statistics Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vocational-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating document statistics in 0 minutes and 1.1943273544311523 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrames for document statistics features\n",
    "features_document_statistics = pd.DataFrame() \n",
    "\n",
    "features_document_statistics[\"f5_total_terms_jira\"] = cartesian_df.apply(lambda x: calculateTotalWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Calculate total terms JIRA for each trace\n",
    "features_document_statistics[\"f6_total_terms_svn\"] = cartesian_df.apply(lambda x: calculateTotalWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "features_document_statistics[\"f7_unique_terms_jira\"] = cartesian_df.apply(lambda x: calculateUniqueWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate unique terms JIRA for each trace\n",
    "features_document_statistics[\"f8_unique_terms_svn\"] = cartesian_df.apply(lambda x: calculateUniqueWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "features_document_statistics[\"f9_overlap_terms_compared_to_jira\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list1'),\n",
    "                                                            axis=1)\n",
    "features_document_statistics[\"f10_overlap_terms_to_svn\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list2'),\n",
    "                                                            axis=1)\n",
    "features_document_statistics[\"f11_overlap_terms_to_union\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'union'),\n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "features_document_statistics.to_pickle(path= \"../data/03_processed/features_document_statistics.pkl\")\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "features_document_statistics.to_excel(excel_writer = \"../data/03_processed/features_document_statistics.xlsx\", index = False)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating document statistics in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-marijuana",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Information Retrieval Features\n",
    "### 3.3.1 Create tfidf for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "olive-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_information_retrieval = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incorporate-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "svn_all_countvectorizer = CountVectorizer()\n",
    "svn_all_tfidf = createFittedTF_IDF(svn_all_countvectorizer, svn_corpus_all)\n",
    "\n",
    "svn_log_countvectorizer = CountVectorizer()\n",
    "svn_log_tfidf = createFittedTF_IDF(svn_log_countvectorizer, svn_corpus_log)\n",
    "\n",
    "svn_unitname_countvectorizer = CountVectorizer()\n",
    "svn_unitname_tfidf = createFittedTF_IDF(svn_unitname_countvectorizer, svn_corpus_unitname)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA - unigram\n",
    "jira_all_countvectorizer = CountVectorizer()\n",
    "jira_all_tfidf = createFittedTF_IDF(jira_all_countvectorizer, jira_corpus_all)\n",
    "\n",
    "jira_summary_countvectorizer = CountVectorizer()\n",
    "jira_summary_tfidf = createFittedTF_IDF(jira_summary_countvectorizer, jira_corpus_summary)\n",
    "\n",
    "jira_description_countvectorizer = CountVectorizer()\n",
    "jira_description_tfidf = createFittedTF_IDF(jira_description_countvectorizer, jira_corpus_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-audience",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polar-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 10.323507070541382 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f12_ir_log_and_summary_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                                                 svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "limiting-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\scipy\\spatial\\distance.py:728: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.272464275360107 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f13_ir_log_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                    jira_summary_countvectorizer, \n",
    "                                                                                                                                    jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-employer",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exciting-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 8.949869394302368 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f14_ir_log_and_description_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                        svn_log_countvectorizer, \n",
    "                                                                                                                        svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "durable-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 8.374996662139893 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f15_ir_log_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, \n",
    "                                                                                                                                jira_description_countvectorizer, \n",
    "                                                                                                                                jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-department",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "suspected-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.392600774765015 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f16_ir_log_and_jira_all_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                              svn_log_countvectorizer, \n",
    "                                                                                                                              svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "governmental-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 8.975703716278076 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f17_ir_log_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                              jira_all_countvectorizer, \n",
    "                                                                                                                              jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-holder",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "convinced-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.149955987930298 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f18_ir_unitname_and_summary_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, \n",
    "                                                                                                                                       svn_unitname_countvectorizer, \n",
    "                                                                                                                                       svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "small-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 8.65738034248352 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f19_ir_unitname_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, \n",
    "                                                                                                                                     jira_summary_countvectorizer, \n",
    "                                                                                                                                     jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-beast",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "violent-pound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 8.766882419586182 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f20_ir_unitname_and_description_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                                        svn_unitname_countvectorizer, \n",
    "                                                                                                                                        svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "psychological-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.315949440002441 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f21_ir_unitname_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                                          jira_description_countvectorizer, \n",
    "                                                                                                                                          jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-district",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "plastic-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.05677342414856 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f22_ir_unitname_and_jira_all_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, \n",
    "                                                                                                                       svn_unitname_countvectorizer, \n",
    "                                                                                                                       svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "broadband-commodity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 11.112643480300903 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f23_ir_unitname_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, \n",
    "                                                                                                                                   jira_all_countvectorizer, \n",
    "                                                                                                                                   jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-duplicate",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "documentary-amazon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 10.683406829833984 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f24_ir_svn_all_and_summary_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, \n",
    "                                                                                                                             svn_all_countvectorizer, \n",
    "                                                                                                                             svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "religious-fruit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.846113443374634 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f25_ir_svn_all_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, \n",
    "                                                                                                                            jira_summary_countvectorizer, \n",
    "                                                                                                                            jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-forwarding",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "located-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 14.74766206741333 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f26_ir_svn_all_and_description_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, \n",
    "                                                                                                                            svn_all_countvectorizer, \n",
    "                                                                                                                            svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "departmental-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 12.054750680923462 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f27_ir_svn_all_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, \n",
    "                                                                                                                                    jira_description_countvectorizer, \n",
    "                                                                                                                                    jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-envelope",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deluxe-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 10.133258581161499 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f28_ir_svn_all_and_jira_all_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, \n",
    "                                                                                                                     svn_all_countvectorizer, \n",
    "                                                                                                                     svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "happy-charger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0 minutes and 9.759091854095459 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f29_ir_svn_all_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, \n",
    "                                                                                                                      jira_all_countvectorizer, \n",
    "                                                                                                                      jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-pressing",
   "metadata": {},
   "source": [
    "## 3.7 Query Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pharmaceutical-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine document counts\n",
    "jira_documentcount = len(jira_df_clean.index)\n",
    "svn_documentcount = len(svn_df_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-stage",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "opened-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 1 minutes and 32.875670194625854 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "features_qq_specificity = pd.DataFrame()\n",
    "\n",
    "#Calculate temporary IDF stats for each svn\n",
    "features_qq_specificity[\"idf_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Commit_natural_text, \n",
    "                                                                                           svn_all_countvectorizer,\n",
    "                                                                                           svn_all_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f30_avgidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f31_maxidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f32_devidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-moment",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "statistical-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 9.874569416046143 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_log_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Logs, \n",
    "                                                                                       svn_log_countvectorizer, \n",
    "                                                                                       svn_log_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f33_avgidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f34_maxidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f35_devidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_log_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-sampling",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNUnitNames as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "polish-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 17.255774974822998 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_unitname_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Unit_names, \n",
    "                                                                                              svn_unitname_countvectorizer, \n",
    "                                                                                              svn_unitname_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f36_avgidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f37_maxidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f38_devidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-arena",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "explicit-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 1 minutes and 35.75914525985718 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Jira_natural_text, \n",
    "                                                                                            jira_all_countvectorizer,\n",
    "                                                                                            jira_all_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f39_avgidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f40_maxidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f41_devidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-gothic",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "underlying-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 3.77023983001709 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Summary, \n",
    "                                                                                                jira_summary_countvectorizer,\n",
    "                                                                                                jira_summary_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f42_avgidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f43_maxidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f44_devidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-specialist",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "seeing-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 1 minutes and 31.597294092178345 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Description, \n",
    "                                                                                                    jira_description_countvectorizer,\n",
    "                                                                                                    jira_description_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f45_avgidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f46_maxidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f47_devidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-ensemble",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "homeless-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 17.94122838973999 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Commit_natural_text,\n",
    "                                                                                            svn_all_countvectorizer,\n",
    "                                                                                            svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f48_avgictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_all_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f49_maxictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f50_devictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-anthropology",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sought-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.6651122570037842 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Logs, \n",
    "                                                                                             svn_log_countvectorizer, \n",
    "                                                                                             svn_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f51_avgictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_log_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f52_maxictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f53_devictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-bikini",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "drawn-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.849517822265625 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Unit_names, \n",
    "                                                                                                  svn_unitname_countvectorizer, \n",
    "                                                                                                  svn_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f54_avgictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_unitname_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f55_maxictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f56_devictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-japan",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "separate-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 1.4035217761993408 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Jira_natural_text, \n",
    "                                                                                              jira_all_countvectorizer, \n",
    "                                                                                              jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f57_avgictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_all_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f58_maxictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f59_devictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-freeware",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "chemical-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.6491963863372803 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Summary,\n",
    "                                                                                                 jira_summary_countvectorizer, \n",
    "                                                                                                 jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f60_avgictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_summary_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f61_maxictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f62_devictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-offset",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "contained-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 1.3346915245056152 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Description,\n",
    "                                                                                                     jira_description_countvectorizer,\n",
    "                                                                                                     jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f63_avgictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_description_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f64_maxictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f65_devictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-sheffield",
   "metadata": {},
   "source": [
    "#### Entropy (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "threaded-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 24.346182584762573 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Commit_natural_text,\n",
    "                                                                                                   svn_all_countvectorizer,\n",
    "                                                                                                   svn_documentcount,\n",
    "                                                                                                   svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f66_avgentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f67_medentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f68_maxentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f69_deventropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-iraqi",
   "metadata": {},
   "source": [
    "#### Entropy (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "tight-engineer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 3.977773427963257 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Logs, \n",
    "                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                 svn_documentcount,\n",
    "                                                                                                 svn_df_clean.Logs),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f70_avgentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f71_medentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f72_maxentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f73_deventropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recorder",
   "metadata": {},
   "source": [
    "#### Entropy (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dirty-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 16.890952348709106 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Unit_names, \n",
    "                                                                                                      svn_unitname_countvectorizer, \n",
    "                                                                                                      svn_documentcount,\n",
    "                                                                                                      svn_df_clean.Unit_names),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f74_avgentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f75_medentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f76_maxentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f77_deventropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-kingston",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "featured-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 7.246494293212891 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Jira_natural_text, \n",
    "                                                                                                    jira_all_countvectorizer,\n",
    "                                                                                                    jira_documentcount,\n",
    "                                                                                                    jira_df_clean.Jira_natural_text),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f78_avgentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f79_medentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f80_maxentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f81_deventropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-technical",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "apart-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.9277384281158447 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Summary, \n",
    "                                                                                                        jira_summary_countvectorizer,\n",
    "                                                                                                        jira_documentcount,\n",
    "                                                                                                        jira_df_clean.Summary),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f82_avgentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f83_medentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f84_maxentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f85_deventropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-wallace",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "continuing-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 5.5930352210998535 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Description,\n",
    "                                                                                                            jira_description_countvectorizer,\n",
    "                                                                                                            jira_documentcount,\n",
    "                                                                                                            jira_df_clean.Description),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f86_avgentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f87_medentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f88_maxentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f89_deventropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-kazakhstan",
   "metadata": {},
   "source": [
    "##### Query Scope (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "integrated-parallel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 2.8513717651367188 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f90_queryscope_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Commit_natural_text,\n",
    "                                                                                                         svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-madrid",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "macro-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 1.4324731826782227 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f91_queryscope_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Logs,\n",
    "                                                                                                         svn_df_clean.Logs),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-brooks",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "turned-invasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.8247652053833008 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f92_queryscope_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Unit_names, \n",
    "                                                                                                              svn_df_clean.Unit_names),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-cocktail",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "soviet-keyboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.5306556224822998 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f93_queryscope_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Jira_natural_text,\n",
    "                                                                                                          jira_df_clean.Jira_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-mumbai",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "coordinated-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.15979290008544922 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f94_queryscope_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Summary, \n",
    "                                                                                                              jira_df_clean.Summary),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-hopkins",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "awful-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.5108222961425781 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f95_queryscope_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Description,\n",
    "                                                                                                                  jira_df_clean.Description),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-yorkshire",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "saving-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.47345781326293945 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f96_scs_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Commit_natural_text,\n",
    "                                                                                           svn_all_countvectorizer,\n",
    "                                                                                           svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-paint",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "substantial-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.10251736640930176 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f97_scs_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Logs,\n",
    "                                                                                           svn_log_countvectorizer,\n",
    "                                                                                           svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-calendar",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "breathing-collective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.47259950637817383 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f98_scs_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Unit_names,\n",
    "                                                                                                svn_unitname_countvectorizer,\n",
    "                                                                                                svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-cargo",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pursuant-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.5278406143188477 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f99_scs_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Jira_natural_text,\n",
    "                                                                                            jira_all_countvectorizer,\n",
    "                                                                                            jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-keeping",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "beginning-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.13031601905822754 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f100_scs_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Summary, \n",
    "                                                                                                jira_summary_countvectorizer,\n",
    "                                                                                                jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-struggle",
   "metadata": {},
   "source": [
    "##### Kullback-Leiber divergence (JIRA Description as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "waiting-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.4284038543701172 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f101_scs_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Description, \n",
    "                                                                                                   jira_description_countvectorizer,\n",
    "                                                                                                   jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-commitment",
   "metadata": {},
   "source": [
    "### Query Quality Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-execution",
   "metadata": {},
   "source": [
    "#### SCQ (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "latin-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 4.63423752784729 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "features_qq_similarity = pd.DataFrame()\n",
    "\n",
    "#Calculate SCQ stats for each svn\n",
    "features_qq_similarity[\"scq_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Commit_natural_text, \n",
    "                                                                                          svn_df_clean.Commit_natural_text,\n",
    "                                                                                          svn_all_countvectorizer,\n",
    "                                                                                          svn_all_tfidf,\n",
    "                                                                                          svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f102_SvnAsQuery_avgSCQ\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_all_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f103_SvnAsQuery_maxSCQ\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_all_as_query), axis=1)\n",
    "features_qq_similarity[\"f104_SvnAsQuery_sumSCQ\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-technical",
   "metadata": {},
   "source": [
    "#### SCQ (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "together-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 5.091441631317139 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Logs, \n",
    "                                                                                          svn_df_clean.Logs,\n",
    "                                                                                          svn_log_countvectorizer,\n",
    "                                                                                          svn_log_tfidf,\n",
    "                                                                                          svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f105_avgscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_log_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f106_maxscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_log_as_query), axis=1)\n",
    "features_qq_similarity[\"f107_sumscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-guess",
   "metadata": {},
   "source": [
    "#### SCQ (SVNUnitNames as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "celtic-better",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 3.9663546085357666 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Unit_names,\n",
    "                                                                                               svn_df_clean.Unit_names,\n",
    "                                                                                               svn_unitname_countvectorizer,\n",
    "                                                                                               svn_unitname_tfidf,\n",
    "                                                                                               svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f108_avgscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_unitname_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f109_maxscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_unitname_as_query), axis=1)\n",
    "features_qq_similarity[\"f110_sumscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-laundry",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "partial-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 5.64606237411499 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Jira_natural_text,\n",
    "                                                                                           jira_df_clean.Jira_natural_text,\n",
    "                                                                                           jira_all_countvectorizer,\n",
    "                                                                                           jira_all_tfidf,\n",
    "                                                                                           jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f111_avgscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_all_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f112_maxscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_all_as_query), axis=1)\n",
    "features_qq_similarity[\"f113_sumscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-updating",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "norwegian-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 4.059334993362427 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Summary, \n",
    "                                                                                               jira_df_clean.Summary,\n",
    "                                                                                               jira_summary_countvectorizer,\n",
    "                                                                                               jira_summary_tfidf,\n",
    "                                                                                               jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f114_avgscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_summary_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f115_maxscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_summary_as_query), axis=1)\n",
    "features_qq_similarity[\"f116_sumscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-spirit",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "affected-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 5.319384813308716 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Description, \n",
    "                                                                                                   jira_df_clean.Description,\n",
    "                                                                                                   jira_description_countvectorizer,\n",
    "                                                                                                   jira_description_tfidf,\n",
    "                                                                                                   jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f117_avgscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_description_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f118_maxscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_description_as_query), axis=1)\n",
    "features_qq_similarity[\"f119_sumscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-enterprise",
   "metadata": {},
   "source": [
    "### Query Quality - Term Relatedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-owner",
   "metadata": {},
   "source": [
    "#### PMI (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "stunning-legislation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\OneDrive\\UU\\Master\\12. Master Thesis\\2. Refactor\\notebooks\\..\\src\\d03_processing\\calculateQueryQuality.py:334: RuntimeWarning: All-NaN axis encountered\n",
      "  maxPMI = np.nanmax(pmiList)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 2 minutes and 30.173901081085205 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_all_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_all_countvectorizer, svn_df_clean.Commit_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Commit_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "features_qq_termrelatedness = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Commit_natural_text,\n",
    "                                                                                               termFrequencies, \n",
    "                                                                                               termPairFrequencies, \n",
    "                                                                                               svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f120_avgpmi_svn_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_all_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f121_maxpmi_svn_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_all_as_query), axis=1)\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-hotel",
   "metadata": {},
   "source": [
    "#### PMI (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "verified-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 1 minutes and 31.093814849853516 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_log_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_log_countvectorizer, svn_df_clean.Logs)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Logs)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Logs,\n",
    "                                                                                               termFrequencies, \n",
    "                                                                                               termPairFrequencies, \n",
    "                                                                                               svn_df_clean.Logs),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f122_avgpmi_svn_log_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_log_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f123_maxpmi_svn_log_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_log_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-efficiency",
   "metadata": {},
   "source": [
    "#### PMI (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "conscious-feeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 26.07052516937256 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_unitname_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_unitname_countvectorizer, svn_df_clean.Unit_names)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Unit_names)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Unit_names,\n",
    "                                                                                                    termFrequencies, \n",
    "                                                                                                    termPairFrequencies, \n",
    "                                                                                                    svn_df_clean.Unit_names),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f124_avgpmi_svn_unitname_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_unitname_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f125_maxpmi_svn_unitname_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_unitname_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stomach",
   "metadata": {},
   "source": [
    "#### PMI (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "durable-murder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 13.071505784988403 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_all_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_all_countvectorizer, jira_df_clean.Jira_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Jira_natural_text)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Jira_natural_text, \n",
    "                                                                                                termFrequencies, \n",
    "                                                                                                termPairFrequencies, \n",
    "                                                                                                jira_df_clean.Jira_natural_text),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f126_avgpmi_jira_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_all_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f127_maxpmi_jira_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_all_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-youth",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "prerequisite-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 0.4532656669616699 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_summary_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_summary_countvectorizer, jira_df_clean.Summary)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Summary)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Summary, \n",
    "                                                                                                   termFrequencies, \n",
    "                                                                                                   termPairFrequencies, \n",
    "                                                                                                   jira_df_clean.Summary),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f128_avgpmi_jira_summary_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_summary_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f129_maxpmi_jira_summary_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_summary_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-award",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "stable-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 0 minutes and 11.56856632232666 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_description_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_description_countvectorizer, jira_df_clean.Description)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Description)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Description, \n",
    "                                                                                                      termFrequencies, \n",
    "                                                                                                      termPairFrequencies, \n",
    "                                                                                                      jira_df_clean.Description),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f130_avgpmi_jira_description_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_description_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f131_maxpmi_jira_description_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_description_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-material",
   "metadata": {},
   "source": [
    "## 3.8 Preprocess Data - Load and transform feature families needed for training\n",
    "Load features and create a normalized set of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "behavioral-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Process-Related Features\n",
    "features_process_related = pd.read_pickle(r'../data/03_processed/features_process_related.pkl')\n",
    "\n",
    "#Load IR-Related Features\n",
    "features_information_retrieval = pd.read_pickle(r'../data/03_processed/features_information_retrieval.pkl')\n",
    "\n",
    "#Load Document Statistics Features\n",
    "features_document_statistics = pd.read_pickle(r'../data/03_processed/features_document_statistics.pkl')\n",
    "\n",
    "#Load Query Quality Features\n",
    "features_qq_specificity = pd.read_pickle(r'../data/03_processed/features_qq_specificity.pkl')\n",
    "features_qq_similarity = pd.read_pickle(r'../data/03_processed/features_qq_similarity.pkl')\n",
    "features_qq_termrelatedness = pd.read_pickle(r'../data/03_processed/features_qq_termrelatedness.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "superb-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Process-Related Features\n",
    "features_process_related_normalized = normalize_data(features_process_related)\n",
    "\n",
    "#Normalize IR-Related Features\n",
    "features_information_retrieval_normalized = normalize_data(features_information_retrieval)\n",
    "\n",
    "#Normalize Document Statistics Features\n",
    "features_document_statistics_normalized = normalize_data(features_document_statistics)\n",
    "\n",
    "#Normalize Query Quality Features\n",
    "features_qq_specificity_normalized = normalize_data(features_qq_specificity)\n",
    "features_qq_similarity_normalized = normalize_data(features_qq_similarity)\n",
    "features_qq_termrelatedness_normalized = normalize_data(features_qq_termrelatedness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-tower",
   "metadata": {},
   "source": [
    "Put all features in a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sophisticated-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a single data frame for the non-normalized features\n",
    "features_all_df = pd.concat([features_process_related,\n",
    "                             features_document_statistics,\n",
    "                             features_information_retrieval,\n",
    "                             features_qq_specificity,\n",
    "                             features_qq_similarity,\n",
    "                             features_qq_termrelatedness], axis=1)\n",
    "\n",
    "#Create a single data frame for the normalized features\n",
    "features_all_normalized_df = pd.concat([features_process_related_normalized,\n",
    "                                        features_document_statistics_normalized,\n",
    "                                        features_information_retrieval_normalized,\n",
    "                                        features_qq_specificity_normalized,\n",
    "                                        features_qq_similarity_normalized,\n",
    "                                        features_qq_termrelatedness_normalized], axis=1)\n",
    "\n",
    "#Save into xlsx files\n",
    "features_all_df.to_excel(excel_writer = \"../results/1. Trace Link Feature Data/features_non-normalized.xlsx\", index = False)\n",
    "features_all_normalized_df.to_excel(excel_writer = \"../results/1. Trace Link Feature Data/features_normalized.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-paint",
   "metadata": {},
   "source": [
    "Perform additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "forty-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the NaN to 0\n",
    "features_all_df = features_all_df.fillna(0)\n",
    "features_all_normalized_df = features_all_normalized_df.fillna(0)\n",
    "\n",
    "#Saving feature names for later use\n",
    "feature_name_df = list(features_all_df.columns)\n",
    "\n",
    "#Transform pandas data frame into numpy arrays\n",
    "features_all_array = np.array(features_all_df)\n",
    "features_all_normalized_array = np.array(features_all_normalized_df)\n",
    "\n",
    "#Load labels\n",
    "labels_df = pd.read_pickle(r'../data/03_processed/labels_df.pkl')\n",
    "labels_array = np.array(labels_df[\"is_valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-album",
   "metadata": {},
   "source": [
    "# 4.1 Evaluation - Non-normalized\n",
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "thorough-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-digest",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "strategic-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:11:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:12:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array,\n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-calculator",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "handmade-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-leader",
   "metadata": {},
   "source": [
    "# 4 Evaluation - Normalized\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "manual-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array,\n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'random_forests', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = False,\n",
    "                            n_runs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-cooler",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "processed-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:13:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:13:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:13:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rande\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'xg_boost', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-binding",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "conventional-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluation_metrics(rebalancing_strategy = 'none', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'over', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = 'under', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)\n",
    "\n",
    "generate_evaluation_metrics(rebalancing_strategy = '5050', \n",
    "                            classification_algorithm = 'light_gbm', \n",
    "                            data = features_all_normalized_array, \n",
    "                            labels = labels_array, \n",
    "                            feature_names = feature_name_df,\n",
    "                            is_normalized = True,\n",
    "                            n_runs = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
