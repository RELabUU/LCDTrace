{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Import Self-written Functions\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from d00_utils.calculateTimeDifference import calculateTimeDifference #Function to calc time difference\n",
    "from d01_data.loadCommits import loadCommits #Function to load SVN data\n",
    "from d02_intermediate.cleanCommitData import cleanCommitData #Function to clean commit data\n",
    "from d02_intermediate.cleanJiraData import cleanJiraData #Function to clean JIRA data\n",
    "\n",
    "from d03_processing.createFittedTF_IDF import createFittedTF_IDF #Function to see if a trace is valid\n",
    "from d03_processing.createCorpusFromDocumentList import createCorpusFromDocumentList #Function to create a corpus\n",
    "from d03_processing.checkValidityTrace import checkValidityTrace #Function to see if a trace is valid\n",
    "from d03_processing.calculateTimeDif import calculateTimeDif #Calculate the time difference between 2 dates in seconds\n",
    "from d03_processing.checkFullnameEqualsEmail import checkFullnameEqualsEmail #Check if fullName is equal to the email\n",
    "from d03_processing.calculateCosineSimilarity import calculateCosineSimilarity #Calculate the cos similarity\n",
    "from d03_processing.calculateDocumentStatistics import calculateUniqueWordCount\n",
    "from d03_processing.calculateDocumentStatistics import calculateTotalWordCount\n",
    "from d03_processing.calculateDocumentStatistics import calculateOverlapBetweenDocuments\n",
    "\n",
    "from d04_modelling.summariseClassDistribution import summariseClassDistribution #Visualize the class distribution\n",
    "from d04_modelling.showModelPerformance import showModelPerformance # Show several performance measures\n",
    "\n",
    "#Display full value of a column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-sellers",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amended-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dataset\n",
    "\n",
    "datasetDirectory = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw JIRA dataset\n",
    "rawData_JIRA_dataProcessing = pd.read_csv('../data/01_raw/JIRA Mendix.csv')\n",
    "rawData_JIRA_academy = pd.read_excel('../data/01_raw/JIRA Mendix Academy export.xlsx')\n",
    "rawData_JIRA_academyMay = pd.read_excel('../data/01_raw/JIRA Mendix Academy export_15_05_2021.xlsx')\n",
    "\n",
    "#import\n",
    "rawData_SVN_dataProcessing = loadCommits(\"../data/01_raw/data-processing-svn-dump.txt\")\n",
    "rawData_SVN_academy= loadCommits(\"../data/01_raw/academy-svn-dump.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawData_SVN_dataProcessing.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-utility",
   "metadata": {},
   "source": [
    "# 2. Clean Raw Data\n",
    "## 2.1 Clean Raw Data - SVN Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#nltk for NLP \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#Function to transform natural text into unigram tokens\n",
    "def preprocessNaturalLanguage(text, porterStemmer, cachedStopWords):\n",
    "    string_text = str(text)\n",
    "    #lowercase the string\n",
    "    lower_case_string = string_text.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    no_interpunction = lower_case_string.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #Remove numbers\n",
    "    no_numbers = ''.join([i for i in no_interpunction if not i.isdigit()])\n",
    "    \n",
    "    #tokenize string\n",
    "    tokens = word_tokenize(no_interpunction)\n",
    "    \n",
    "    #remove stopwords\n",
    "    tokens_without_sw = [word for word in tokens if not word in cachedStopWords]\n",
    "    \n",
    "    #Stem the tokens\n",
    "    stemmedToken = list(map(porterStemmer.stem, tokens_without_sw))\n",
    "\n",
    "    return(stemmedToken)\n",
    "\n",
    "#Function to transform natural text into n-gram tokens\n",
    "def preprocessNGrams(text, porterStemmer, cachedStopWords, nGramSize):\n",
    "    string_text = str(text)\n",
    "    \n",
    "    #lowercase the string\n",
    "    lower_case_string = string_text.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    no_interpunction = lower_case_string.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #Remove numbers\n",
    "    no_numbers = ''.join([i for i in no_interpunction if not i.isdigit()])\n",
    "    \n",
    "    #tokenize string\n",
    "    tokens = word_tokenize(no_interpunction)\n",
    "    \n",
    "    #Create the ngrams\n",
    "    ngrams = list(nltk.ngrams(tokens, nGramSize))\n",
    "    \n",
    "    #remove all the n-grams containing a stopword\n",
    "    cleanNGrams = [ngram for ngram in ngrams if not any(stop in ngram for stop in cachedStopWords)]\n",
    "    \n",
    "    #Stem the tokens\n",
    "    stemmedNGrams = []\n",
    "    for ngram in cleanNGrams:\n",
    "        stemmed = list(map(porterStemmer.stem, ngram))\n",
    "        stemmedNGrams.append(stemmed)\n",
    "    return(stemmedNGrams)\n",
    "\n",
    "#Function to transform date into a date object\n",
    "def preprocessCommitDate(date_string):\n",
    "    date_time_obj = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S.%fZ')  \n",
    "    return(date_time_obj)\n",
    "    \n",
    "#Remove the found Issue key from the log\n",
    "def removeIssueKey(log_message):\n",
    "    issue_keys = re.findall(r\"LRN+.[0-9]+|AFM+.[0-9]+|MA+.[0-9]+|AFI+.[0-9]+|EM+.[0-9]+|OE+.[0-9]+|EM+.[0-9]+\", log_message)\n",
    "    log_message_without_key = log_message\n",
    "    for issue_key in issue_keys:\n",
    "        log_message_without_key = log_message_without_key.replace(issue_key, \"\")\n",
    "    return(log_message_without_key)\n",
    "\n",
    "def unitNamesLambdaFunc(unitName, stemmer):\n",
    "    #Lower case\n",
    "    unitNameLowered = unitName.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    noInterpunction = unitNameLowered.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #Remove numbers\n",
    "    noNumbers = ''.join([i for i in noInterpunction if not i.isdigit()])\n",
    "    \n",
    "    stemmendUnitName = stemmer.stem(noInterpunction)\n",
    "    \n",
    "    \n",
    "    return(stemmendUnitName)\n",
    "    \n",
    "\n",
    "def preprocessUnitNames(unitName, porterStemmer, cachedStopWords):\n",
    "    if (isinstance(unitName, str)):\n",
    "        #Split camelCasing\n",
    "        unitNameSplitList = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', unitName)).split()\n",
    "        \n",
    "        porterStemmer = PorterStemmer() #create an object of class PorterStemmer\n",
    "        \n",
    "        #Preprocess each split found.\n",
    "        unitNameLowered = list(map(lambda unitName: unitNamesLambdaFunc(unitName, porterStemmer), \n",
    "                                   unitNameSplitList))\n",
    "        \n",
    "        #Check for stopwords\n",
    "        tokensWithoutSW = [word for word in unitNameLowered if not word in cachedStopWords]\n",
    "\n",
    "        return(tokensWithoutSW)\n",
    "\n",
    "def preprocessNGramsUnitNames(unitName, porterStemmer, cachedStopWords, nGramSize):\n",
    "    if (isinstance(unitName, str)):\n",
    "        #Split camelCasing\n",
    "        unitNameSplitList = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', unitName)).split()\n",
    "        \n",
    "        cleanedUnitNames = []\n",
    "        for unitNameSplit in unitNameSplitList:\n",
    "            #Lower case unit names\n",
    "            lowerCased = unitNameSplit.lower()\n",
    "\n",
    "            #Remove interpunction\n",
    "            removedInterpunction = lowerCased.translate(str.maketrans('','',string.punctuation))\n",
    "            cleanedUnitNames.append(removedInterpunction)\n",
    "            \n",
    "        #Transform to string (needed for tokenizer\n",
    "        unitNameString = ' '.join(cleanedUnitNames)\n",
    "\n",
    "        #Tokenzize words\n",
    "        tokenized = word_tokenize(unitNameString)\n",
    "        \n",
    "        #Create the ngrams\n",
    "        ngrams = list(nltk.ngrams(tokenized, nGramSize))\n",
    "        \n",
    "        porterStemmer = PorterStemmer() #create an object of class PorterStemmer\n",
    "        \n",
    "        #remove all the n-grams containing a stopword\n",
    "        cleanNGrams = [ngram for ngram in ngrams if not any(stop in ngram for stop in cachedStopWords)]\n",
    "    \n",
    "        #Stem the tokens\n",
    "        stemmedNGrams = []\n",
    "        for ngram in cleanNGrams:\n",
    "            stemmed = list(map(porterStemmer.stem, ngram))\n",
    "            stemmedNGrams.append(stemmed)\n",
    "            \n",
    "        return(stemmedNGrams)\n",
    "\n",
    "#Method to clean all columns of the provided data\n",
    "def cleanCommitData(rawCommitData): \n",
    "    #create an object of class PorterStemmer\n",
    "    porterStemmer = PorterStemmer()\n",
    "    \n",
    "    #Find all stopwords\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    \n",
    "    #Remove all revisions without an issue key in the log message\n",
    "    commit_df = rawCommitData[rawCommitData[\"related_issue_key\"].notna()]\n",
    "\n",
    "    #Execute cleaning methods on dataset\n",
    "    cleaned_commit_logs = commit_df['log'].apply(lambda x: removeIssueKey(x))\n",
    "    processed_commit_logs = cleaned_commit_logs.apply(lambda x: preprocessNaturalLanguage(x, porterStemmer, cachedStopWords))\n",
    "    processed_commit_logs_2grams = cleaned_commit_logs.apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "    processed_commit_logs_3grams = cleaned_commit_logs.apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 3))\n",
    "    processed_date_times = commit_df['date'].apply(lambda x: preprocessCommitDate(x))\n",
    "    processed_unit_names = commit_df['impacted_unit_names'].apply(lambda x: preprocessUnitNames(x, porterStemmer, cachedStopWords))\n",
    "    processed_unit_names_2grams = commit_df['impacted_unit_names'].apply(lambda x: preprocessNGramsUnitNames(x, porterStemmer, cachedStopWords, 2))\n",
    "    processed_unit_names_3grams = commit_df['impacted_unit_names'].apply(lambda x: preprocessNGramsUnitNames(x, porterStemmer, cachedStopWords, 3))\n",
    "    \n",
    "\n",
    "    #Put all data together into a new dataframe\n",
    "    commit_data = {'Revision': commit_df[\"revision\"],\n",
    "               'Email' : commit_df[\"email\"],\n",
    "               'Commit_date': processed_date_times,\n",
    "               \"Issue_key_commit\": commit_df[\"related_issue_key\"],\n",
    "               'Logs': processed_commit_logs, \n",
    "               'Logs_2grams': processed_commit_logs_2grams, \n",
    "               'Logs_3grams': processed_commit_logs_3grams, \n",
    "               'Unit_names': processed_unit_names,\n",
    "               'Unit_names_2grams': processed_unit_names_2grams,\n",
    "               'Unit_names_3grams': processed_unit_names_3grams,\n",
    "               'Commit_natural_text': processed_commit_logs + processed_unit_names,\n",
    "               'Commit_natural_text_2grams': processed_commit_logs_2grams + processed_unit_names_2grams,\n",
    "               'Commit_natural_text_3grams': processed_commit_logs_3grams + processed_unit_names_3grams\n",
    "               }\n",
    "               \n",
    "    commit_processed_df = pd.DataFrame(data=commit_data)\n",
    "\n",
    "    return(commit_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "intermediateData_SVN_dataProcessing = cleanCommitData(rawData_SVN_dataProcessing)\n",
    "intermediateData_SVN_academy = cleanCommitData(rawData_SVN_academy)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "intermediateData_SVN_dataProcessing.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_SVN_dataProcessing.xlsx\", index = False)\n",
    "intermediateData_SVN_academy.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_SVN_academy.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "intermediateData_SVN_dataProcessing.to_pickle(path= \"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")\n",
    "intermediateData_SVN_academy.to_pickle(path= \"../data/02_intermediate/intermediateData_SVN_academy.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished cleaning after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import string\n",
    "#nltk for NLP \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag  import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#Function to clean the comments\n",
    "def clean_comments(comment):\n",
    "    try:\n",
    "        commentDates = re.findall(r\"[0-9]{2} [A-Z][a-z]{2} [0-9]{4} [0-9]{2}:[0-9]{2};[a-zA-Z0-9_]{24};\", comment)\n",
    "        accountIds = re.findall(r\"\\[~accountid:[a-zA-Z0-9]{24}\\]\", comment)\n",
    "               \n",
    "        \n",
    "        cleanedComment = comment.replace(\"nan\",'')\n",
    "        for commentDate in commentDates:\n",
    "            cleanedComment = cleanedComment.replace(commentDate,'')\n",
    "        \n",
    "        for accountId in accountIds: \n",
    "            cleanedComment = cleanedComment.replace(accountId,'')\n",
    "        \n",
    "        return(cleanedComment)\n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "def preprocess(text, porterStemmer, cachedStopwords):\n",
    "    string_text = str(text)\n",
    "    #lowercase the string\n",
    "    lower_case_string = string_text.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    no_interpunction = lower_case_string.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #Remove numbers\n",
    "    no_numbers = ''.join([i for i in no_interpunction if not i.isdigit()])\n",
    "    \n",
    "    #tokenize string\n",
    "    tokens = word_tokenize(no_numbers)\n",
    "    \n",
    "    #remove stopwords\n",
    "    tokens_without_sw = [word for word in tokens if not word in cachedStopwords]\n",
    "    \n",
    "    #Stem the tokens\n",
    "    stemmedToken = list(map(porterStemmer.stem, tokens_without_sw))\n",
    "\n",
    "    return(stemmedToken)\n",
    "\n",
    "def preprocessNGrams(text, porterStemmer, cachedStopWords, nGramSize):\n",
    "    string_text = str(text)\n",
    "    \n",
    "    #lowercase the string\n",
    "    lower_case_string = string_text.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    no_interpunction = lower_case_string.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #Remove numbers\n",
    "    no_numbers = ''.join([i for i in no_interpunction if not i.isdigit()])\n",
    "    \n",
    "    #tokenize string\n",
    "    tokens = word_tokenize(no_numbers)\n",
    "    \n",
    "    #Create the ngrams\n",
    "    ngrams = list(nltk.ngrams(tokens, nGramSize))\n",
    "    \n",
    "    #remove all the n-grams containing a stopword\n",
    "    cleanNGrams = [ngram for ngram in ngrams if not any(stop in ngram for stop in cachedStopWords)]\n",
    "    \n",
    "    #Stem the tokens\n",
    "    stemmedNGrams = []\n",
    "    for ngram in cleanNGrams:\n",
    "        stemmed = list(map(porterStemmer.stem, ngram))\n",
    "        stemmedNGrams.append(stemmed)\n",
    "    return(stemmedNGrams)\n",
    "\n",
    "#Function to transform date into a date object\n",
    "def preprocess_jira_date(date_string):\n",
    "    if(isinstance(date_string, str)):\n",
    "        try:\n",
    "            date_time_obj = datetime.strptime(date_string, '%d %b %Y %H:%M')\n",
    "        except:\n",
    "            date_time_obj = datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S:%f')\n",
    "        return(date_time_obj)\n",
    "    elif(isinstance(date_string, datetime)): \n",
    "        return(date_string)\n",
    "    else:\n",
    "        return(np.nan)\n",
    "    \n",
    "    \n",
    "def findVerbs(tokenList):\n",
    "    posTags = pos_tag(tokenList)\n",
    "    verbAbrList = ['VBP', 'VBG', 'VBN', 'VBP', 'VBZ', 'RB', 'RBR', 'RBS']\n",
    "    verbList = []\n",
    "    for posTag in posTags:\n",
    "        if posTag[1] in verbAbrList:\n",
    "            verbList.append(posTag[0])\n",
    "    return(verbList)\n",
    "\n",
    "#Preprocess all the features and transform to the format needed for further processing.\n",
    "def preprocessJiraData(cleanDataFrame, preprocessComments, porterStemmer, cachedStopWords, startTime):\n",
    "    if (preprocessComments == True):\n",
    "        nOfSteps = '4'\n",
    "    else:\n",
    "        nOfSteps = '3'\n",
    "\n",
    "    #preprocess Summaries\n",
    "    jira_summaries = cleanDataFrame['Summary'].apply(lambda x: preprocess(x, porterStemmer, cachedStopWords))\n",
    "    jira_summaries_2grams = cleanDataFrame['Summary'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "    jira_summaries_3grams = cleanDataFrame['Summary'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 3))\n",
    "    \n",
    "    endTimeCleaningSummaries = time.time() - startTime\n",
    "    print(\"1/\" + nOfSteps + \") Finished Cleaning Summaries after \" + str(endTimeCleaningSummaries) + \" sec\")\n",
    "\n",
    "    #preprocess Descriptions\n",
    "    jira_descriptions = cleanDataFrame['Description'].apply(lambda x: preprocess(x, porterStemmer, cachedStopWords))\n",
    "    jira_descriptions_2grams = cleanDataFrame['Description'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "    jira_descriptions_3grams = cleanDataFrame['Description'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "    \n",
    "    endTimeCleaningDescriptions = time.time() - startTime\n",
    "    print(\"2/\" + nOfSteps + \") Finished Cleaning Description after \" + str(endTimeCleaningDescriptions) + \" sec\")\n",
    "\n",
    "    #preprocess Dates\n",
    "    jira_creation = cleanDataFrame['Created'].apply(lambda x: preprocess_jira_date(x))\n",
    "    jira_updated = cleanDataFrame['Updated'].apply(lambda x: preprocess_jira_date(x))\n",
    "    jira_resolved = cleanDataFrame['Resolved'].apply(lambda x: preprocess_jira_date(x))\n",
    "    endTimeCleaningDates = time.time() - startTime\n",
    "    print(\"3/\" + nOfSteps + \") Finished Cleaning Dates after \" + str(endTimeCleaningDates) + \" sec\")\n",
    "\n",
    "    #Comments take too long for a test run.\n",
    "    if (preprocessComments == True):\n",
    "        jira_comments = cleanDataFrame['Comments'].apply(lambda x: preprocess(x, porterStemmer, cachedStopWords))\n",
    "        jira_comments_2grams = cleanDataFrame['Comments'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "        jira_comments_3grams = cleanDataFrame['Comments'].apply(lambda x: preprocessNGrams(x, porterStemmer, cachedStopWords, 2))\n",
    "        endTimeCleaningComments = time.time() - startTime\n",
    "        print(\"4/\" + nOfSteps + \") Finished Cleaning Comments after \" + str(endTimeCleaningComments) + \" sec\")\n",
    "\n",
    "         #create JIRA corpus by merging Summary and Description\n",
    "        jira_data = {'Issue_key_jira': cleanDataFrame['Issue key'], \n",
    "             'Assignee': cleanDataFrame['Assignee'],\n",
    "             'Jira_created_date': jira_creation, \n",
    "             'Jira_updated_date': jira_updated, \n",
    "             'Jira_resolved_date': jira_resolved, \n",
    "             'Summary': jira_summaries, \n",
    "             'Summary_2grams': jira_summaries_2grams,\n",
    "             'Summary_3grams': jira_summaries_3grams, \n",
    "             'Description': jira_descriptions,\n",
    "             'Description_2grams': jira_descriptions_2grams,\n",
    "             'Description_3grams': jira_descriptions_3grams,\n",
    "             'Comments': jira_comments,\n",
    "             'Comments_2grams': jira_comments_2grams,\n",
    "             'Comments_3grams': jira_comments_3grams,\n",
    "             'Jira_natural_text': jira_summaries +  jira_descriptions + jira_comments,\n",
    "             'Jira_natural_text_2grams': jira_summaries_2grams +  jira_descriptions_2grams + jira_comments_2grams,\n",
    "             'Jira_natural_text_3grams': jira_summaries_3grams +  jira_descriptions_3grams + jira_comments_3grams}\n",
    "    else:\n",
    "         #create JIRA corpus by merging Summary and Description\n",
    "        jira_data = {'Issue_key_jira': cleanDataFrame['Issue key'], \n",
    "             'Assignee': cleanDataFrame['Assignee'],\n",
    "             'Jira_created_date': jira_creation, \n",
    "             'Jira_updated_date': jira_updated, \n",
    "             'Jira_resolved_date': jira_resolved, \n",
    "             'Summary': jira_summaries,\n",
    "             'Summary_2grams': jira_summaries_2grams,\n",
    "             'Summary_3grams': jira_summaries_3grams,\n",
    "             'Description': jira_descriptions,\n",
    "             'Description_2grams': jira_descriptions_2grams,\n",
    "             'Description_3grams': jira_descriptions_3grams,\n",
    "             'Jira_natural_text': jira_summaries +  jira_descriptions,\n",
    "             'Jira_natural_text_2grams': jira_summaries_2grams +  jira_descriptions_2grams,\n",
    "             'Jira_natural_text_3grams': jira_summaries_3grams +  jira_descriptions_3grams}\n",
    "\n",
    "    jira_processed_df = pd.DataFrame(data=jira_data)\n",
    "    \n",
    "    #Find verbs\n",
    "    jira_processed_df['verbs'] = jira_processed_df['Jira_natural_text'].apply(lambda x: findVerbs(x))\n",
    "    \n",
    "    return(jira_processed_df)\n",
    "\n",
    "#Input dataframe and num of_comments, and bool to determine if comments need to be cleaned\n",
    "def cleanJiraData(dataFrame, cleanComments, commentAmount):\n",
    "    startTime = time.time()\n",
    "\n",
    "    #create an object of class PorterStemmer\n",
    "    porterStemmer = PorterStemmer()\n",
    "    \n",
    "    #Find all stopwords\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "    if (cleanComments == True):\n",
    "        #Subset only all comments \n",
    "        loc_first_comment = dataFrame.columns.get_loc('Comment') # Variable storing the col location of the 1st comment\n",
    "    \n",
    "        dataFrame[\"Comments\"] = dataFrame.iloc[:,loc_first_comment:loc_first_comment+commentAmount].apply(\n",
    "            lambda x: \" \".join(x.astype(str)), axis=1)\n",
    "    \n",
    "        #First remove the date and comment string from the comments\n",
    "        dataFrame[\"Comments\"] = dataFrame[\"Comments\"].apply(lambda x: clean_comments(x))\n",
    "\n",
    "        #Subset JIRA ID, Summary, Description, comments\n",
    "        jira_issues_subset = dataFrame[[\"Issue key\", \"Assignee\", \"Summary\", \"Description\", \"Comments\", \"Created\", \"Resolved\", \"Updated\"]]\n",
    "        cleanedAndProcessedJiraData = preprocessJiraData(jira_issues_subset, preprocessComments = True, porterStemmer = porterStemmer, cachedStopWords = cachedStopWords, startTime = startTime)\n",
    "        return(cleanedAndProcessedJiraData)\n",
    "    else: \n",
    "        jira_issues_subset = dataFrame[[\"Issue key\", \"Assignee\", \"Summary\", \"Description\", \"Created\", \"Resolved\", \"Updated\"]]\n",
    "        cleanedAndProcessedJiraData = preprocessJiraData(jira_issues_subset, preprocessComments = False, porterStemmer = porterStemmer, cachedStopWords = cachedStopWords, startTime = startTime)\n",
    "        return(cleanedAndProcessedJiraData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename key to Issue key\n",
    "rawData_JIRA_academyMay = rawData_JIRA_academyMay.rename({'Key': 'Issue key'}, axis=1)\n",
    "\n",
    "#Clean Data sets\n",
    "intermediateData_JIRA_dataProcessing = cleanJiraData(dataFrame = rawData_JIRA_dataProcessing, cleanComments = True, commentAmount = 39)\n",
    "intermediateData_JIRA_academyMay = cleanJiraData(dataFrame = rawData_JIRA_academyMay, cleanComments = False, commentAmount = 0)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "intermediateData_JIRA_dataProcessing.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_JIRA_dataProcessing.xlsx\", index = False)\n",
    "intermediateData_JIRA_academyMay.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_JIRA_academyMay.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "intermediateData_JIRA_dataProcessing.to_pickle(path= \"../data/02_intermediate/intermediateData_JIRA_dataProcessing.pkl\")\n",
    "intermediateData_JIRA_academyMay.to_pickle(path= \"../data/02_intermediate/intermediateData_JIRA_academyMay.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-bankruptcy",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create JIRA Corpora\n",
    "Create the corpora for JIRA UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpusFromDocumentList(token_column):\n",
    "    token_list = token_column.tolist()\n",
    "    corpus_list = []\n",
    "    \n",
    "    for document in token_list:\n",
    "        #Only join to the string when a list. When it is not a list, then it is np.NaN, thus no changes\n",
    "        if(isinstance(document, list)):\n",
    "            #Transform list to a string for SKLEARN to accept the input.\n",
    "            token_string = ' '.join(document)\n",
    "        \n",
    "            #Add string to the corpus list\n",
    "            corpus_list.append(token_string)\n",
    "    return(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "intermediateData_JIRA_dataProcessingCorpusSummary = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Summary)\n",
    "intermediateData_JIRA_dataProcessingCorpusDescription = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Description)\n",
    "intermediateData_JIRA_dataProcessingCorpusComments = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Comments)\n",
    "\n",
    "#Create JIRA corpus for academy dataset\n",
    "intermediateData_JIRA_academyMayCorpusSummary = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Summary)\n",
    "intermediateData_JIRA_academyMayCorpusDescription = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "intermediateData_JIRA_dataProcessingCorpus = [i+\" \"+j for i,j in zip(intermediateData_JIRA_dataProcessingCorpusSummary,\n",
    "                                                                             intermediateData_JIRA_dataProcessingCorpusDescription,\n",
    "                                                                             #intermediateData_JIRA_dataProcessingCorpusComments\n",
    "                                                                            )]\n",
    "\n",
    "intermediateData_JIRA_academyMayCorpus = [i+\" \"+j for i,j in zip(intermediateData_JIRA_academyMayCorpusSummary,\n",
    "                                                                 intermediateData_JIRA_academyMayCorpusDescription)]\n",
    "\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/intermediateData_JIRA_dataProcessingCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_JIRA_dataProcessingCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_JIRA_academyMayCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_JIRA_academyMayCorpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-tanzania",
   "metadata": {},
   "source": [
    "Bigram corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpusNGrams(tokenColumn):\n",
    "    tokenList = tokenColumn.tolist()\n",
    "    corpusList = []\n",
    "    \n",
    "    #Transform to strings\n",
    "    for document in tokenList:\n",
    "        if(isinstance(document, list)):\n",
    "            for ngram in document:\n",
    "                ngramString = ' '.join(ngram)\n",
    "                corpusList.append(ngramString)         \n",
    "    return(corpusList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "intermediateData_JIRA_dataProcessingCorpusSummary_2grams = createCorpusNGrams(intermediateData_JIRA_dataProcessing.Summary_2grams)\n",
    "intermediateData_JIRA_dataProcessingCorpusDescription_2grams = createCorpusNGrams(intermediateData_JIRA_dataProcessing.Description_2grams)\n",
    "intermediateData_JIRA_dataProcessingCorpusComments_2grams = createCorpusNGrams(intermediateData_JIRA_dataProcessing.Comments_2grams)\n",
    "\n",
    "#Create JIRA corpus for academy dataset\n",
    "#intermediateData_JIRA_academyMayCorpusSummary = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Summary)\n",
    "#intermediateData_JIRA_academyMayCorpusDescription = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "intermediateData_JIRA_dataProcessingCorpus_2gram = [i+\" \"+j for i,j in zip(intermediateData_JIRA_dataProcessingCorpusSummary_2grams,\n",
    "                                                                             intermediateData_JIRA_dataProcessingCorpusDescription_2grams,\n",
    "                                                                             #intermediateData_JIRA_dataProcessingCorpusComments_2grams\n",
    "                                                                                  )]\n",
    "\n",
    "#intermediateData_JIRA_academyMayCorpus = [i+\" \"+j for i,j in zip(intermediateData_JIRA_academyMayCorpusSummary,\n",
    "#                                                                 intermediateData_JIRA_academyMayCorpusDescription)]\n",
    "\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/intermediateData_JIRA_dataProcessingCorpus_2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_JIRA_dataProcessingCorpus_2gram, f)\n",
    "#with open('../data/02_intermediate/intermediateData_JIRA_academyMayCorpus.pkl', 'wb') as f:\n",
    "#    pickle.dump(intermediateData_JIRA_academyMayCorpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-syntax",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create SVN Corpora\n",
    "Create the corpora for SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediateData_SVN_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corpus for log messages\n",
    "intermediateData_SVNLogs_dataProcessingCorpus = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Logs)\n",
    "intermediateData_SVNLogs_academyCorpus = createCorpusFromDocumentList(intermediateData_SVN_academy.Logs)\n",
    "intermediateData_SVNLogs_academyCorpus = createCorpusFromDocumentList(intermediateData_SVN_academy.Logs)\n",
    "\n",
    "#Create corpus for unit names\n",
    "intermediateData_SVNUnitNames_dataProcessingCorpus = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Unit_names)\n",
    "intermediateData_SVNUnitNames_academyCorpus = createCorpusFromDocumentList(intermediateData_SVN_academy.Unit_names)\n",
    "\n",
    "#Create corpus for entire commit (log message + model)\n",
    "intermediateData_SVN_dataProcessingCorpusAll = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Logs + intermediateData_SVN_dataProcessing.Unit_names)\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/intermediateData_SVNLogs_dataProcessingCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNLogs_dataProcessingCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVNLogs_academyCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNLogs_academyCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVNUnitNames_dataProcessingCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNUnitNames_dataProcessingCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVNUnitNames_academyCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNUnitNames_academyCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusAll.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_dataProcessingCorpusAll, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-tactics",
   "metadata": {},
   "source": [
    "bigram corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediateData_SVNLogs_dataProcessingCorpus_2gram = createCorpusNGrams(intermediateData_SVN_dataProcessing.Logs_2grams)\n",
    "intermediateData_SVNUnitNames_dataProcessingCorpus_2gram = createCorpusNGrams(intermediateData_SVN_dataProcessing.Unit_names_2grams)\n",
    "with open('../data/02_intermediate/intermediateData_SVNLogs_dataProcessingCorpus_2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNLogs_dataProcessingCorpus_2gram, f)\n",
    "    \n",
    "    \n",
    "with open('../data/02_intermediate/intermediateData_SVNUnitNames_dataProcessingCorpus_2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVNUnitNames_dataProcessingCorpus_2gram, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-collaboration",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained results.\n",
    "intermediateData_JIRA_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_JIRA_dataProcessing.pkl\")\n",
    "intermediateData_JIRA_academyMay = pd.read_pickle(\"../data/02_intermediate/intermediateData_JIRA_academyMay.pkl\")\n",
    "\n",
    "intermediateData_SVN_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")\n",
    "intermediateData_SVN_academy = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_academy.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "intermediateData_JIRA_dataProcessingCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_JIRA_dataProcessingCorpus.pkl')\n",
    "intermediateData_JIRA_academyMayCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_JIRA_academyMayCorpus.pkl')\n",
    "\n",
    "intermediateData_SVN_dataProcessingCorpusAll = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusAll.pkl')\n",
    "intermediateData_SVN_dataProcessingCorpusModel = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusModel.pkl')\n",
    "intermediateData_SVN_dataProcessingCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpus.pkl')\n",
    "intermediateData_SVN_academyCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_academyCorpus.pkl')\n",
    "\n",
    "############# Bigrams\n",
    "\n",
    "\n",
    "############# Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-mouse",
   "metadata": {},
   "source": [
    "## 3.0 Preprocess Data - Create cartesian product JIRA x Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "processedData_dataProcessingCartesian = intermediateData_JIRA_dataProcessing.merge(intermediateData_SVN_dataProcessing, how='cross')\n",
    "processedData_academyCartesian = intermediateData_JIRA_academyMay.merge(intermediateData_SVN_academy, how='cross')\n",
    "\n",
    "processedData_dataProcessingCartesian = processedData_dataProcessingCartesian.drop(processedData_dataProcessingCartesian[processedData_dataProcessingCartesian.Jira_created_date > processedData_dataProcessingCartesian.Commit_date].index)\n",
    "processedData_academyCartesian = processedData_academyCartesian.drop(processedData_academyCartesian[processedData_academyCartesian.Jira_created_date > processedData_academyCartesian.Commit_date].index)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingCartesian.to_pickle(path= \"../data/03_processed/processedData_dataProcessingCartesian.pkl\")\n",
    "processedData_academyCartesian.to_pickle(path= \"../data/03_processed/processedData_academyCartesian.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-texas",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess Data - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "processedData_dataProcessingLabels = pd.DataFrame() \n",
    "processedData_academyLabels = pd.DataFrame() \n",
    "\n",
    "#Create a column, which indicates which traces are valid.\n",
    "processedData_dataProcessingLabels[\"is_valid\"] = processedData_dataProcessingCartesian.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for dataProcessing\")\n",
    "processedData_academyLabels[\"is_valid\"] = processedData_academyCartesian.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for academy\")\n",
    "\n",
    "#Save intermediate results\n",
    "processedData_dataProcessingLabels.to_pickle(path= \"../data/03_processed/processedData_dataProcessingLabels.pkl\")\n",
    "processedData_academyLabels.to_pickle(path= \"../data/03_processed/processedData_academyLabels.pkl\")\n",
    "\n",
    "processedData_dataProcessingLabels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processedData_dataProcessingLabels[processedData_dataProcessingLabels.is_valid == True].count()\n",
    "processedData_dataProcessingLabels[processedData_dataProcessingLabels.is_valid == True].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-helicopter",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess Data - Create Time-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "processedData_dataProcessingFeaturesTime = pd.DataFrame() \n",
    "processedData_academyFeaturesTime = pd.DataFrame() \n",
    "\n",
    "#Calculate the time features for data Processing Dataset\n",
    "processedData_dataProcessingFeaturesTime['Creation_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "processedData_dataProcessingFeaturesTime['Updated_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "processedData_dataProcessingFeaturesTime['Resolved_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished data Processing\")\n",
    "\n",
    "\n",
    "#Calculate the time features for academy Dataset\n",
    "processedData_academyFeaturesTime['Creation_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "processedData_academyFeaturesTime['Updated_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "processedData_academyFeaturesTime['Resolved_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished academy\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingFeaturesTime.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesTime.pkl\")\n",
    "processedData_academyFeaturesTime.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesTime.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-exchange",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Stakeholder-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the Stakeholder features\n",
    "processedData_dataProcessingFeaturesStakeholder = pd.DataFrame() \n",
    "processedData_academyFeaturesStakeholder = pd.DataFrame() \n",
    "\n",
    "processedData_dataProcessingFeaturesStakeholder['Assignee_is_commiter'] = processedData_dataProcessingCartesian.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "print(\"Finished dataProcessing\")\n",
    "processedData_academyFeaturesStakeholder['Assignee_is_commiter'] = processedData_academyCartesian.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "print(\"Finished academy\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingFeaturesStakeholder.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesStakeholder.pkl\")\n",
    "processedData_academyFeaturesStakeholder.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesStakeholder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_dataProcessingFeaturesStakeholder.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-marijuana",
   "metadata": {},
   "source": [
    "## 3.4 Preprocess Data - Create Cosine Similarity Features\n",
    "### 3.4.1 DataProcessing - Cosine Similarity UniGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "\n",
    "def calc_vector_representation(document, cv, fittedTF_IDF):        \n",
    "    #Transform document type to a string\n",
    "    documentString = document\n",
    "    \n",
    "    #Calculate the Term Frequency of the document\n",
    "    inputDocs = [documentString] \n",
    "\n",
    "    # count matrix \n",
    "    count_vector = cv.transform(inputDocs) \n",
    " \n",
    "    #tf-idf scores \n",
    "    tf_idf_vector = fittedTF_IDF.transform(count_vector)\n",
    "\n",
    "    feature_names = cv.get_feature_names() \n",
    " \n",
    "    #get tfidf vector for first document \n",
    "    document_vector=tf_idf_vector[0] \n",
    " \n",
    "    #print the scores \n",
    "    \n",
    "    # place tf-idf values in a pandas data frame \n",
    "    df = pd.DataFrame(document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "    return(document_vector.T.todense())\n",
    "\n",
    "def calculateCosineSimilarity(document1, document2, cv, fittedTF_IDF):\n",
    "\n",
    "    #If both doc1 and doc2 are lists\n",
    "    if (isinstance(document1, list) & isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ' '.join(document2)\n",
    "\n",
    "    #Only document1 is a list\n",
    "    elif(isinstance(document1, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ''\n",
    "\n",
    "    #Only document2 is a list\n",
    "    elif(isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ''\n",
    "        document2String = ' '.join(document2)\n",
    "        \n",
    "    else:\n",
    "        document1String = ''\n",
    "        document2String = ''\n",
    "\n",
    "    vector1 = calc_vector_representation(document1String, cv, fittedTF_IDF)\n",
    "    vector2 = calc_vector_representation(document2String, cv, fittedTF_IDF)\n",
    "    \n",
    "    #The cosine similarity. Produces NaN if no terms are found in the corpus.\n",
    "    result = 1 - spatial.distance.cosine(vector1, vector2)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def calculateCosineSimilarityNGrams(document1, document2, cv, fittedTF_IDF):\n",
    "\n",
    "    #If both doc1 and doc2 are lists\n",
    "    if (isinstance(document1, list) & isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ' '.join(document2)\n",
    "\n",
    "    #Only document1 is a list\n",
    "    elif(isinstance(document1, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ''\n",
    "\n",
    "    #Only document2 is a list\n",
    "    elif(isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ''\n",
    "        document2String = ' '.join(document2)\n",
    "        \n",
    "    else:\n",
    "        document1String = ''\n",
    "        document2String = ''\n",
    "\n",
    "    vector1 = calc_vector_representation(document1String, cv, fittedTF_IDF)\n",
    "    vector2 = calc_vector_representation(document2String, cv, fittedTF_IDF)\n",
    "    \n",
    "    #The cosine similarity. Produces NaN if no terms are found in the corpus.\n",
    "    result = 1 - spatial.distance.cosine(vector1, vector2)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "\n",
    "def calculateCosineSimilarityWithPOSPruning(document1, document2, cv, fittedTF_IDF, verbList):\n",
    "\n",
    "    #If both doc1 and doc2 are lists\n",
    "    if (isinstance(document1, list) & isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ' '.join(document2)\n",
    "\n",
    "    #Only document1 is a list\n",
    "    elif(isinstance(document1, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ' '.join(document1)\n",
    "        document2String = ''\n",
    "\n",
    "    #Only document2 is a list\n",
    "    elif(isinstance(document2, list)):\n",
    "        #Transform document to string type\n",
    "        document1String = ''\n",
    "        document2String = ' '.join(document2)\n",
    "        \n",
    "    else:\n",
    "        document1String = ''\n",
    "        document2String = ''\n",
    "\n",
    "    vector1 = calc_vector_representation(document1String, cv, fittedTF_IDF)\n",
    "    vector2 = calc_vector_representation(document2String, cv, fittedTF_IDF)\n",
    "    \n",
    "    #The cosine similarity. Produces NaN if no terms are found in the corpus.\n",
    "    result = 1 - spatial.distance.cosine(vector1, vector2)\n",
    "    \n",
    "    verbCounter = 0\n",
    "    if(isinstance(document2, list)):\n",
    "        for token in document2:\n",
    "            if token in verbList:\n",
    "                verbCounter = verbCounter + 1\n",
    "    \n",
    "    if verbCounter > 0:\n",
    "        result = result * (1 + (0.1 * verbCounter))\n",
    "    else:\n",
    "        result = 0\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "######################################################\n",
    "#                       dataProcessing               #\n",
    "######################################################\n",
    "\n",
    "################# Unigrams ###############\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessingCorpus)\n",
    "\n",
    "processedData_SVNLogs_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVNLogs_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_SVNLogs_dataProcessingCountVectorizer, intermediateData_SVNLogs_dataProcessingCorpus)\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVNUnitNames_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_SVNUnitNames_dataProcessingCountVectorizer, intermediateData_SVNUnitNames_dataProcessingCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA - unigram\n",
    "processedData_JIRA_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpus)\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRASummaries_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_JIRASummaries_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpusSummary)\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRADescriptions_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_JIRADescriptions_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpusDescription)\n",
    "\n",
    "processedData_JIRAComments_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRAComments_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_JIRAComments_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpusComments)\n",
    "\n",
    "\n",
    "################# Bigrams ###############\n",
    "#instantiate CountVectorizer() for SVN - bigrams\n",
    "processedData_SVNLogs_dataProcessingCountVectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "processedData_SVNLogs_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_SVNLogs_dataProcessingCountVectorizer_2gram, intermediateData_SVNLogs_dataProcessingCorpus_2gram)\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingCountVectorizer_2gram = CountVectorizer()\n",
    "processedData_SVNUnitNames_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_SVNUnitNames_dataProcessingCountVectorizer_2gram, intermediateData_SVNUnitNames_dataProcessingCorpus_2gram)\n",
    "\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA - biigram\n",
    "processedData_JIRA_dataProcessingCountVectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "processedData_JIRA_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_JIRA_dataProcessingCountVectorizer_2gram, intermediateData_JIRA_dataProcessingCorpus_2gram)\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingCountVectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "processedData_JIRASummaries_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_JIRASummaries_dataProcessingCountVectorizer_2gram, intermediateData_JIRA_dataProcessingCorpusSummary_2grams)\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingCountVectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "processedData_JIRADescriptions_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_JIRADescriptions_dataProcessingCountVectorizer_2gram, intermediateData_JIRA_dataProcessingCorpusDescription_2grams)\n",
    "\n",
    "processedData_JIRAComments_dataProcessingCountVectorizer_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "processedData_JIRAComments_dataProcessingCountTF_IDF_2gram = createFittedTF_IDF(processedData_JIRAComments_dataProcessingCountVectorizer_2gram, intermediateData_JIRA_dataProcessingCorpusComments_2grams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "#                       academy               #\n",
    "######################################################\n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_academyCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_academyCountTF_IDF = createFittedTF_IDF(processedData_SVN_academyCountVectorizer, intermediateData_SVN_academyCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA\n",
    "processedData_JIRA_academyCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_academyCountTF_IDF = createFittedTF_IDF(processedData_JIRA_academyCountVectorizer, intermediateData_JIRA_academyMayCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for Model\n",
    "#processedData_Model_academyCountVectorizer = CountVectorizer()\n",
    "#processedData_Model_academyCountTF_IDF = createFittedTF_IDF(processedData_Model_academyCountVectorizer, intermediateData_SVN_academyCorpusModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-audience",
   "metadata": {},
   "source": [
    "#### 3.4.1 [VSM unigram] Similarity between JIRA issue and Commit Log - Jira As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery[\"vsm_logs_jira_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-september",
   "metadata": {},
   "source": [
    "#### 3.4.2 [VSM unigram] Similarity between JIRA issue and Commit Log - Log As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery[\"vsm_logs_log_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, processedData_SVNLogs_dataProcessingCountVectorizer, processedData_SVNLogs_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-basic",
   "metadata": {},
   "source": [
    "#### 3.4.3 [VSM unigram] Similarity between JIRA issue and Unit Names - JIRA As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery[\"vsm_unit_names_jira_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-drink",
   "metadata": {},
   "source": [
    "#### 3.4.1 [VSM unigram] Similarity between JIRA Summary and Commit Log - Jira As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery[\"vsm_summary_logs_summary_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_JIRASummaries_dataProcessingCountVectorizer, processedData_JIRASummaries_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-victor",
   "metadata": {},
   "source": [
    "#### 3.4.1 [VSM unigram] Similarity between JIRA Summary and Commit Log - Log As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery[\"vsm_summary_logs_logs_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_SVNLogs_dataProcessingCountVectorizer, processedData_SVNLogs_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-receiver",
   "metadata": {},
   "source": [
    "#### 3.4.1 [VSM unigram] Similarity between JIRA Summary and UnitNames - Summary As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery[\"vsm_summary_unitNames_summary_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, processedData_JIRASummaries_dataProcessingCountVectorizer, processedData_JIRASummaries_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-guinea",
   "metadata": {},
   "source": [
    "#### 3.4.1 [VSM unigram] Similarity between JIRA Summary and UnitNames - UnitNames As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery[\"vsm_summary_unitNames_unitNames_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-greek",
   "metadata": {},
   "source": [
    "#### 3.4.3 [VSM unigram - verb pruning] Similarity between JIRA issue and Unit Names - JIRA As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery[\"vsm_verb_pruning_unit_names_jira_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarityWithPOSPruning(x.Jira_natural_text, x.Unit_names, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF, x.verbs), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query and verb pruning' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-jungle",
   "metadata": {},
   "source": [
    "#### 3.4.4 [VSM unigram] Similarity between JIRA issue and Unit Names  - Unit Names As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery[\"vsm_unit_names_log_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-hearts",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram] Similarity between JIRA description and commit log - Description as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery[\"vsm_description_description_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_JIRADescriptions_dataProcessingCountVectorizer, processedData_JIRADescriptions_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-trash",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram Silarity between JIRA description and commit log - Log as descrintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery[\"vsm_description_log_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmDescriptionLogsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-fraud",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram Silarity between JIRA Comment and unitnames - Comment as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentsUnitNamesCommentsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentsUnitNamesCommentsAsQuery[\"vsm_comments_unitnames_comments_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Unit_names, processedData_JIRAComments_dataProcessingCountVectorizer, processedData_JIRAComments_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentsUnitNamesCommentsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentsUnitNamesCommentsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-beginning",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram Silarity between JIRA Comment and unitnames - Comment as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentUnitNamesUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentUnitNamesUnitNamesAsQuery[\"vsm_comments_unitnames_unitnames_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentUnitNamesUnitNamesAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentUnitNamesUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-accused",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram Silarity between JIRA Comment and commit log - Comment as description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery[\"vsm_comments_comments_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_JIRAComments_dataProcessingCountVectorizer, processedData_JIRAComments_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentsCommentsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-medicaid",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM unigram Silarity between JIRA description and commit log - Log as description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery[\"vsm_comments_log_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_SVNLogs_dataProcessingCountVectorizer, processedData_SVNLogs_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentsLogsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-romantic",
   "metadata": {},
   "source": [
    "#### [VSM bigram] Similarity between JIRA comments and Commit Logs - Logs as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram[\"vsm_comments_log_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_SVNUnitNames_dataProcessingCountVectorizer_2gram, processedData_SVNUnitNames_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-festival",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM bigram] Silarity between JIRA Comment and commit log - Comment as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram[\"vsm_comments_comments_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_JIRAComments_dataProcessingCountVectorizer_2gram, processedData_JIRAComments_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-fairy",
   "metadata": {},
   "source": [
    "#### [VSM Unigram] Similarity between Unit Names and Description - Unit Names as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery[\"vsm_unitnames_description_unitnames_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-patio",
   "metadata": {},
   "source": [
    "#### [VSM Unigram] Similarity between Unit Names and Description - Description as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery[\"vsm_unitnames_description_description_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_JIRADescriptions_dataProcessingCountVectorizer, processedData_JIRADescriptions_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-schedule",
   "metadata": {},
   "source": [
    "#### [VSM Unigram] Similarity between Unit Names and Comments - Unit Names as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery[\"vsm_unitnames_comments_unitnames_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-council",
   "metadata": {},
   "source": [
    "#### [VSM Unigram] Similarity between Unit Names and Comments - Comments as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery[\"vsm_unitnames_comments_comments_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Unit_names, processedData_JIRAComments_dataProcessingCountVectorizer, processedData_JIRAComments_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and JIRA (entirely)- JIRA as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery[\"vsm_svn_jira_jira_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and JIRA (entirely) - SVN as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery[\"vsm_svn_jira_svn_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Summary - SVN as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery[\"vsm_svn_summary_svn_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Summary - Summary as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery[\"vsm_svn_summary_summary_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, processedData_JIRASummaries_dataProcessingCountVectorizer, processedData_JIRASummaries_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Description - SVN as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery[\"vsm_svn_description_svn_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Description - Description as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery[\"vsm_svn_description_description_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, processedData_JIRADescriptions_dataProcessingCountVectorizer, processedData_JIRADescriptions_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Comments - SVN as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery[\"vsm_svn_comments_svn_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Comments, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [VSM Unigram] Similarity between SVN (entirely) and Comments - Comments as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery[\"vsm_svn_comments_comments_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Comments, processedData_JIRAComments_dataProcessingCountVectorizer, processedData_JIRAComments_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-absorption",
   "metadata": {},
   "source": [
    "#### 3.4.3 [VSM unigram - verb pruning] Similarity between JIRA issue and Unit Names and verb pruning - Unit Names As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery[\"vsm_verb_pruning_unit_names_log_as_query\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarityWithPOSPruning(x.Jira_natural_text, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer, processedData_SVNUnitNames_dataProcessingCountTF_IDF, x.verbs), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-freeze",
   "metadata": {},
   "source": [
    "#### 3.4.5 [VSM bigram] Similarity between JIRA issue and Commit Log - Jira As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram[\"vsm_logs_jira_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, processedData_JIRA_dataProcessingCountVectorizer_2gram, processedData_JIRA_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-northeast",
   "metadata": {},
   "source": [
    "#### 3.4.6 [VSM bigram] Similarity between JIRA issue and Commit Log - Logs As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram[\"vsm_logs_log_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, processedData_SVNLogs_dataProcessingCountVectorizer_2gram, processedData_SVNLogs_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-indie",
   "metadata": {},
   "source": [
    "#### 3.4.6 [VSM bigram] Similarity between JIRA issue and Unit Names - Jira As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram[\"vsm_unit_names_jira_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, processedData_JIRA_dataProcessingCountVectorizer_2gram, processedData_JIRA_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-sixth",
   "metadata": {},
   "source": [
    "#### 3.4.6 [VSM bigram] Similarity between JIRA issue and Unit Names - UnitNames As Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram[\"vsm_unit_names_log_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer_2gram, processedData_SVNUnitNames_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-sector",
   "metadata": {},
   "source": [
    "#### [VSM bigram] Similarity between Logs and Description - Logs as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_2gram[\"vsm_description_log_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_SVNUnitNames_dataProcessingCountVectorizer_2gram, processedData_SVNUnitNames_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM UnitNames Unit Names as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-prince",
   "metadata": {},
   "source": [
    "#### [VSM bigram] Similarity between Logs and Description - Description as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_2gram[\"vsm_description_description_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_JIRADescriptions_dataProcessingCountVectorizer_2gram, processedData_JIRADescriptions_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Bigrams' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-simon",
   "metadata": {},
   "source": [
    "#### [VSM bigram] Similarity between Logs and Summary - Logs as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-throat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "continent-african",
   "metadata": {},
   "source": [
    "#### [VSM bigram] Similarity between Logs and Summary - Summary as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_2gram = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_2gram[\"vsm_summary_logs_summary_as_query_2gram\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarityNGrams(x.Summary, x.Logs, processedData_JIRASummaries_dataProcessingCountVectorizer_2gram, processedData_JIRASummaries_dataProcessingCountTF_IDF_2gram), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_2gram.to_pickle(path= \"../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_2gram.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM Logs Jira as query' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-separation",
   "metadata": {},
   "source": [
    "## 3.6 Document Statistics\n",
    "\n",
    "### dataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount = pd.DataFrame() \n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount = pd.DataFrame()\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount[\"unique_term_count_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateUniqueWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount[\"unique_term_count_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateUniqueWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount[\"total_term_count_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateTotalWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount[\"total_term_count_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateTotalWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list1'),\n",
    "                                                            axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list2'),\n",
    "                                                            axis=1)\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_union\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'union'),\n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_UNION_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating document statistics in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-pressing",
   "metadata": {},
   "source": [
    "## 3.7 Query Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from statistics import mean, median, mode, stdev, variance\n",
    "from math import log, sqrt\n",
    "import itertools\n",
    "\n",
    "#Function calculating the IDFs of all query terms. Returns a list containing all IDFs\n",
    "def calcIDFList(document, cv, tfidf_transformer):\n",
    "    idfScoreList=[]\n",
    "    if isinstance(document, list):\n",
    "        termCount = len(document)\n",
    "        for term in document:\n",
    "            try:\n",
    "                indexOfWord = cv.get_feature_names().index(term)\n",
    "                idfScore = tfidf_transformer.idf_[indexOfWord]\n",
    "                idfScoreList.append(idfScore)\n",
    "            except:\n",
    "                idfScoreList.append(0)\n",
    "    else:\n",
    "        termCount = 0\n",
    "    return(idfScoreList)\n",
    "\n",
    "\n",
    "def calcAvgIDF(IDFList):\n",
    "    termCount = len(IDFList)\n",
    "    if(termCount != 0):\n",
    "        avgIdf = sum(IDFList) / termCount\n",
    "    else:\n",
    "        avgIdf = 0\n",
    "    return(avgIdf)\n",
    "\n",
    "def calcMaxIDF(IDFList): \n",
    "    termCount = len(IDFList)\n",
    "    if(termCount != 0):\n",
    "        maxIdf = np.amax(IDFList)\n",
    "    else: \n",
    "        maxIdf = 0\n",
    "    return(maxIdf)\n",
    "\n",
    "def calcDevIDF(IDFList):\n",
    "    termCount = len(IDFList)\n",
    "    if(termCount > 1):\n",
    "        stdevIdf = stdev(IDFList)\n",
    "    else: \n",
    "        stdevIdf = 0\n",
    "    return(stdevIdf)\n",
    "\n",
    "#Function calculating the ICTF of all query terms. Returns a list containing all IDFs\n",
    "def calcICTFList(document, cv, documentCount):\n",
    "    ICTFList = []\n",
    "        #For all terms in query, find how often they occur in the Corpus\n",
    "    if isinstance(document, list):\n",
    "        for term in document:\n",
    "            try:\n",
    "            #Find out how often the term occurs in the corpus\n",
    "                termFrequency = (cv.vocabulary_[term])\n",
    "                \n",
    "                #Compute the log\n",
    "                ictF = log(documentCount/termFrequency)\n",
    "            except:\n",
    "                ictF = 0\n",
    "            \n",
    "            ICTFList.append(ictF)\n",
    "    return(ICTFList)\n",
    "\n",
    "def calcAvgICTF(ICTFList, documentCount):\n",
    "    avgICTF = sum(ICTFList) / documentCount\n",
    "    return(avgICTF)\n",
    "\n",
    "\n",
    "def calcMaxICTF(ICTFList): \n",
    "    termCount = len(ICTFList)\n",
    "    if(termCount != 0):\n",
    "        maxICTF = np.amax(ICTFList)\n",
    "    else: \n",
    "        maxICTF = 0\n",
    "    return(maxICTF)\n",
    "\n",
    "def calcDevICTF(ICTFList):\n",
    "    termCount = len(ICTFList)\n",
    "    if(termCount > 1):\n",
    "        stdevICTF = stdev(ICTFList)\n",
    "    else: \n",
    "        stdevICTF = 0\n",
    "    return(stdevICTF)\n",
    "\n",
    "\n",
    "def calcEntropyList(query, cv, documentCount, docCollection):\n",
    "    #entropy(t) =  (dDt)  ( tf(t,d) / tf(t, D) ) * log |D|(tf(t,d) / tf(t, D) )\n",
    "        \n",
    "    entropyValueList = []\n",
    "    #for each term in the query, calculate the entropy of the query\n",
    "    if isinstance(query, list):\n",
    "        for queryTerm in query:\n",
    "            #For each d  D\n",
    "            \n",
    "            partialEntropyList = []\n",
    "            \n",
    "            for d in docCollection:\n",
    "                #Check if queryTerm occurs in D (i.e/ dDt)\n",
    "                if (isinstance(d, list)):\n",
    "                    if queryTerm in d:\n",
    "                        try:\n",
    "                            #Calculate the frequency of the term occurs in the document (i.e tf(t,d))\n",
    "                            queryTermFrequencyInDocument = d.count(queryTerm)\n",
    "                            \n",
    "                            #calculate the frequency the term occurs in the query corpus (i.e tf(t,D))\n",
    "                            queryTermFrequencyInCorpus = (cv.vocabulary_[queryTerm])\n",
    "                             \n",
    "                            # This part of the calculation tf(t,d) / tf(t, D)  * log |D|(tf(t,d) / tf(t, D))\n",
    "                            partialEntropy1stHalf = queryTermFrequencyInDocument / queryTermFrequencyInCorpus\n",
    "                            partialEntropy2ndHalf = log((queryTermFrequencyInDocument / queryTermFrequencyInCorpus), documentCount)\n",
    "                            partialEntropy = partialEntropy1stHalf\n",
    "                            partialEntropyList.append(partialEntropy)\n",
    "                        except:\n",
    "                            partialEntropyList.append(0) #If term not found entropy is 0\n",
    "            #this part of the calculation  (dDt)\n",
    "            entropyValueOfQueryTerm = sum(partialEntropyList)\n",
    "            entropyValueList.append(entropyValueOfQueryTerm)\n",
    "    \n",
    "    return(entropyValueList)\n",
    "\n",
    "\n",
    "def calcAvgEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        #Calculate the average of all the entropies\n",
    "        avgEntropy = sum(entropyValueList) / len(entropyValueList)\n",
    "    else:\n",
    "        avgEntropy = 0\n",
    "    return(avgEntropy)\n",
    "\n",
    "    \n",
    "def calcMedEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        #Calculate the average of all the entropies\n",
    "        medEntropy = median(entropyValueList)\n",
    "    else:\n",
    "        medEntropy = 0\n",
    "    return(medEntropy)\n",
    "    \n",
    "def calcMaxEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        maxEntropy = np.amax(entropyValueList)\n",
    "    else: \n",
    "        maxEntropy = 0\n",
    "    return(maxEntropy)\n",
    "    \n",
    "def calcDevEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount > 1):\n",
    "        #Calculate the average of all the entropies\n",
    "        devEntropy = stdev(entropyValueList)\n",
    "    else:\n",
    "        devEntropy = 0\n",
    "    return(devEntropy)\n",
    "\n",
    "#The percentage of documents in the collection containing at least one of the query terms\n",
    "def calcQueryScope(query, docCollection): \n",
    "    counter = 0\n",
    "    if isinstance(query, list):\n",
    "        for document in docCollection:\n",
    "            #check if query occurs in term. \n",
    "            if(isinstance(document, list)):\n",
    "                for queryTerm in query:\n",
    "                    if queryTerm in document:\n",
    "                        counter = counter + 1\n",
    "                        break\n",
    "    queryScope = counter / len(docCollection)\n",
    "    return(queryScope)\n",
    "\n",
    "#The Kullback-Leiber divergence of the query language model from the collection language model\n",
    "def calcSCS(query, cv, docCount):\n",
    "    divergenceList = []\n",
    "    if isinstance(query, list):\n",
    "        for queryTerm in query:\n",
    "            try:\n",
    "                #frequency of term in query - tf(q, Q)/|Q|\n",
    "                pqQ = query.count(queryTerm) / len(query)\n",
    "                \n",
    "                #frequency of term in documentlist - tf(q, D)/|D|\n",
    "                pqD = cv.vocabulary_[queryTerm]\n",
    "                \n",
    "                divergence = pqQ * log(pqQ / pqD)\n",
    "                divergenceList.append(divergence)\n",
    "            except:\n",
    "                continue\n",
    "    SCS = sum(divergenceList)\n",
    "    return(SCS)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcSCQList(query, docCollection, cv, fittedTF_IDF, documentCount):\n",
    "    SCQList = []\n",
    "    if isinstance(query, list):\n",
    "        documentString = ' '.join(query)\n",
    "        \n",
    "        #Calculate the Term Frequency of the document\n",
    "        inputDocs = [documentString] \n",
    "        \n",
    "        # count matrix \n",
    "        count_vector = cv.transform(inputDocs) \n",
    " \n",
    "        #tf-idf scores \n",
    "        tf_idf_vector = fittedTF_IDF.transform(count_vector)\n",
    "        \n",
    "        feature_names = cv.get_feature_names() \n",
    "        # place tf-idf values in a pandas data frame \n",
    "        df = pd.DataFrame(tf_idf_vector.T.todense(), \n",
    "                          index=feature_names, columns=[\"tfidf\"])\n",
    "    \n",
    "        \n",
    "        #Find the tfidf of the term\n",
    "        for queryTerm in query:    \n",
    "            try:\n",
    "                tfidf = df[\"tfidf\"][queryTerm]\n",
    "                SCQ = (1 + log(tfidf))\n",
    "                SCQList.append(SCQ)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    avgSCQ = sum(SCQList) / documentCount\n",
    "    return(SCQList)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcAvgSCQ(SCQList, documentCount):\n",
    "    avgSCQ = sum(SCQList) / documentCount\n",
    "    return(avgSCQ)\n",
    "    \n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcMaxSCQ(SCQList):\n",
    "    termCount = len(SCQList)\n",
    "    if(termCount != 0):\n",
    "        maxSCQ = np.amax(SCQList)\n",
    "    else:\n",
    "        maxSCQ = np.NaN\n",
    "    return(maxSCQ)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcSumSCQ(SCQList):\n",
    "    sumSCQ = sum(SCQList)\n",
    "    return(sumSCQ)\n",
    "\n",
    "def createTermPairs(cv):\n",
    "    terms = list(cv.vocabulary_.keys())\n",
    "    #Create all possible pair combinations from the terms in the query \n",
    "    pairCombinationList = list(itertools.combinations(terms, 2))\n",
    "    return(pairCombinationList)\n",
    "\n",
    "#Method to find out how often a term occurs in a document\n",
    "def findTermFrequencies(cv, docCollection):\n",
    "    terms = list(cv.vocabulary_.keys())\n",
    "    termFrequencies = {}\n",
    "    for term in terms:\n",
    "        termCounter = 0\n",
    "        for document in docCollection:\n",
    "            if isinstance(document, list):\n",
    "                if term in document: \n",
    "                    termCounter = termCounter + 1\n",
    "        termFrequencies[term] = termCounter\n",
    "    return(termFrequencies)\n",
    "\n",
    "#Method to find out how often both terms occur in a document. \n",
    "def findTermPairFrequencies(termPairs, docCollection):\n",
    "    termPairFrequencies = {}\n",
    "    for termPair in termPairs:\n",
    "        termPairCount = 0\n",
    "        for document in docCollection:\n",
    "            if (isinstance(document, list)):\n",
    "                if all(i in document for i in termPair):\n",
    "                    termPairCount = termPairCount + 1\n",
    "        termPairFrequencies[termPair] = termPairCount\n",
    "    return(termPairFrequencies)   \n",
    "\n",
    "def calcPMIList(query, termFrequencies, termPairFrequencies, docCollection):\n",
    "    if isinstance(query, list):\n",
    "    #Find the frequencies of the individual terms and the pairs\n",
    "        pairCombinationList = list(itertools.combinations(query, 2))\n",
    "        termOccurances = []\n",
    "        for pair in pairCombinationList:\n",
    "            try:\n",
    "                q1Freq = termFrequencies[pair[0]]\n",
    "            except:\n",
    "                q1Freq = 0\n",
    "            try:\n",
    "                q2Freq = termFrequencies[pair[1]]\n",
    "            except:\n",
    "                q2Freq = 0\n",
    "            try:\n",
    "                q1q2Freq = termPairFrequencies[pair]\n",
    "            except:\n",
    "                q1q2Freq = 0\n",
    "                    \n",
    "            termOccurances.append({'q1Freq': q1Freq, \n",
    "                                   'q2Freq': q2Freq, \n",
    "                                   'q1q2Freq': q1q2Freq})\n",
    "    \n",
    "        docCount = len(docCollection)\n",
    "        pmiList = []\n",
    "        for term in termOccurances:\n",
    "            pq1 = term['q1Freq'] / docCount\n",
    "            pq2 = term['q2Freq'] / docCount\n",
    "            pq1q2 = term['q1q2Freq'] / docCount\n",
    "\n",
    "            try:\n",
    "                pmi = log(pq1q2 /(pq1 * pq2))\n",
    "            except:\n",
    "                pmi = np.nan\n",
    "            pmiList.append(pmi)\n",
    "        return(pmiList)\n",
    "    else:\n",
    "        return(np.nan)\n",
    "\n",
    "def calcAvgPMI(pmiList):\n",
    "    if(isinstance(pmiList, list)):\n",
    "        pairCount = len(pmiList)\n",
    "        if(pairCount != 0):\n",
    "            #Calculate the average of all the entropies\n",
    "            avgPMI= np.nansum(pmiList) / pairCount\n",
    "        else:\n",
    "            avgPMI = 0\n",
    "        return(avgPMI)\n",
    "    return(np.nan)\n",
    "\n",
    "def calcMaxPMI(pmiList): \n",
    "    if(isinstance(pmiList, list)):\n",
    "        pairCount = len(pmiList)\n",
    "        if(pairCount != 0):\n",
    "            maxPMI = np.nanmax(pmiList)\n",
    "        else: \n",
    "            maxPMI = np.nan\n",
    "        return(maxPMI)\n",
    "    return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read datasets from disk\n",
    "processedData_dataProcessingCartesian = pd.read_pickle(r\"../data/03_processed/processedData_dataProcessingCartesian.pkl\")\n",
    "processedData_academyCartesian = pd.read_pickle(r\"../data/03_processed/processedData_academyCartesian.pkl\")\n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_dataProcessingTF_IDF = createFittedTF_IDF(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessingCorpusAll)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA\n",
    "processedData_JIRA_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_dataProcessingTF_IDF = createFittedTF_IDF(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpus)\n",
    "\n",
    "#Determine document counts\n",
    "intermediateData_JIRA_dataProcessing_documentCount = len(intermediateData_JIRA_dataProcessing.index)\n",
    "intermediateData_SVN_dataProcessing_documentCount = len(intermediateData_SVN_dataProcessing.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-stage",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_SVN_dataProcessingTF_IDF),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_avgIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_maxIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_devIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-moment",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF[\"SvnLogsAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Logs, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountTF_IDF),axis=1)\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF[\"SvnLogsAsQuery_avgIDF\"] = processedData_SVNLogs_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.SvnLogsAsQuery_IDF), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF[\"SvnLogsAsQuery_maxIDF\"] = processedData_SVNLogs_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.SvnLogsAsQuery_IDF), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF[\"SvnLogsAsQuery_devIDF\"] = processedData_SVNLogs_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.SvnLogsAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-sampling",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNUnitNames as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF[\"SvnUnitNamesAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Logs, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountTF_IDF),axis=1)\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF[\"SvnUnitNamesAsQuery_avgIDF\"] = processedData_SVNUnitNames_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.SvnUnitNamesAsQuery_IDF), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF[\"SvnUnitNamesAsQuery_maxIDF\"] = processedData_SVNUnitNames_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.SvnUnitNamesAsQuery_IDF), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF[\"SvnUnitNamesAsQuery_devIDF\"] = processedData_SVNUnitNames_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.SvnUnitNamesAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-arena",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_JIRA_dataProcessingTF_IDF),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_avgIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_maxIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_devIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-gothic",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF[\"JiraSummariesAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Summary, \n",
    "                                                                                                                processedData_JIRASummaries_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_JIRASummaries_dataProcessingCountTF_IDF),axis=1)\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF[\"JiraSummariesAsQuery_avgIDF\"] = processedData_JIRASummaries_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.JiraSummariesAsQuery_IDF), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF[\"JiraSummariesAsQuery_maxIDF\"] = processedData_JIRASummaries_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.JiraSummariesAsQuery_IDF), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF[\"JiraSummariesAsQuery_devIDF\"] = processedData_JIRASummaries_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.JiraSummariesAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-specialist",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF[\"JiraDescriptionsAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Description, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountTF_IDF),axis=1)\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF[\"JiraDescriptionsAsQuery_avgIDF\"] = processedData_JIRADescriptions_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.JiraDescriptionsAsQuery_IDF), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF[\"JiraDescriptionsAsQuery_maxIDF\"] = processedData_JIRADescriptions_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.JiraDescriptionsAsQuery_IDF), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF[\"JiraDescriptionsAsQuery_devIDF\"] = processedData_JIRADescriptions_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.JiraDescriptionsAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-chile",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Comments as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF[\"JiraCommentsAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Comments, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountTF_IDF),axis=1)\n",
    "\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF[\"JiraCommentsAsQuery_avgIDF\"] = processedData_JIRAComments_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.JiraCommentsAsQuery_IDF), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF[\"JiraCommentsAsQuery_maxIDF\"] = processedData_JIRAComments_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.JiraCommentsAsQuery_IDF), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF[\"JiraCommentsAsQuery_devIDF\"] = processedData_JIRAComments_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.JiraCommentsAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-ensemble",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_avgICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.SvnAsQuery_ICTF, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_maxICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.SvnAsQuery_ICTF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_devICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.SvnAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-anthropology",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Logs, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_avgICTF\"] = processedData_SVNLogs_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.SvnLogsAsQuery_ICTF, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_maxICTF\"] = processedData_SVNLogs_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.SvnLogsAsQuery_ICTF), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_devICTF\"] = processedData_SVNLogs_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.SvnLogsAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-bikini",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Unit_names, \n",
    "                                                                                                                processedData_SVNUnitNames_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_avgICTF\"] = processedData_SVNUnitNames_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.SvnUnitNamesAsQuery_ICTF, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_maxICTF\"] = processedData_SVNUnitNames_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.SvnUnitNamesAsQuery_ICTF), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_devICTF\"] = processedData_SVNUnitNames_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.SvnUnitNamesAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-japan",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_avgICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.JiraAsQuery_ICTF, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_maxICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.JiraAsQuery_ICTF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_devICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.JiraAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-freeware",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Summary, \n",
    "                                                                                                                processedData_JIRASummaries_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_avgICTF\"] = processedData_JIRASummaries_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.JiraSummariesAsQuery_ICTF, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_maxICTF\"] = processedData_JIRASummaries_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.JiraSummariesAsQuery_ICTF), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_devICTF\"] = processedData_JIRASummaries_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.JiraSummariesAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-offset",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Description, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_avgICTF\"] = processedData_JIRADescriptions_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.JiraDescriptionsAsQuery_ICTF, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_maxICTF\"] = processedData_JIRADescriptions_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.JiraDescriptionsAsQuery_ICTF), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_devICTF\"] = processedData_JIRADescriptions_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.JiraDescriptionsAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-ontario",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Comments as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Comments, \n",
    "                                                                                                                processedData_JIRAComments_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_avgICTF\"] = processedData_JIRAComments_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.JiraCommentsAsQuery_ICTF, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_maxICTF\"] = processedData_JIRAComments_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.JiraCommentsAsQuery_ICTF), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_devICTF\"] = processedData_JIRAComments_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.JiraCommentsAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-sheffield",
   "metadata": {},
   "source": [
    "#### Entropy (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_avgEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_medEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_maxEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_devEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-iraqi",
   "metadata": {},
   "source": [
    "#### Entropy (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Logs, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Logs),axis=1)\n",
    "##\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_avgEntropy\"] = processedData_SVNLogs_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.SvnLogsAsQuery_Entropy), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_medEntropy\"] = processedData_SVNLogs_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.SvnLogsAsQuery_Entropy), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_maxEntropy\"] = processedData_SVNLogs_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.SvnLogsAsQuery_Entropy), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_devEntropy\"] = processedData_SVNLogs_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.SvnLogsAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recorder",
   "metadata": {},
   "source": [
    "#### Entropy (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Unit_names, \n",
    "                                                                                                                processedData_SVNUnitNames_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Unit_names),axis=1)\n",
    "##\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_avgEntropy\"] = processedData_SVNUnitNames_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.SvnUnitNamesAsQuery_Entropy), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_medEntropy\"] = processedData_SVNUnitNames_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.SvnUnitNamesAsQuery_Entropy), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_maxEntropy\"] = processedData_SVNUnitNames_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.SvnUnitNamesAsQuery_Entropy), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_devEntropy\"] = processedData_SVNUnitNames_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.SvnUnitNamesAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-kingston",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "##\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_avgEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_medEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_maxEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_devEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-technical",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Summary, \n",
    "                                                                                                                processedData_JIRASummaries_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Summary),axis=1)\n",
    "##\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_avgEntropy\"] = processedData_JIRASummaries_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.JiraSummariesAsQuery_Entropy), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_medEntropy\"] = processedData_JIRASummaries_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.JiraSummariesAsQuery_Entropy), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_maxEntropy\"] = processedData_JIRASummaries_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.JiraSummariesAsQuery_Entropy), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_devEntropy\"] = processedData_JIRASummaries_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.JiraSummariesAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-wallace",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Description, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Description),axis=1)\n",
    "##\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_avgEntropy\"] = processedData_JIRADescriptions_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.JiraDescriptionsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_medEntropy\"] = processedData_JIRADescriptions_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.JiraDescriptionsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_maxEntropy\"] = processedData_JIRADescriptions_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.JiraDescriptionsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_devEntropy\"] = processedData_JIRADescriptions_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.JiraDescriptionsAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-affiliation",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Comments as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Comments, \n",
    "                                                                                                                processedData_JIRAComments_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Comments),axis=1)\n",
    "##\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_avgEntropy\"] = processedData_JIRAComments_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.JiraCommentsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_medEntropy\"] = processedData_JIRAComments_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.JiraCommentsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_maxEntropy\"] = processedData_JIRAComments_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.JiraCommentsAsQuery_Entropy), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_devEntropy\"] = processedData_JIRAComments_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.JiraCommentsAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-kazakhstan",
   "metadata": {},
   "source": [
    "##### Query Scope (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope[\"SvnAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Commit_natural_text, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-madrid",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope[\"SvnLogsAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Logs, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Logs),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-brooks",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope[\"SvnUnitNamesAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Unit_names, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Unit_names),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-cocktail",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope[\"JiraAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Jira_natural_text, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-mumbai",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope[\"JiraSummariesAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Summary, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Summary),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-hopkins",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope[\"JiraDescriptionsAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Description, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Description),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-israel",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Comments as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope[\"JiraCommentsAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Comments, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Comments),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-yorkshire",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesSCS[\"SvnAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-paint",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS[\"SvnLogsAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Logs, \n",
    "                                                                                                                processedData_SVNLogs_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-calendar",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS[\"SvnUnitNamesAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Unit_names, \n",
    "                                                                                                                processedData_SVNUnitNames_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-cargo",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesSCS[\"JiraAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-keeping",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS[\"JiraSummariesAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Summary, \n",
    "                                                                                                                processedData_JIRASummaries_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Kullback-Leiber divergence (JIRA Description as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS[\"JiraDescriptionsAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Description, \n",
    "                                                                                                                processedData_JIRADescriptions_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Kullback-Leiber divergence (JIRA Comments as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS[\"JiraCommentsAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Comments, \n",
    "                                                                                                                processedData_JIRAComments_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-execution",
   "metadata": {},
   "source": [
    "#### SCQ (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Commit_natural_text, intermediateData_SVN_dataProcessing.Commit_natural_text,\n",
    "                                                                                                                                         processedData_SVN_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_SVN_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_avgSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.SvnAsQuery_SCQ, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_maxSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.SvnAsQuery_SCQ), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_sumSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.SvnAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-technical",
   "metadata": {},
   "source": [
    "#### SCQ (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Logs, intermediateData_SVN_dataProcessing.Logs,\n",
    "                                                                                                                                         processedData_SVNLogs_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_SVNLogs_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_avgSCQ\"] = processedData_SVNLogs_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.SvnLogsAsQuery_SCQ, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_maxSCQ\"] = processedData_SVNLogs_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.SvnLogsAsQuery_SCQ), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_sumSCQ\"] = processedData_SVNLogs_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.SvnLogsAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-guess",
   "metadata": {},
   "source": [
    "#### SCQ (SVNUnitNames as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Unit_names, intermediateData_SVN_dataProcessing.Unit_names,\n",
    "                                                                                                                                         processedData_SVNUnitNames_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_SVNUnitNames_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_avgSCQ\"] = processedData_SVNUnitNames_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.SvnUnitNamesAsQuery_SCQ, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_maxSCQ\"] = processedData_SVNUnitNames_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.SvnUnitNamesAsQuery_SCQ), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_sumSCQ\"] = processedData_SVNUnitNames_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.SvnUnitNamesAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-laundry",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Jira_natural_text, intermediateData_JIRA_dataProcessing.Jira_natural_text,\n",
    "                                                                                                                                         processedData_JIRA_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_JIRA_dataProcessingTF_IDF,\n",
    "                                                                                                                                         intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_avgSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.JiraAsQuery_SCQ, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_maxSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.JiraAsQuery_SCQ), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_sumSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.JiraAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-updating",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Summary, intermediateData_JIRA_dataProcessing.Summary,\n",
    "                                                                                                                                         processedData_JIRASummaries_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_JIRASummaries_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_avgSCQ\"] = processedData_JIRASummaries_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.JiraSummariesAsQuery_SCQ, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_maxSCQ\"] = processedData_JIRASummaries_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.JiraSummariesAsQuery_SCQ), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_sumSCQ\"] = processedData_JIRASummaries_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.JiraSummariesAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-spirit",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Description, intermediateData_JIRA_dataProcessing.Description,\n",
    "                                                                                                                                         processedData_JIRADescriptions_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_JIRADescriptions_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_avgSCQ\"] = processedData_JIRADescriptions_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.JiraDescriptionsAsQuery_SCQ, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_maxSCQ\"] = processedData_JIRADescriptions_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.JiraDescriptionsAsQuery_SCQ), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_sumSCQ\"] = processedData_JIRADescriptions_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.JiraDescriptionsAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-walker",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Comments as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Comments, intermediateData_JIRA_dataProcessing.Comments,\n",
    "                                                                                                                                         processedData_JIRAComments_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_JIRAComments_dataProcessingCountTF_IDF,\n",
    "                                                                                                                                         intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_avgSCQ\"] = processedData_JIRAComments_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.JiraCommentsAsQuery_SCQ, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_maxSCQ\"] = processedData_JIRAComments_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.JiraCommentsAsQuery_SCQ), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_sumSCQ\"] = processedData_JIRAComments_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.JiraCommentsAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-owner",
   "metadata": {},
   "source": [
    "#### PMI (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_SVN_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessing.Commit_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_SVN_dataProcessing.Commit_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Commit_natural_text, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_avgPMI\"] = processedData_SVN_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.SvnAsQuery_PMI), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_maxPMI\"] = processedData_SVN_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.SvnAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI.drop('SvnAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-hotel",
   "metadata": {},
   "source": [
    "#### PMI (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_SVNLogs_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_SVNLogs_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessing.Logs)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_SVN_dataProcessing.Logs)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI[\"SvnLogsAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Logs, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_SVN_dataProcessing.Logs),axis=1)\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI[\"SvnLogsAsQuery_avgPMI\"] = processedData_SVNLogs_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.SvnLogsAsQuery_PMI), axis=1)\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI[\"SvnLogsAsQuery_maxPMI\"] = processedData_SVNLogs_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.SvnLogsAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI.drop('SvnLogsAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-efficiency",
   "metadata": {},
   "source": [
    "#### PMI (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_SVNUnitNames_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_SVNUnitNames_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessing.Unit_names)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_SVN_dataProcessing.Unit_names)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI[\"SvnUnitNamesAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Unit_names, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_SVN_dataProcessing.Unit_names),axis=1)\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI[\"SvnUnitNamesAsQuery_avgPMI\"] = processedData_SVNUnitNames_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.SvnUnitNamesAsQuery_PMI), axis=1)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI[\"SvnUnitNamesAsQuery_maxPMI\"] = processedData_SVNUnitNames_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.SvnUnitNamesAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI.drop('SvnUnitNamesAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stomach",
   "metadata": {},
   "source": [
    "#### PMI (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_JIRA_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessing.Jira_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_JIRA_dataProcessing.Jira_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Jira_natural_text, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_avgPMI\"] = processedData_JIRA_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.JiraAsQuery_PMI), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_maxPMI\"] = processedData_JIRA_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.JiraAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesPMI.drop('JiraAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-youth",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_JIRASummaries_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_JIRASummaries_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessing.Summary)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_JIRA_dataProcessing.Summary)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI[\"JiraSummariesAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Summary, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_JIRA_dataProcessing.Summary),axis=1)\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI[\"JiraSummariesAsQuery_avgPMI\"] = processedData_JIRASummaries_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.JiraSummariesAsQuery_PMI), axis=1)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI[\"JiraSummariesAsQuery_maxPMI\"] = processedData_JIRASummaries_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.JiraSummariesAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI.drop('JiraSummariesAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-award",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_JIRADescriptions_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_JIRADescriptions_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessing.Description)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_JIRA_dataProcessing.Description)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI[\"JiraDescriptionsAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Description, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_JIRA_dataProcessing.Description),axis=1)\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI[\"JiraDescriptionsAsQuery_avgPMI\"] = processedData_JIRADescriptions_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.JiraDescriptionsAsQuery_PMI), axis=1)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI[\"JiraDescriptionsAsQuery_maxPMI\"] = processedData_JIRADescriptions_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.JiraDescriptionsAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI.drop('JiraDescriptionsAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-convert",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Comments as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_JIRAComments_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_JIRAComments_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessing.Comments)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_JIRA_dataProcessing.Comments)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI[\"JiraCommentsAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Comments, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_JIRA_dataProcessing.Comments),axis=1)\n",
    "\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI[\"JiraCommentsAsQuery_avgPMI\"] = processedData_JIRAComments_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.JiraCommentsAsQuery_PMI), axis=1)\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI[\"JiraCommentssAsQuery_maxPMI\"] = processedData_JIRAComments_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.JiraCommentsAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI.drop('JiraCommentsAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalizeData(dataFrame):\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    names = dataFrame.columns\n",
    "    d = scaler.fit_transform(dataFrame)\n",
    "    scaledDataFrame = pd.DataFrame(d, columns=names)\n",
    "    return(scaledDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-fifteen",
   "metadata": {},
   "source": [
    "# Normalize all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "################################## Loading #################################\n",
    "#Load Process-Related Features\n",
    "processedData_dataProcessingFeaturesTime = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesTime.pkl')\n",
    "processedData_dataProcessingFeaturesStakeholder = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesStakeholder.pkl')\n",
    "\n",
    "#Load IR-Related Features - unigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery.pkl')\n",
    "\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery.pkl')\n",
    "\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmDescriptionLogsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmCommentsCommentsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmCommentsLogsAsQuery.pkl')\n",
    "\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery.pkl')\n",
    "\n",
    "\n",
    "#Load IR-Related Features - bigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram.pkl')\n",
    "\n",
    "\n",
    "#Load Document Statistics Features\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_UNION_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "#Load Query Quality Features\n",
    "processedData_dataProcessingFeaturesQueryQuality = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesQueryQuality.pkl')\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesIDF.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesICTF.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesEntropy.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesQueryScope.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCS.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCQ.pkl')\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesPMI.pkl')\n",
    "\n",
    "\n",
    "################################## Drop query array for normalization ###############################################\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesIDF.drop('SvnAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF.drop('SvnLogsAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF.drop('SvnUnitNamesAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF.drop('JiraAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF.drop('JiraSummariesAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF.drop('JiraDescriptionsAsQuery_IDF', axis = 1, inplace=True)\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF.drop('JiraCommentsAsQuery_IDF', axis = 1, inplace=True)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesICTF.drop('SvnAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF.drop('SvnLogsAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF.drop('SvnUnitNamesAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF.drop('JiraAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF.drop('JiraSummariesAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF.drop('JiraDescriptionsAsQuery_ICTF', axis = 1, inplace=True)\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF.drop('JiraCommentsAsQuery_ICTF', axis = 1, inplace=True)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesEntropy.drop('SvnAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy.drop('SvnLogsAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy.drop('SvnUnitNamesAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy.drop('JiraAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy.drop('JiraSummariesAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy.drop('JiraDescriptionsAsQuery_Entropy', axis = 1, inplace=True)\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy.drop('JiraCommentsAsQuery_Entropy', axis = 1, inplace=True)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCQ.drop('SvnAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ.drop('SvnLogsAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ.drop('SvnUnitNamesAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ.drop('JiraAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ.drop('JiraSummariesAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ.drop('JiraDescriptionsAsQuery_SCQ', axis = 1, inplace=True)\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ.drop('JiraCommentsAsQuery_SCQ', axis = 1, inplace=True)\n",
    "\n",
    "################################## Normalizing ################################################\n",
    "\n",
    "processedData_dataProcessingFeaturesTime_normalized = normalizeData(processedData_dataProcessingFeaturesTime)\n",
    "processedData_dataProcessingFeaturesStakeholder_normalized = normalizeData(processedData_dataProcessingFeaturesStakeholder)\n",
    "\n",
    "#Load IR-Related Features - unigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmLogsJiraAsQuery)\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmLogsLogAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery)\n",
    "processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery)\n",
    "\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery)\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery)\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery)\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery)\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery)\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery)\n",
    "processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery)\n",
    "processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmDescriptionLogsAsQuery)\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmCommentsCommentsAsQuery)\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmCommentsLogsAsQuery)\n",
    "\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery)\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery_normalized = normalizeData(processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery)\n",
    "\n",
    "\n",
    "\n",
    "#Load IR-Related Features - bigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram)\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram)\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram)\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram)\n",
    "processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmCommentsLogsAsQuery_2gram)\n",
    "processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram_normalized = normalizeData(processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_2gram)\n",
    "\n",
    "\n",
    "#Load Document Statistics Features\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesUniqueWordCount)\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesUniqueWordCount)\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesTotalWordCount)\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesTotalWordCount)\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesOverlapPercentage)\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesOverlapPercentage)\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage_normalized = normalizeData(processedData_UNION_dataProcessingFeaturesOverlapPercentage)\n",
    "\n",
    "#Load Query Quality Features\n",
    "processedData_SVN_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesIDF)\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesIDF)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesIDF)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesIDF)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesIDF)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesIDF)\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesIDF)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesICTF)\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesICTF)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesICTF)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesICTF)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesICTF)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesICTF)\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesICTF)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesEntropy)\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesEntropy)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesEntropy)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesEntropy)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesEntropy)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesEntropy)\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesEntropy)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesQueryScope)\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesQueryScope)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesQueryScope)\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesQueryScope)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesQueryScope)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesQueryScope)\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesQueryScope)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesSCS)\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesSCS)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesSCS)\n",
    "processedData_JIRA_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesSCS)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesSCS)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesSCS)\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesSCS)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesSCQ)\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesSCQ)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesSCQ)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesSCQ)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesSCQ)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesSCQ)\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesSCQ)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_SVN_dataProcessingFeaturesPMI)\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_SVNLogs_dataProcessingFeaturesPMI)\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_SVNUnitNames_dataProcessingFeaturesPMI)\n",
    "processedData_JIRA_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_JIRA_dataProcessingFeaturesPMI)\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_JIRASummaries_dataProcessingFeaturesPMI)\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_JIRADescriptions_dataProcessingFeaturesPMI)\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI_normalized = normalizeData(processedData_JIRAComments_dataProcessingFeaturesPMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-material",
   "metadata": {},
   "source": [
    "## 3.8 Preprocess Data - Load and transform feature families needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Merge features into 1 dataframe\n",
    "processedData_dataProcessingFeatures = pd.concat([processedData_dataProcessingFeaturesTime_normalized,\n",
    "                                                  processedData_dataProcessingFeaturesStakeholder_normalized,\n",
    "                                                  #IR-based\n",
    "                                                  processedData_dataProcessing_features_VsmLogsJiraAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmLogsLogAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmDescriptionDescriptionAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmDescriptionLogsAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmCommentsCommentsAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmCommentsLogsAsQuery_normalized,\n",
    "                                                 # processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram_normalized,\n",
    "                                                 # processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram_normalized,\n",
    "                                                 # processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram_normalized,\n",
    "                                                 # processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram_normalized,\n",
    "                                                  #processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery_normalized,\n",
    "                                                 # processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery_normalized,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery_normalized,\n",
    "\n",
    "                                                  \n",
    "                                                  #Document Statistics\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesUniqueWordCount_normalized,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesUniqueWordCount_normalized,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesTotalWordCount_normalized,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesTotalWordCount_normalized,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesOverlapPercentage_normalized,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesOverlapPercentage_normalized,\n",
    "                                                  processedData_UNION_dataProcessingFeaturesOverlapPercentage_normalized,\n",
    "                                                 #Query Quality\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF_normalized['SvnAsQuery_avgIDF'],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF_normalized['SvnAsQuery_maxIDF'],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF_normalized['SvnAsQuery_devIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF_normalized['SvnLogsAsQuery_avgIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF_normalized['SvnLogsAsQuery_maxIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF_normalized['SvnLogsAsQuery_devIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF_normalized['SvnUnitNamesAsQuery_avgIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF_normalized['SvnUnitNamesAsQuery_maxIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF_normalized['SvnUnitNamesAsQuery_devIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF_normalized['JiraAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF_normalized['JiraAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF_normalized['JiraAsQuery_devIDF'],  \n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF_normalized['JiraSummariesAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF_normalized['JiraSummariesAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF_normalized['JiraSummariesAsQuery_devIDF'],  \n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF_normalized['JiraDescriptionsAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF_normalized['JiraDescriptionsAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF_normalized['JiraDescriptionsAsQuery_devIDF'],  \n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF_normalized['JiraCommentsAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF_normalized['JiraCommentsAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF_normalized['JiraCommentsAsQuery_devIDF'],  \n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF_normalized[\"SvnAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF_normalized[\"SvnAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF_normalized[\"SvnAsQuery_devICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF_normalized[\"SvnLogsAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF_normalized[\"SvnLogsAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF_normalized[\"SvnLogsAsQuery_devICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF_normalized[\"SvnUnitNamesAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF_normalized[\"SvnUnitNamesAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF_normalized[\"SvnUnitNamesAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF_normalized[\"JiraAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF_normalized[\"JiraAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF_normalized[\"JiraAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF_normalized[\"JiraSummariesAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF_normalized[\"JiraSummariesAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF_normalized[\"JiraSummariesAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF_normalized[\"JiraDescriptionsAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF_normalized[\"JiraDescriptionsAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF_normalized[\"JiraDescriptionsAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF_normalized[\"JiraCommentsAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF_normalized[\"JiraCommentsAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF_normalized[\"JiraCommentsAsQuery_devICTF\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy_normalized[\"SvnAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy_normalized[\"SvnAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy_normalized[\"SvnAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy_normalized[\"SvnAsQuery_devEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy_normalized[\"SvnLogsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy_normalized[\"SvnLogsAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy_normalized[\"SvnLogsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy_normalized[\"SvnLogsAsQuery_devEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy_normalized[\"SvnUnitNamesAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy_normalized[\"SvnUnitNamesAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy_normalized[\"SvnUnitNamesAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy_normalized[\"SvnUnitNamesAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy_normalized[\"JiraAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy_normalized[\"JiraAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy_normalized[\"JiraAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy_normalized[\"JiraAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy_normalized[\"JiraSummariesAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy_normalized[\"JiraSummariesAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy_normalized[\"JiraSummariesAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy_normalized[\"JiraSummariesAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy_normalized[\"JiraDescriptionsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy_normalized[\"JiraDescriptionsAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy_normalized[\"JiraDescriptionsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy_normalized[\"JiraDescriptionsAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy_normalized[\"JiraCommentsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy_normalized[\"JiraCommentsAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy_normalized[\"JiraCommentsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy_normalized[\"JiraCommentsAsQuery_devEntropy\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesQueryScope_normalized,\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCS_normalized,\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ_normalized[\"SvnAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ_normalized[\"SvnAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ_normalized[\"SvnAsQuery_sumSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ_normalized[\"SvnLogsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ_normalized[\"SvnLogsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ_normalized[\"SvnLogsAsQuery_sumSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ_normalized[\"SvnUnitNamesAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ_normalized[\"SvnUnitNamesAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ_normalized[\"SvnUnitNamesAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ_normalized[\"JiraAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ_normalized[\"JiraAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ_normalized[\"JiraAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ_normalized[\"JiraSummariesAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ_normalized[\"JiraSummariesAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ_normalized[\"JiraSummariesAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ_normalized[\"JiraDescriptionsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ_normalized[\"JiraDescriptionsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ_normalized[\"JiraDescriptionsAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ_normalized[\"JiraCommentsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ_normalized[\"JiraCommentsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ_normalized[\"JiraCommentsAsQuery_sumSCQ\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesPMI_normalized[\"SvnAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesPMI_normalized[\"SvnAsQuery_maxPMI\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesPMI_normalized[\"SvnLogsAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesPMI_normalized[\"SvnLogsAsQuery_maxPMI\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesPMI_normalized[\"SvnUnitNamesAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesPMI_normalized[\"SvnUnitNamesAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesPMI_normalized[\"JiraAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesPMI_normalized[\"JiraAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesPMI_normalized[\"JiraSummariesAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesPMI_normalized[\"JiraSummariesAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesPMI_normalized[\"JiraDescriptionsAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesPMI_normalized[\"JiraDescriptionsAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesPMI_normalized[\"JiraCommentsAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesPMI_normalized[\"JiraCommentssAsQuery_maxPMI\"],                                                  \n",
    "                                                 ], axis=1)\n",
    "#Set the NaN to 0\n",
    "processedData_dataProcessingFeatures = processedData_dataProcessingFeatures.fillna(0)\n",
    "\n",
    "#Saving feature names for later use\n",
    "processedData_dataProcessingFeatureNames = list(processedData_dataProcessingFeatures.columns)\n",
    "\n",
    "#Transform pandas data frame into numpy arrays\n",
    "processedData_dataProcessingFeatures = np.array(processedData_dataProcessingFeatures)\n",
    "\n",
    "#Load labels\n",
    "processedData_dataProcessingLabels = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingLabels.pkl')\n",
    "processedData_dataProcessingLabels = np.array(processedData_dataProcessingLabels[\"is_valid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Process-Related Features\n",
    "processedData_dataProcessingFeaturesTime = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesTime.pkl')\n",
    "processedData_dataProcessingFeaturesStakeholder = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesStakeholder.pkl')\n",
    "\n",
    "#Load IR-Related Features - unigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery.pkl')\n",
    "processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery.pkl')\n",
    "\n",
    "#Load IR-Related Features - bigram\n",
    "processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram.pkl')\n",
    "processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram = pd.read_pickle(r'../data/03_processed/processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram.pkl')\n",
    "\n",
    "\n",
    "\n",
    "#Load Document Statistics Features\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_UNION_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "#Load Query Quality Features\n",
    "processedData_dataProcessingFeaturesQueryQuality = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesQueryQuality.pkl')\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVN_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_SVNLogs_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesIDF.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesIDF = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesIDF.pkl')\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesICTF.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesICTF = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesICTF.pkl')\n",
    "\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesEntropy.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesEntropy = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesEntropy.pkl')\n",
    "\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesQueryScope.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesQueryScope = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesQueryScope.pkl')\n",
    "\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCS.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCS = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCS.pkl')\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesSCQ.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesSCQ = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesSCQ.pkl')\n",
    "\n",
    "\n",
    "processedData_SVNLogs_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_SVNLogs_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_SVNUnitNames_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_SVNUnitNames_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRA_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRA_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRASummaries_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRASummaries_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRADescriptions_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRADescriptions_dataProcessingFeaturesPMI.pkl')\n",
    "processedData_JIRAComments_dataProcessingFeaturesPMI = pd.read_pickle(r'../data/03_processed/processedData_JIRAComments_dataProcessingFeaturesPMI.pkl')\n",
    "\n",
    "#Merge features into 1 dataframe\n",
    "processedData_dataProcessingFeatures = pd.concat([processedData_dataProcessingFeaturesTime,\n",
    "                                                  processedData_dataProcessingFeaturesStakeholder,\n",
    "                                                  #IR-based\n",
    "                                                  processedData_dataProcessing_features_VsmLogsJiraAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmLogsLogAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesCommentsCommentsAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesCommentsUnitNamesAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesDescriptionDescriptionAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmUnitNamesDescriptionUnitNamesAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryLogsSummaryAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryLogsLogsAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryUnitNamesSummaryAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSummaryUnitNamesUnitNamesAsQuery,\n",
    "                                                  \n",
    "                                                 # processedData_dataProcessing_features_VsmLogsJiraAsQuery_2gram,\n",
    "                                                 # processedData_dataProcessing_features_VsmLogsLogAsQuery_2gram,\n",
    "                                                 # processedData_dataProcessing_features_VsmUnitNamesJiraAsQuery_2gram,\n",
    "                                                 # processedData_dataProcessing_features_VsmUnitNamesUnitNamesAsQuery_2gram,\n",
    "                                                 # processedData_dataProcessing_features_VsmVerbPruningUnitNamesJiraAsQuery,\n",
    "                                                 # processedData_dataProcessing_features_VsmVerbPruningUnitNamesUnitNamesAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnJiraJiraAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnJiraSvnAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnSummarySvnAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnSummarySummaryAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnDescriptionSvnAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnDescriptionDescriptionAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnCommentsSvnAsQuery,\n",
    "                                                  processedData_dataProcessing_features_VsmSvnCommentsCommentsAsQuery,\n",
    "\n",
    "                                                  \n",
    "                                                  #Document Statistics\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesUniqueWordCount,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesUniqueWordCount,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesTotalWordCount,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesTotalWordCount,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                  processedData_UNION_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                 #Query Quality\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF['SvnAsQuery_avgIDF'],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF['SvnAsQuery_maxIDF'],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesIDF['SvnAsQuery_devIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF['SvnLogsAsQuery_avgIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF['SvnLogsAsQuery_maxIDF'],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesIDF['SvnLogsAsQuery_devIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF['SvnUnitNamesAsQuery_avgIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF['SvnUnitNamesAsQuery_maxIDF'],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesIDF['SvnUnitNamesAsQuery_devIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF['JiraAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF['JiraAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF['JiraAsQuery_devIDF'], \n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF['JiraSummariesAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF['JiraSummariesAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesIDF['JiraSummariesAsQuery_devIDF'], \n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF['JiraDescriptionsAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF['JiraDescriptionsAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesIDF['JiraDescriptionsAsQuery_devIDF'], \n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF['JiraCommentsAsQuery_avgIDF'],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF['JiraCommentsAsQuery_maxIDF'],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesIDF['JiraCommentsAsQuery_devIDF'], \n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_devICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesICTF[\"SvnLogsAsQuery_devICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_avgICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_maxICTF\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesICTF[\"SvnUnitNamesAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesICTF[\"JiraSummariesAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesICTF[\"JiraDescriptionsAsQuery_devICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_avgICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_maxICTF\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesICTF[\"JiraCommentsAsQuery_devICTF\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_devEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesEntropy[\"SvnLogsAsQuery_devEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_avgEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_medEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_maxEntropy\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesEntropy[\"SvnUnitNamesAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesEntropy[\"JiraSummariesAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesEntropy[\"JiraDescriptionsAsQuery_devEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_avgEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_medEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_maxEntropy\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesEntropy[\"JiraCommentsAsQuery_devEntropy\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesQueryScope,\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCS,\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_sumSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesSCQ[\"SvnLogsAsQuery_sumSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_avgSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_maxSCQ\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesSCQ[\"SvnUnitNamesAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesSCQ[\"JiraSummariesAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesSCQ[\"JiraDescriptionsAsQuery_sumSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_avgSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_maxSCQ\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesSCQ[\"JiraCommentsAsQuery_sumSCQ\"],\n",
    "                                                  \n",
    "                                                  processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_maxPMI\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesPMI[\"SvnLogsAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVNLogs_dataProcessingFeaturesPMI[\"SvnLogsAsQuery_maxPMI\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesPMI[\"SvnUnitNamesAsQuery_avgPMI\"],\n",
    "                                                  processedData_SVNUnitNames_dataProcessingFeaturesPMI[\"SvnUnitNamesAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesPMI[\"JiraSummariesAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRASummaries_dataProcessingFeaturesPMI[\"JiraSummariesAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesPMI[\"JiraDescriptionsAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRADescriptions_dataProcessingFeaturesPMI[\"JiraDescriptionsAsQuery_maxPMI\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesPMI[\"JiraCommentsAsQuery_avgPMI\"],\n",
    "                                                  processedData_JIRAComments_dataProcessingFeaturesPMI[\"JiraCommentssAsQuery_maxPMI\"],\n",
    "                                                 ], axis=1)\n",
    "#Set the NaN to 0\n",
    "processedData_dataProcessingFeatures = processedData_dataProcessingFeatures.fillna(0)\n",
    "\n",
    "#Saving feature names for later use\n",
    "processedData_dataProcessingFeatureNames = list(processedData_dataProcessingFeatures.columns)\n",
    "\n",
    "#Transform pandas data frame into numpy arrays\n",
    "processedData_dataProcessingFeatures = np.array(processedData_dataProcessingFeatures)\n",
    "\n",
    "#Load labels\n",
    "processedData_dataProcessingLabels = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingLabels.pkl')\n",
    "processedData_dataProcessingLabels = np.array(processedData_dataProcessingLabels[\"is_valid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-location",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "First select which data set to train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = processedData_dataProcessingFeatures\n",
    "labels = processedData_dataProcessingLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processedData_dataProcessingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-senegal",
   "metadata": {},
   "source": [
    "## 4.1 Create a Test and Training set (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(modelsData_trainFeatures, \n",
    " modelsData_testFeatures, \n",
    " modelsData_trainLabels, \n",
    " modelsData_testLabels) = train_test_split(features,\n",
    "                                           labels,\n",
    "                                           test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-boating",
   "metadata": {},
   "source": [
    "## 4.2 Modeling - Rebalancing the Training set (OLD, only used for quick testing)\n",
    "Select a dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample with SMOTE and random undersample for imbalanced dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#Visualise class distribution before rebalancing\n",
    "summariseClassDistribution(modelsData_trainFeatures, \n",
    "                           modelsData_trainLabels)\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy = 0.5)\n",
    "steps = [('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dataset\n",
    "modelsData_trainFeatures, modelsData_trainLabels = pipeline.fit_resample(modelsData_trainFeatures, modelsData_trainLabels)\n",
    "\n",
    "#Visualise class distribution after rebalancing\n",
    "summariseClassDistribution(modelsData_trainFeatures, \n",
    "                           modelsData_trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-louisiana",
   "metadata": {},
   "source": [
    "## 4.3 Modeling - Random Forest (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 100 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(modelsData_trainFeatures, modelsData_trainLabels.astype(bool));\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = rf, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "forest_importances = pd.Series(importances, index=[processedData_dataProcessingFeatureNames])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-intellectual",
   "metadata": {},
   "source": [
    "## 4.4 Modeling - XGBoost (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Instantiate xgboost\n",
    "GXBoost = xgb.XGBClassifier(#scale_pos_weight=1,\n",
    "                            learning_rate=0.17,\n",
    "                            colsample_bytree = 0.4,\n",
    "                            subsample = 1.0,\n",
    "                            objective='binary:logistic',\n",
    "                            n_estimators=750,\n",
    "                            max_depth=12,\n",
    "                            gamma=0.03,\n",
    "                            n_jobs=-1\n",
    "                           # seed=27\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on training data\n",
    "GXBoost.fit(modelsData_trainFeatures, modelsData_trainLabels);\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = GXBoost, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-marsh",
   "metadata": {},
   "source": [
    "# OLD Model Pipeline - Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import the model we are using\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Instantiate Light GBM\n",
    "LightGBM_Model = lgb.LGBMClassifier(n_jobs=-1, n_estimators = 3400, max_depth = 25)                         \n",
    "\n",
    "# Train the model on training data\n",
    "LightGBM_Model.fit(modelsData_trainFeatures, modelsData_trainLabels);\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = LightGBM_Model, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-store",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-immune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-burton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "nasty-robertson",
   "metadata": {},
   "source": [
    "# Model - Pipeline for GXBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "\n",
    "GXBoostPipeline = Pipeline(steps = [#['smote', SMOTE(sampling_strategy = 0.1, n_jobs=2)],\n",
    "                                    #['under', RandomUnderSampler(sampling_strategy = 0.5)],\n",
    "                                ['classifier', xgb.XGBClassifier(n_jobs=2)]])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['classifier__n_estimators'] = [450, 500, 550, 600, 650, 700, 750, 800, 850, 900]\n",
    "space['classifier__max_depth'] = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "space['classifier__subsample'] = [0.7, 0.8, 0.9, 1.0]\n",
    "space['classifier__learning_rate'] = [0.17, 0.18, 0.19, 0.2]\n",
    "space['classifier__colsample_bytree'] = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "space['classifier__gamma'] = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "GXBoostSearch = RandomizedSearchCV(estimator = GXBoostPipeline, \n",
    "                            param_distributions=space, \n",
    "                            n_iter=100, \n",
    "                            scoring='f1', \n",
    "                            n_jobs=2, \n",
    "                            cv = stratified_kfold)\n",
    "\n",
    "optimizedGXBoostModel = GXBoostSearch.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute best fit: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = optimizedGXBoostModel.best_score_\n",
    "test_score = optimizedGXBoostModel.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print('Best Hyperparameters: %s' % optimizedGXBoostModel.best_params_)\n",
    "\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = optimizedGXBoostModel, \n",
    "                     testFeatures = X_test, \n",
    "                     testLabels = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-bangladesh",
   "metadata": {},
   "source": [
    "# Model - Pipeline for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps = [['smote', SMOTE(sampling_strategy=0.1, n_jobs=2)],\n",
    "                             ['under', RandomUnderSampler(sampling_strategy = 0.5)],\n",
    "                                ['classifier', RandomForestClassifier(n_jobs=2)]])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# define search space\n",
    "space = dict() \n",
    "space['classifier__n_estimators'] = [1000, 1100, 1200, 1300, 1400]\n",
    "space['classifier__max_depth'] = [9, 10, 11, 12, 14, 15, 16]\n",
    "space['classifier__min_samples_split'] = [1, 2, 3]\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(estimator = pipeline, \n",
    "                            param_distributions=space, \n",
    "                            n_iter=100, \n",
    "                            scoring='f1', \n",
    "                            n_jobs=2, \n",
    "                            cv = stratified_kfold)\n",
    "\n",
    "optimizedRFModel = search.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute best fit: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = optimizedRFModel.best_score_\n",
    "test_score = optimizedRFModel.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print('Best Hyperparameters: %s' % optimizedRFModel.best_params_)\n",
    "\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = optimizedRFModel, \n",
    "                     testFeatures = X_test, \n",
    "                     testLabels = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-sherman",
   "metadata": {},
   "source": [
    "# Model - Pipeline for Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "\n",
    "#Import feature selection stuff\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "\n",
    "# Import the model we are using\n",
    "import lightgbm as lgb\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "\n",
    "LightGBMPipeline = Pipeline(steps = [#['smote', SMOTE(sampling_strategy = 0.1, n_jobs=2)],\n",
    "                                    #['under', RandomUnderSampler(sampling_strategy = 0.5)],\n",
    "                                ['classifier', lgb.LGBMClassifier(n_jobs=2)]])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['classifier__num_leaves'] = [60, 70, 80, 90, 100]\n",
    "space['classifier__max_depth'] = [5, 6, 7, 8]\n",
    "space['classifier__min_data_in_leaf'] = [250, 500, 750, 1000, 1250, 1500]\n",
    "\n",
    "\n",
    "LightGBMSearch = RandomizedSearchCV(estimator = LightGBMPipeline, \n",
    "                            param_distributions=space, \n",
    "                            n_iter=100, \n",
    "                            scoring= ftwo_scorer, \n",
    "                            n_jobs=2, \n",
    "                            cv = stratified_kfold)\n",
    "\n",
    "optimizedLightGBMModel = LightGBMSearch.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute best fit: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = optimizedLightGBMModel.best_score_\n",
    "test_score = optimizedLightGBMModel.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print('Best Hyperparameters: %s' % optimizedLightGBMModel.best_params_)\n",
    "\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = optimizedLightGBMModel, \n",
    "                     testFeatures = X_test, \n",
    "                     testLabels = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-meditation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-picnic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
