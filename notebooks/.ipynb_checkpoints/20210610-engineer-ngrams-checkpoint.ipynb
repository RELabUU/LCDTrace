{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-discovery",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies of the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Import Self-written Functions\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from d00_utils.calculateTimeDifference import calculateTimeDifference #Function to calc time difference\n",
    "from d01_data.loadCommits import loadCommits #Function to load SVN data\n",
    "from d02_intermediate.cleanCommitData import cleanCommitData #Function to clean commit data\n",
    "from d02_intermediate.cleanJiraData import cleanJiraData #Function to clean JIRA data\n",
    "\n",
    "from d03_processing.createFittedTF_IDF import createFittedTF_IDF #Function to see if a trace is valid\n",
    "from d03_processing.createCorpusFromDocumentList import createCorpusFromDocumentList #Function to create a corpus\n",
    "from d03_processing.checkValidityTrace import checkValidityTrace #Function to see if a trace is valid\n",
    "from d03_processing.calculateTimeDif import calculateTimeDif #Calculate the time difference between 2 dates in seconds\n",
    "from d03_processing.checkFullnameEqualsEmail import checkFullnameEqualsEmail #Check if fullName is equal to the email\n",
    "from d03_processing.calculateCosineSimilarity import calculateCosineSimilarity #Calculate the cos similarity\n",
    "from d03_processing.calculateDocumentStatistics import calculateUniqueWordCount\n",
    "from d03_processing.calculateDocumentStatistics import calculateTotalWordCount\n",
    "from d03_processing.calculateDocumentStatistics import calculateOverlapBetweenDocuments\n",
    "\n",
    "from d04_modelling.summariseClassDistribution import summariseClassDistribution #Visualize the class distribution\n",
    "from d04_modelling.showModelPerformance import showModelPerformance # Show several performance measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "current-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display full value of a column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-destination",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tight-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw JIRA dataset\n",
    "rawData_JIRA_dataProcessing = pd.read_csv('../data/01_raw/JIRA Mendix.csv')\n",
    "rawData_JIRA_academy = pd.read_excel('../data/01_raw/JIRA Mendix Academy export.xlsx')\n",
    "rawData_JIRA_academyMay = pd.read_excel('../data/01_raw/JIRA Mendix Academy export_15_05_2021.xlsx')\n",
    "\n",
    "#import\n",
    "rawData_SVN_dataProcessing = loadCommits(\"../data/01_raw/data-processing-svn-dump.txt\")\n",
    "rawData_SVN_academy= loadCommits(\"../data/01_raw/academy-svn-dump.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-heater",
   "metadata": {},
   "source": [
    "# 2. Clean Raw Data\n",
    "## 2.1 Clean Raw Data - SVN Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capable-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#nltk for NLP \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Function to transform natural text into unigram tokens\n",
    "def preprocessNaturalLanguage(text, porterStemmer, cachedStopWords):\n",
    "    string_text = str(text)\n",
    "    #lowercase the string\n",
    "    lower_case_string = string_text.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    no_interpunction = lower_case_string.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #tokenize string\n",
    "    tokens = word_tokenize(no_interpunction)\n",
    "    \n",
    "    #remove stopwords\n",
    "    tokens_without_sw = [word for word in tokens if not word in cachedStopWords]\n",
    "    \n",
    "    #Stem the tokens\n",
    "    stemmedToken = list(map(porterStemmer.stem, tokens_without_sw))\n",
    "\n",
    "    return(stemmedToken)\n",
    "\n",
    "#Function to transform date into a date object\n",
    "def preprocessCommitDate(date_string):\n",
    "    date_time_obj = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S.%fZ')  \n",
    "    return(date_time_obj)\n",
    "    \n",
    "#Remove the found Issue key from the log\n",
    "def removeIssueKey(log_message):\n",
    "    issue_keys = re.findall(r\"LRN+.[0-9]+|AFM+.[0-9]+|MA+.[0-9]+\", log_message)\n",
    "    log_message_without_key = log_message\n",
    "    for issue_key in issue_keys:\n",
    "        log_message_without_key = log_message_without_key.replace(issue_key, \"\")\n",
    "    return(log_message_without_key)\n",
    "\n",
    "def unitNamesLambdaFunc(unitName, stemmer):\n",
    "    #Lower case\n",
    "    unitNameLowered = unitName.lower()\n",
    "    \n",
    "    #Remove interpunction\n",
    "    noInterpunction = unitNameLowered.translate(str.maketrans('','',string.punctuation))\n",
    "    stemmendUnitName = stemmer.stem(noInterpunction)\n",
    "    \n",
    "    \n",
    "    return(stemmendUnitName)\n",
    "    \n",
    "\n",
    "def preprocessUnitNames(unitName, porterStemmer, cachedStopWords):\n",
    "    if (isinstance(unitName, str)):\n",
    "        #Split camelCasing\n",
    "        unitNameSplitList = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', unitName)).split()\n",
    "        \n",
    "        porterStemmer = PorterStemmer() #create an object of class PorterStemmer\n",
    "        \n",
    "        #Preprocess each split found.\n",
    "        unitNameLowered = list(map(lambda unitName: unitNamesLambdaFunc(unitName, porterStemmer), \n",
    "                                   unitNameSplitList))\n",
    "        \n",
    "        #Check for stopwords\n",
    "        tokensWithoutSW = [word for word in unitNameLowered if not word in cachedStopWords]\n",
    "\n",
    "        return(tokensWithoutSW)\n",
    "   \n",
    "    \n",
    "\n",
    "#Method to clean all columns of the provided data\n",
    "def cleanCommitData(rawCommitData): \n",
    "    #create an object of class PorterStemmer\n",
    "    porterStemmer = PorterStemmer()\n",
    "    \n",
    "    #Find all stopwords\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    \n",
    "    \n",
    "    #Remove all revisions without an issue key in the log message\n",
    "    commit_df = rawCommitData[rawCommitData[\"related_issue_key\"].notna()]\n",
    "\n",
    "    #Execute cleaning methods on dataset\n",
    "    cleaned_commit_logs = commit_df['log'].apply(lambda x: removeIssueKey(x))\n",
    "    processed_commit_logs = cleaned_commit_logs.apply(lambda x: preprocessNaturalLanguage(x, porterStemmer, cachedStopWords))\n",
    "    processed_date_times = commit_df['date'].apply(lambda x: preprocessCommitDate(x))\n",
    "    processed_unit_names = commit_df['impacted_unit_names'].apply(lambda x: preprocessUnitNames(x, porterStemmer, cachedStopWords))\n",
    "\n",
    "    #Put all data together into a new dataframe\n",
    "    commit_data = {'Revision': commit_df[\"revision\"],\n",
    "               'Email' : commit_df[\"email\"],\n",
    "               'Commit_date': processed_date_times,\n",
    "               \"Issue_key_commit\": commit_df[\"related_issue_key\"],\n",
    "               'Logs': processed_commit_logs, 'Unit_names': processed_unit_names,\n",
    "               'Commit_natural_text': processed_commit_logs + processed_unit_names\n",
    "               }\n",
    "               \n",
    "    commit_processed_df = pd.DataFrame(data=commit_data)\n",
    "\n",
    "    return(commit_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning after 0 minutes and 0.7304017543792725 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "intermediateData_SVN_dataProcessing = cleanCommitData(rawData_SVN_dataProcessing)\n",
    "#intermediateData_SVN_academy = cleanCommitData(rawData_SVN_academy)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "intermediateData_SVN_dataProcessing.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_SVN_dataProcessing.xlsx\", index = False)\n",
    "#intermediateData_SVN_academy.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_SVN_academy.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "intermediateData_SVN_dataProcessing.to_pickle(path= \"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")\n",
    "#intermediateData_SVN_academy.to_pickle(path= \"../data/02_intermediate/intermediateData_SVN_academy.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished cleaning after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-arena",
   "metadata": {},
   "source": [
    "## 2.2 Clean Raw Data - JIRA Data\n",
    "Clean the raw data of the JIRA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename key to Issue key\n",
    "rawData_JIRA_academyMay = rawData_JIRA_academyMay.rename({'Key': 'Issue key'}, axis=1)\n",
    "\n",
    "#Clean Data sets\n",
    "intermediateData_JIRA_dataProcessing = cleanJiraData(dataFrame = rawData_JIRA_dataProcessing, cleanComments = True, commentAmount = 39)\n",
    "intermediateData_JIRA_academyMay = cleanJiraData(dataFrame = rawData_JIRA_academyMay, cleanComments = False, commentAmount = 0)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "intermediateData_JIRA_dataProcessing.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_JIRA_dataProcessing.xlsx\", index = False)\n",
    "intermediateData_JIRA_academyMay.to_excel(excel_writer = \"../data/02_intermediate/intermediateData_JIRA_academyMay.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "intermediateData_JIRA_dataProcessing.to_pickle(path= \"../data/02_intermediate/intermediateData_JIRA_dataProcessing.pkl\")\n",
    "intermediateData_JIRA_academyMay.to_pickle(path= \"../data/02_intermediate/intermediateData_JIRA_academyMay.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-female",
   "metadata": {},
   "source": [
    "## 2.3 Clean Raw Data - Create JIRA Corpora\n",
    "Create the corpora for JIRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "intermediateData_JIRA_dataProcessingCorpusSummary = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Summary)\n",
    "intermediateData_JIRA_dataProcessingCorpusDescription = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Description)\n",
    "intermediateData_JIRA_dataProcessingCorpusComments = createCorpusFromDocumentList(intermediateData_JIRA_dataProcessing.Comments)\n",
    "\n",
    "#Create JIRA corpus for academy dataset\n",
    "intermediateData_JIRA_academyMayCorpusSummary = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Summary)\n",
    "intermediateData_JIRA_academyMayCorpusDescription = createCorpusFromDocumentList(intermediateData_JIRA_academyMay.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "intermediateData_JIRA_dataProcessingCorpus = [i+\" \"+j+\" \"+k for i,j,k in zip(intermediateData_JIRA_dataProcessingCorpusSummary,\n",
    "                                                                             intermediateData_JIRA_dataProcessingCorpusDescription,\n",
    "                                                                             intermediateData_JIRA_dataProcessingCorpusComments)]\n",
    "\n",
    "intermediateData_JIRA_academyMayCorpus = [i+\" \"+j for i,j in zip(intermediateData_JIRA_academyMayCorpusSummary,\n",
    "                                                                 intermediateData_JIRA_academyMayCorpusDescription)]\n",
    "\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/intermediateData_JIRA_dataProcessingCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_JIRA_dataProcessingCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_JIRA_academyMayCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_JIRA_academyMayCorpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-hayes",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create SVN Corpora\n",
    "Create the corpora for SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediateData_SVN_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corpus for log messages\n",
    "intermediateData_SVN_dataProcessingCorpus = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Logs)\n",
    "intermediateData_SVN_academyCorpus = createCorpusFromDocumentList(intermediateData_SVN_academy.Logs)\n",
    "intermediateData_SVN_academyCorpus = createCorpusFromDocumentList(intermediateData_SVN_academy.Logs)\n",
    "\n",
    "#Create corpus for models\n",
    "intermediateData_SVN_dataProcessingCorpusModel = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Unit_names)\n",
    "intermediateData_SVN_academyCorpusModel = createCorpusFromDocumentList(intermediateData_SVN_academy.Unit_names)\n",
    "\n",
    "#Create corpus for entire commit (log message + model)\n",
    "intermediateData_SVN_dataProcessingCorpusAll = createCorpusFromDocumentList(intermediateData_SVN_dataProcessing.Logs + intermediateData_SVN_dataProcessing.Unit_names)\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/intermediateData_SVN_dataProcessingCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_dataProcessingCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVN_academyCorpus.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_academyCorpus, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusModel.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_dataProcessingCorpusModel, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVN_academyCorpusModel.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_academyCorpusModel, f)\n",
    "with open('../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusAll.pkl', 'wb') as f:\n",
    "    pickle.dump(intermediateData_SVN_dataProcessingCorpusAll, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-stomach",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "individual-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained results.\n",
    "intermediateData_JIRA_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_JIRA_dataProcessing.pkl\")\n",
    "intermediateData_JIRA_academyMay = pd.read_pickle(\"../data/02_intermediate/intermediateData_JIRA_academyMay.pkl\")\n",
    "\n",
    "intermediateData_SVN_dataProcessing = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_dataProcessing.pkl\")\n",
    "intermediateData_SVN_academy = pd.read_pickle(\"../data/02_intermediate/intermediateData_SVN_academy.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "intermediateData_JIRA_dataProcessingCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_JIRA_dataProcessingCorpus.pkl')\n",
    "intermediateData_JIRA_academyMayCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_JIRA_academyMayCorpus.pkl')\n",
    "\n",
    "intermediateData_SVN_dataProcessingCorpusAll = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusAll.pkl')\n",
    "intermediateData_SVN_dataProcessingCorpusModel = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpusModel.pkl')\n",
    "intermediateData_SVN_dataProcessingCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_dataProcessingCorpus.pkl')\n",
    "intermediateData_SVN_academyCorpus = pd.read_pickle(r'../data/02_intermediate/intermediateData_SVN_academyCorpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-processor",
   "metadata": {},
   "source": [
    "## 3.0 Preprocess Data - Create cartesian product JIRA x Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "professional-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "processedData_dataProcessingCartesian = intermediateData_JIRA_dataProcessing.merge(intermediateData_SVN_dataProcessing, how='cross')\n",
    "processedData_academyCartesian = intermediateData_JIRA_academyMay.merge(intermediateData_SVN_academy, how='cross')\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingCartesian.to_pickle(path= \"../data/03_processed/processedData_dataProcessingCartesian.pkl\")\n",
    "processedData_academyCartesian.to_pickle(path= \"../data/03_processed/processedData_academyCartesian.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_dataProcessingCartesian.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-recipient",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess Data - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "processedData_dataProcessingLabels = pd.DataFrame() \n",
    "processedData_academyLabels = pd.DataFrame() \n",
    "\n",
    "#Create a column, which indicates which traces are valid.\n",
    "processedData_dataProcessingLabels[\"is_valid\"] = processedData_dataProcessingCartesian.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for dataProcessing\")\n",
    "processedData_academyLabels[\"is_valid\"] = processedData_academyCartesian.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for academy\")\n",
    "\n",
    "#Save intermediate results\n",
    "processedData_dataProcessingLabels.to_pickle(path= \"../data/03_processed/processedData_dataProcessingLabels.pkl\")\n",
    "processedData_academyLabels.to_pickle(path= \"../data/03_processed/processedData_academyLabels.pkl\")\n",
    "\n",
    "processedData_dataProcessingLabels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-projector",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess Data - Create Time-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "processedData_dataProcessingFeaturesTime = pd.DataFrame() \n",
    "processedData_academyFeaturesTime = pd.DataFrame() \n",
    "\n",
    "#Calculate the time features for data Processing Dataset\n",
    "processedData_dataProcessingFeaturesTime['Creation_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "processedData_dataProcessingFeaturesTime['Updated_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "processedData_dataProcessingFeaturesTime['Resolved_commit_date_dif'] = processedData_dataProcessingCartesian.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished data Processing\")\n",
    "\n",
    "\n",
    "#Calculate the time features for academy Dataset\n",
    "processedData_academyFeaturesTime['Creation_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "processedData_academyFeaturesTime['Updated_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "processedData_academyFeaturesTime['Resolved_commit_date_dif'] = processedData_academyCartesian.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished academy\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingFeaturesTime.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesTime.pkl\")\n",
    "processedData_academyFeaturesTime.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesTime.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-monster",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Stakeholder-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the Stakeholder features\n",
    "processedData_dataProcessingFeaturesStakeholder = pd.DataFrame() \n",
    "processedData_academyFeaturesStakeholder = pd.DataFrame() \n",
    "\n",
    "processedData_dataProcessingFeaturesStakeholder['Assignee_is_commiter'] = processedData_dataProcessingCartesian.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "print(\"Finished dataProcessing\")\n",
    "processedData_academyFeaturesStakeholder['Assignee_is_commiter'] = processedData_academyCartesian.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "print(\"Finished academy\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "processedData_dataProcessingFeaturesStakeholder.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesStakeholder.pkl\")\n",
    "processedData_academyFeaturesStakeholder.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesStakeholder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-matrix",
   "metadata": {},
   "source": [
    "## 3.4 Preprocess Data - Create Cosine Similarity Features\n",
    "\n",
    "### 3.4.1 DataProcessing - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "####################### dataProcessing ###########################\n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessingCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA\n",
    "processedData_JIRA_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for Model\n",
    "processedData_Model_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_Model_dataProcessingCountTF_IDF = createFittedTF_IDF(processedData_Model_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessingCorpusModel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################### academy ###########################\n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_academyCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_academyCountTF_IDF = createFittedTF_IDF(processedData_SVN_academyCountVectorizer, intermediateData_SVN_academyCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA\n",
    "processedData_JIRA_academyCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_academyCountTF_IDF = createFittedTF_IDF(processedData_JIRA_academyCountVectorizer, intermediateData_JIRA_academyMayCorpus)\n",
    "\n",
    "#instantiate CountVectorizer() for Model\n",
    "processedData_Model_academyCountVectorizer = CountVectorizer()\n",
    "processedData_Model_academyCountTF_IDF = createFittedTF_IDF(processedData_Model_academyCountVectorizer, intermediateData_SVN_academyCorpusModel)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitSummary = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitSummary[\"vsm_jira_to_commit_summary\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitSummary.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitSummary.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Summary JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraSummary = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraSummary[\"vsm_commit_to_jira_summary\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraSummary.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraSummary.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Summary SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitDescription[\"vsm_jira_to_commit_description\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitDescription.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Description JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraDescription[\"vsm_commit_to_jira_description\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraDescription.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Description SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitComments = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitComments[\"vsm_jira_to_commit_description\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitComments.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitComments.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Comments JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraComments = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraComments[\"vsm_commit_to_jira_comments\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Comments, x.Logs, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraComments.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraComments.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Comments SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read intermediate results\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitSummary = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitSummary.pkl')\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitDescription = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitDescription.pkl')\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraSummary = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraSummary.pkl')\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraDescription.pkl')\n",
    "\n",
    "#Merge all dataframes into 1\n",
    "processedData_dataProcessingFeaturesVsm = pd.concat([processedData_dataProcessingFeaturesVsmJiraToCommitSummary,\n",
    "                                                     processedData_dataProcessingFeaturesVsmJiraToCommitDescription,\n",
    "                                                     processedData_dataProcessingFeaturesVsmCommitToJiraSummary,\n",
    "                                                     processedData_dataProcessingFeaturesVsmCommitToJiraDescription], axis=1)\n",
    "#Save intermediate results\n",
    "processedData_dataProcessingFeaturesVsm.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsm.pkl\")\n",
    "\n",
    "processedData_dataProcessingFeaturesVsm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText[\"vsm_commit_to_jira_all_natural_text\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - All Natural Text SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText[\"vsm_jira_to_commit_all_natural_text\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - All Natural Text JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-market",
   "metadata": {},
   "source": [
    "## 3.5 Cosine similarity for JIRA issue to Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmJiraToModelDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmJiraToModelDescription[\"vsm_jira_to_model_description\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_JIRA_dataProcessingCountVectorizer, processedData_JIRA_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmJiraToModelDescription.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToModelDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Comments JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_dataProcessingFeaturesVsmModelToJiraDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_dataProcessingFeaturesVsmModelToJiraDescription[\"vsm_commit_to_model_description\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_Model_dataProcessingCountVectorizer, processedData_Model_dataProcessingCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesVsmModelToJiraDescription.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesVsmModelToJiraDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Models SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-raising",
   "metadata": {},
   "source": [
    "### 3.4.2 Academy - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmJiraToCommitSummary = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmJiraToCommitSummary[\"vsm_jira_to_commit_summary\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_JIRA_academyCountVectorizer, processedData_JIRA_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmJiraToCommitSummary.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmJiraToCommitSummary.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Summary JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmCommitToJiraSummary = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmCommitToJiraSummary[\"vsm_commit_to_jira_summary\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, processedData_SVN_academyCountVectorizer, processedData_SVN_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmCommitToJiraSummary.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmCommitToJiraSummary.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Summary SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmJiraToCommitDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmJiraToCommitDescription[\"vsm_jira_to_commit_description\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_JIRA_academyCountVectorizer, processedData_JIRA_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmJiraToCommitDescription.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmJiraToCommitDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Description JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmCommitToJiraDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmCommitToJiraDescription[\"vsm_commit_to_jira_description\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, processedData_SVN_academyCountVectorizer, processedData_SVN_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmCommitToJiraDescription.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmCommitToJiraDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Description SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmCommitToJiraAllNaturalText = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmCommitToJiraAllNaturalText[\"vsm_commit_to_jira_all_natural_text\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_SVN_academyCountVectorizer, processedData_SVN_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmCommitToJiraAllNaturalText.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmCommitToJiraAllNaturalText.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - All Natural Text SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmJiraToCommitAllNaturalText = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmJiraToCommitAllNaturalText[\"vsm_jira_to_commit_all_natural_text\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, processedData_JIRA_academyCountVectorizer, processedData_JIRA_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmJiraToCommitAllNaturalText.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmJiraToCommitAllNaturalText.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - All Natural Text JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmJiraToModelDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmJiraToModelDescription[\"vsm_jira_to_model_description\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_JIRA_academyCountVectorizer, processedData_JIRA_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmJiraToModelDescription.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmJiraToModelDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Comments JIRA -> SVN' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_academyFeaturesVsmModelToJiraDescription = pd.DataFrame() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "processedData_academyFeaturesVsmModelToJiraDescription[\"vsm_commit_to_model_description\"] = processedData_academyCartesian.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, processedData_Model_academyCountVectorizer, processedData_Model_academyCountTF_IDF), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_academyFeaturesVsmModelToJiraDescription.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsmModelToJiraDescription.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating 'VSM - Models SVN -> JIRA' after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read intermediate results\n",
    "processedData_academyFeaturesVsmJiraToCommitSummary = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmJiraToCommitSummary.pkl')\n",
    "processedData_academyFeaturesVsmJiraToCommitDescription = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmJiraToCommitDescription.pkl')\n",
    "processedData_academyFeaturesVsmCommitToJiraSummary = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmCommitToJiraSummary.pkl')\n",
    "processedData_academyFeaturesVsmCommitToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmCommitToJiraDescription.pkl')\n",
    "\n",
    "#Merge all dataframes into 1\n",
    "processedData_academyFeaturesVsm = pd.concat([processedData_academyFeaturesVsmJiraToCommitSummary,\n",
    "                                                     processedData_academyFeaturesVsmJiraToCommitDescription,\n",
    "                                                     processedData_academyFeaturesVsmCommitToJiraSummary,\n",
    "                                                     processedData_academyFeaturesVsmCommitToJiraDescription\n",
    "                                              ], axis=1)\n",
    "#Save intermediate results\n",
    "processedData_academyFeaturesVsm.to_pickle(path= \"../data/03_processed/processedData_academyFeaturesVsm.pkl\")\n",
    "\n",
    "processedData_dataProcessingFeaturesVsm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-nature",
   "metadata": {},
   "source": [
    "## 3.6 Document Statistics\n",
    "\n",
    "### dataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount = pd.DataFrame() \n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount = pd.DataFrame()\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage = pd.DataFrame()\n",
    "\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount[\"unique_term_count_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateUniqueWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount[\"unique_term_count_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateUniqueWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount[\"total_term_count_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateTotalWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount[\"total_term_count_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateTotalWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_jira\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list1'),\n",
    "                                                            axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_svn\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list2'),\n",
    "                                                            axis=1)\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage[\"overlap_percentage_compared_to_union\"] = processedData_dataProcessingCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'union'),\n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_UNION_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating document statistics in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_academyFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_SVN_academyFeaturesUniqueWordCount = pd.DataFrame() \n",
    "processedData_JIRA_academyFeaturesTotalWordCount = pd.DataFrame() \n",
    "processedData_SVN_academyFeaturesTotalWordCount = pd.DataFrame()\n",
    "\n",
    "processedData_JIRA_academyFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_SVN_academyFeaturesOverlapPercentage = pd.DataFrame()\n",
    "processedData_UNION_academyFeaturesOverlapPercentage = pd.DataFrame()\n",
    "\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_JIRA_academyFeaturesUniqueWordCount[\"unique_term_count_jira\"] = processedData_academyCartesian.apply(lambda x: calculateUniqueWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate unique terms JIRA for each trace\n",
    "processedData_SVN_academyFeaturesUniqueWordCount[\"unique_term_count_svn\"] = processedData_academyCartesian.apply(lambda x: calculateUniqueWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_JIRA_academyFeaturesTotalWordCount[\"total_term_count_jira\"] = processedData_academyCartesian.apply(lambda x: calculateTotalWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate total terms JIRA for each trace\n",
    "processedData_SVN_academyFeaturesTotalWordCount[\"total_term_count_svn\"] = processedData_academyCartesian.apply(lambda x: calculateTotalWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "processedData_JIRA_academyFeaturesOverlapPercentage[\"overlap_percentage_compared_to_jira\"] = processedData_academyCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list1'),\n",
    "                                                            axis=1)\n",
    "processedData_SVN_academyFeaturesOverlapPercentage[\"overlap_percentage_compared_to_svn\"] = processedData_academyCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list2'),\n",
    "                                                            axis=1)\n",
    "processedData_UNION_academyFeaturesOverlapPercentage[\"overlap_percentage_compared_to_union\"] = processedData_academyCartesian.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'union'),\n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_academyFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_academyFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_academyFeaturesUniqueWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_academyFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_academyFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_JIRA_academyFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_academyFeaturesTotalWordCount.to_pickle(path= \"../data/03_processed/processedData_SVN_academyFeaturesTotalWordCount.pkl\")\n",
    "\n",
    "processedData_JIRA_academyFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_JIRA_academyFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_academyFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_SVN_academyFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_academyFeaturesOverlapPercentage.to_pickle(path= \"../data/03_processed/processedData_UNION_academyFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating document statistics in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-tomorrow",
   "metadata": {},
   "source": [
    "## 3.7 Query Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rotary-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from statistics import mean, median, mode, stdev, variance\n",
    "from math import log, sqrt\n",
    "import itertools\n",
    "\n",
    "#Function calculating the IDFs of all query terms. Returns a list containing all IDFs\n",
    "def calcIDFList(document, cv, tfidf_transformer):\n",
    "    idfScoreList=[]\n",
    "    if isinstance(document, list):\n",
    "        termCount = len(document)\n",
    "        for term in document:\n",
    "            try:\n",
    "                indexOfWord = cv.get_feature_names().index(term)\n",
    "                idfScore = tfidf_transformer.idf_[indexOfWord]\n",
    "                idfScoreList.append(idfScore)\n",
    "            except:\n",
    "                idfScoreList.append(0)\n",
    "    else:\n",
    "        termCount = 0\n",
    "    return(idfScoreList)\n",
    "\n",
    "\n",
    "def calcAvgIDF(IDFList):\n",
    "    termCount = len(IDFList)\n",
    "    if(termCount != 0):\n",
    "        avgIdf = sum(IDFList) / termCount\n",
    "    else:\n",
    "        avgIdf = 0\n",
    "    return(avgIdf)\n",
    "\n",
    "def calcMaxIDF(IDFList): \n",
    "    termCount = len(IDFList)\n",
    "    if(termCount != 0):\n",
    "        maxIdf = np.amax(IDFList)\n",
    "    else: \n",
    "        maxIdf = 0\n",
    "    return(maxIdf)\n",
    "\n",
    "def calcDevIDF(IDFList):\n",
    "    termCount = len(IDFList)\n",
    "    if(termCount > 1):\n",
    "        stdevIdf = stdev(IDFList)\n",
    "    else: \n",
    "        stdevIdf = 0\n",
    "    return(stdevIdf)\n",
    "\n",
    "#Function calculating the ICTF of all query terms. Returns a list containing all IDFs\n",
    "def calcICTFList(document, cv, documentCount):\n",
    "    ICTFList = []\n",
    "        #For all terms in query, find how often they occur in the Corpus\n",
    "    if isinstance(document, list):\n",
    "        for term in document:\n",
    "            try:\n",
    "            #Find out how often the term occurs in the corpus\n",
    "                termFrequency = (cv.vocabulary_[term])\n",
    "                \n",
    "                #Compute the log\n",
    "                ictF = log(documentCount/termFrequency)\n",
    "            except:\n",
    "                ictF = 0\n",
    "            \n",
    "            ICTFList.append(ictF)\n",
    "    return(ICTFList)\n",
    "\n",
    "def calcAvgICTF(ICTFList, documentCount):\n",
    "    avgICTF = sum(ICTFList) / documentCount\n",
    "    return(avgICTF)\n",
    "\n",
    "\n",
    "def calcMaxICTF(ICTFList): \n",
    "    termCount = len(ICTFList)\n",
    "    if(termCount != 0):\n",
    "        maxICTF = np.amax(ICTFList)\n",
    "    else: \n",
    "        maxICTF = 0\n",
    "    return(maxICTF)\n",
    "\n",
    "def calcDevICTF(ICTFList):\n",
    "    termCount = len(ICTFList)\n",
    "    if(termCount > 1):\n",
    "        stdevICTF = stdev(ICTFList)\n",
    "    else: \n",
    "        stdevICTF = 0\n",
    "    return(stdevICTF)\n",
    "\n",
    "\n",
    "def calcEntropyList(query, cv, documentCount, docCollection):\n",
    "    #entropy(t) = ∑ (d∈Dt)  ( tf(t,d) / tf(t, D) ) * log |D|(tf(t,d) / tf(t, D) )\n",
    "        \n",
    "    entropyValueList = []\n",
    "    #for each term in the query, calculate the entropy of the query\n",
    "    if isinstance(query, list):\n",
    "        for queryTerm in query:\n",
    "            #For each d ∈ D\n",
    "            \n",
    "            partialEntropyList = []\n",
    "            \n",
    "            for d in docCollection:\n",
    "                #Check if queryTerm occurs in D (i.e/ d∈Dt)\n",
    "                if (isinstance(d, list)):\n",
    "                    if queryTerm in d:\n",
    "                        try:\n",
    "                            #Calculate the frequency of the term occurs in the document (i.e tf(t,d))\n",
    "                            queryTermFrequencyInDocument = d.count(queryTerm)\n",
    "                            \n",
    "                            #calculate the frequency the term occurs in the query corpus (i.e tf(t,D))\n",
    "                            queryTermFrequencyInCorpus = (cv.vocabulary_[queryTerm])\n",
    "                             \n",
    "                            # This part of the calculation tf(t,d) / tf(t, D)  * log |D|(tf(t,d) / tf(t, D))\n",
    "                            partialEntropy1stHalf = queryTermFrequencyInDocument / queryTermFrequencyInCorpus\n",
    "                            partialEntropy2ndHalf = log((queryTermFrequencyInDocument / queryTermFrequencyInCorpus), documentCount)\n",
    "                            partialEntropy = partialEntropy1stHalf\n",
    "                            partialEntropyList.append(partialEntropy)\n",
    "                        except:\n",
    "                            partialEntropyList.append(0) #If term not found entropy is 0\n",
    "            #this part of the calculation ∑ (d∈Dt)\n",
    "            entropyValueOfQueryTerm = sum(partialEntropyList)\n",
    "            entropyValueList.append(entropyValueOfQueryTerm)\n",
    "    \n",
    "    return(entropyValueList)\n",
    "\n",
    "\n",
    "def calcAvgEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        #Calculate the average of all the entropies\n",
    "        avgEntropy = sum(entropyValueList) / len(entropyValueList)\n",
    "    else:\n",
    "        avgEntropy = 0\n",
    "    return(avgEntropy)\n",
    "\n",
    "    \n",
    "def calcMedEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        #Calculate the average of all the entropies\n",
    "        medEntropy = median(entropyValueList)\n",
    "    else:\n",
    "        medEntropy = 0\n",
    "    return(medEntropy)\n",
    "    \n",
    "def calcMaxEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount != 0):\n",
    "        maxEntropy = np.amax(entropyValueList)\n",
    "    else: \n",
    "        maxEntropy = 0\n",
    "    return(maxEntropy)\n",
    "    \n",
    "def calcDevEntropy(entropyValueList):\n",
    "    termCount = len(entropyValueList)\n",
    "    if(termCount > 1):\n",
    "        #Calculate the average of all the entropies\n",
    "        devEntropy = stdev(entropyValueList)\n",
    "    else:\n",
    "        devEntropy = 0\n",
    "    return(devEntropy)\n",
    "\n",
    "#The percentage of documents in the collection containing at least one of the query terms\n",
    "def calcQueryScope(query, docCollection): \n",
    "    counter = 0\n",
    "    if isinstance(query, list):\n",
    "        for document in docCollection:\n",
    "            #check if query occurs in term. \n",
    "            if(isinstance(document, list)):\n",
    "                for queryTerm in query:\n",
    "                    if queryTerm in document:\n",
    "                        counter = counter + 1\n",
    "                        break\n",
    "    queryScope = counter / len(docCollection)\n",
    "    return(queryScope)\n",
    "\n",
    "#The Kullback-Leiber divergence of the query language model from the collection language model\n",
    "def calcSCS(query, cv, docCount):\n",
    "    divergenceList = []\n",
    "    if isinstance(query, list):\n",
    "        for queryTerm in query:\n",
    "            try:\n",
    "                #frequency of term in query - tf(q, Q)/|Q|\n",
    "                pqQ = query.count(queryTerm) / len(query)\n",
    "                \n",
    "                #frequency of term in documentlist - tf(q, D)/|D|\n",
    "                pqD = cv.vocabulary_[queryTerm]\n",
    "                \n",
    "                divergence = pqQ * log(pqQ / pqD)\n",
    "                divergenceList.append(divergence)\n",
    "            except:\n",
    "                continue\n",
    "    SCS = sum(divergenceList)\n",
    "    return(SCS)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcSCQList(query, docCollection, cv, fittedTF_IDF, documentCount):\n",
    "    SCQList = []\n",
    "    if isinstance(query, list):\n",
    "        documentString = ' '.join(query)\n",
    "        \n",
    "        #Calculate the Term Frequency of the document\n",
    "        inputDocs = [documentString] \n",
    "        \n",
    "        # count matrix \n",
    "        count_vector = cv.transform(inputDocs) \n",
    " \n",
    "        #tf-idf scores \n",
    "        tf_idf_vector = fittedTF_IDF.transform(count_vector)\n",
    "        \n",
    "        feature_names = cv.get_feature_names() \n",
    "        # place tf-idf values in a pandas data frame \n",
    "        df = pd.DataFrame(tf_idf_vector.T.todense(), \n",
    "                          index=feature_names, columns=[\"tfidf\"])\n",
    "    \n",
    "        \n",
    "        #Find the tfidf of the term\n",
    "        for queryTerm in query:    \n",
    "            try:\n",
    "                tfidf = df[\"tfidf\"][queryTerm]\n",
    "                SCQ = (1 + log(tfidf))\n",
    "                SCQList.append(SCQ)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "    avgSCQ = sum(SCQList) / documentCount\n",
    "    return(SCQList)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcAvgSCQ(SCQList, documentCount):\n",
    "    avgSCQ = sum(SCQList) / documentCount\n",
    "    return(avgSCQ)\n",
    "    \n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcMaxSCQ(SCQList):\n",
    "    termCount = len(SCQList)\n",
    "    if(termCount != 0):\n",
    "        maxSCQ = np.amax(SCQList)\n",
    "    else:\n",
    "        maxSCQ = np.NaN\n",
    "    return(maxSCQ)\n",
    "\n",
    "#The average of the collection-query similarity (SCQ) over all query terms\n",
    "def calcSumSCQ(SCQList):\n",
    "    sumSCQ = sum(SCQList)\n",
    "    return(sumSCQ)\n",
    "\n",
    "def createTermPairs(cv):\n",
    "    terms = list(cv.vocabulary_.keys())\n",
    "    #Create all possible pair combinations from the terms in the query \n",
    "    pairCombinationList = list(itertools.combinations(terms, 2))\n",
    "    return(pairCombinationList)\n",
    "\n",
    "#Method to find out how often a term occurs in a document\n",
    "def findTermFrequencies(cv, docCollection):\n",
    "    terms = list(cv.vocabulary_.keys())\n",
    "    termFrequencies = {}\n",
    "    for term in terms:\n",
    "        termCounter = 0\n",
    "        for document in docCollection:\n",
    "            if isinstance(document, list):\n",
    "                if term in document: \n",
    "                    termCounter = termCounter + 1\n",
    "        termFrequencies[term] = termCounter\n",
    "    return(termFrequencies)\n",
    "\n",
    "#Method to find out how often both terms occur in a document. \n",
    "def findTermPairFrequencies(termPairs, docCollection):\n",
    "    termPairFrequencies = {}\n",
    "    for termPair in termPairs:\n",
    "        termPairCount = 0\n",
    "        for document in docCollection:\n",
    "            if (isinstance(document, list)):\n",
    "                if all(i in document for i in termPair):\n",
    "                    termPairCount = termPairCount + 1\n",
    "        termPairFrequencies[termPair] = termPairCount\n",
    "    return(termPairFrequencies)   \n",
    "\n",
    "def calcPMIList(query, termFrequencies, termPairFrequencies, docCollection):\n",
    "    if isinstance(query, list):\n",
    "    #Find the frequencies of the individual terms and the pairs\n",
    "        pairCombinationList = list(itertools.combinations(query, 2))\n",
    "        termOccurances = []\n",
    "        for pair in pairCombinationList:\n",
    "            try:\n",
    "                q1Freq = termFrequencies[pair[0]]\n",
    "            except:\n",
    "                q1Freq = 0\n",
    "            try:\n",
    "                q2Freq = termFrequencies[pair[1]]\n",
    "            except:\n",
    "                q2Freq = 0\n",
    "            try:\n",
    "                q1q2Freq = termPairFrequencies[pair]\n",
    "            except:\n",
    "                q1q2Freq = 0\n",
    "                    \n",
    "            termOccurances.append({'q1Freq': q1Freq, \n",
    "                                   'q2Freq': q2Freq, \n",
    "                                   'q1q2Freq': q1q2Freq})\n",
    "    \n",
    "        docCount = len(docCollection)\n",
    "        pmiList = []\n",
    "        for term in termOccurances:\n",
    "            pq1 = term['q1Freq'] / docCount\n",
    "            pq2 = term['q2Freq'] / docCount\n",
    "            pq1q2 = term['q1q2Freq'] / docCount\n",
    "\n",
    "            try:\n",
    "                pmi = log(pq1q2 /(pq1 * pq2))\n",
    "            except:\n",
    "                pmi = np.nan\n",
    "            pmiList.append(pmi)\n",
    "        return(pmiList)\n",
    "    else:\n",
    "        return(np.nan)\n",
    "\n",
    "def calcAvgPMI(pmiList):\n",
    "    if(isinstance(pmiList, list)):\n",
    "        pairCount = len(pmiList)\n",
    "        if(pairCount != 0):\n",
    "            #Calculate the average of all the entropies\n",
    "            avgPMI= np.nansum(pmiList) / pairCount\n",
    "        else:\n",
    "            avgPMI = 0\n",
    "        return(avgPMI)\n",
    "    return(np.nan)\n",
    "\n",
    "def calcMaxPMI(pmiList): \n",
    "    if(isinstance(pmiList, list)):\n",
    "        pairCount = len(pmiList)\n",
    "        if(pairCount != 0):\n",
    "            maxPMI = np.nanmax(pmiList)\n",
    "        else: \n",
    "            maxPMI = np.nan\n",
    "        return(maxPMI)\n",
    "    return(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-clock",
   "metadata": {},
   "source": [
    "Instantiating neccesary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "retained-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read datasets from disk\n",
    "processedData_dataProcessingCartesian = pd.read_pickle(r\"../data/03_processed/processedData_dataProcessingCartesian.pkl\")\n",
    "processedData_academyCartesian = pd.read_pickle(r\"../data/03_processed/processedData_academyCartesian.pkl\")\n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "processedData_SVN_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_SVN_dataProcessingTF_IDF = createFittedTF_IDF(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessingCorpusAll)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA\n",
    "processedData_JIRA_dataProcessingCountVectorizer = CountVectorizer()\n",
    "processedData_JIRA_dataProcessingTF_IDF = createFittedTF_IDF(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessingCorpus)\n",
    "\n",
    "#Determine document counts\n",
    "intermediateData_JIRA_dataProcessing_documentCount = len(intermediateData_JIRA_dataProcessing.index)\n",
    "intermediateData_SVN_dataProcessing_documentCount = len(intermediateData_SVN_dataProcessing.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-hungarian",
   "metadata": {},
   "source": [
    "IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_SVN_dataProcessingTF_IDF),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_avgIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_maxIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesIDF[\"SvnAsQuery_devIDF\"] = processedData_SVN_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.SvnAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_IDF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcIDFList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                processedData_JIRA_dataProcessingTF_IDF),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_avgIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcAvgIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_maxIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcMaxIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesIDF[\"JiraAsQuery_devIDF\"] = processedData_JIRA_dataProcessingFeaturesIDF.apply(lambda x: calcDevIDF(x.JiraAsQuery_IDF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesIDF.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesIDF.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-township",
   "metadata": {},
   "source": [
    "ICTF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_avgICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.SvnAsQuery_ICTF, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_maxICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.SvnAsQuery_ICTF), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesICTF[\"SvnAsQuery_devICTF\"] = processedData_SVN_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.SvnAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesICTF = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_ICTF\"] = processedData_dataProcessingCartesian.apply(lambda x: calcICTFList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "##\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_avgICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcAvgICTF(x.JiraAsQuery_ICTF, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_maxICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcMaxICTF(x.JiraAsQuery_ICTF), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesICTF[\"JiraAsQuery_devICTF\"] = processedData_JIRA_dataProcessingFeaturesICTF.apply(lambda x: calcDevICTF(x.JiraAsQuery_ICTF), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesICTF.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesICTF.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-individual",
   "metadata": {},
   "source": [
    "Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "##\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_avgEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_medEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQueryt_maxEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesEntropy[\"SvnAsQuery_devEntropy\"] = processedData_SVN_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.SvnAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_Entropy\"] = processedData_dataProcessingCartesian.apply(lambda x: calcEntropyList(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount,\n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "##\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_avgEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcAvgEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_medEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcMedEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_maxEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcMaxEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy[\"JiraAsQuery_devEntropy\"] = processedData_JIRA_dataProcessingFeaturesEntropy.apply(lambda x: calcDevEntropy(x.JiraAsQuery_Entropy), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesEntropy.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-evidence",
   "metadata": {},
   "source": [
    "Query Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope[\"SvnAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Commit_natural_text, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope[\"JiraAsQuery_QueryScope\"] = processedData_dataProcessingCartesian.apply(lambda x: calcQueryScope(x.Jira_natural_text, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesQueryScope.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediateData_JIRA_dataProcessing.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-toner",
   "metadata": {},
   "source": [
    "Kullback-Leiber divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesSCS[\"SvnAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Commit_natural_text, \n",
    "                                                                                                                processedData_SVN_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesSCS = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesSCS[\"JiraAsQuery_SCS\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCS(x.Jira_natural_text, \n",
    "                                                                                                                processedData_JIRA_dataProcessingCountVectorizer, \n",
    "                                                                                                                intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesSCS.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCS.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-elements",
   "metadata": {},
   "source": [
    "SCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Commit_natural_text, intermediateData_SVN_dataProcessing.Commit_natural_text,\n",
    "                                                                                                                                         processedData_SVN_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_SVN_dataProcessingTF_IDF,\n",
    "                                                                                                                                         intermediateData_SVN_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_avgSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.SvnAsQuery_SCQ, intermediateData_SVN_dataProcessing_documentCount), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_maxSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.SvnAsQuery_SCQ), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesSCQ[\"SvnAsQuery_sumSCQ\"] = processedData_SVN_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.SvnAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_SCQ\"] = processedData_dataProcessingCartesian.apply(lambda x: calcSCQList(x.Jira_natural_text, intermediateData_JIRA_dataProcessing.Jira_natural_text,\n",
    "                                                                                                                                         processedData_JIRA_dataProcessingCountVectorizer,\n",
    "                                                                                                                                         processedData_JIRA_dataProcessingTF_IDF,\n",
    "                                                                                                                                         intermediateData_JIRA_dataProcessing_documentCount),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_avgSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcAvgSCQ(x.JiraAsQuery_SCQ, intermediateData_JIRA_dataProcessing_documentCount), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_maxSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcMaxSCQ(x.JiraAsQuery_SCQ), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ[\"JiraAsQuery_sumSCQ\"] = processedData_JIRA_dataProcessingFeaturesSCQ.apply(lambda x: calcSumSCQ(x.JiraAsQuery_SCQ), axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCQ.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-seattle",
   "metadata": {},
   "source": [
    "## Term-Relatedness PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_SVN_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_SVN_dataProcessingCountVectorizer, intermediateData_SVN_dataProcessing.Commit_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_SVN_dataProcessing.Commit_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Commit_natural_text, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_SVN_dataProcessing.Commit_natural_text),axis=1)\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_avgPMI\"] = processedData_SVN_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.SvnAsQuery_PMI), axis=1)\n",
    "processedData_SVN_dataProcessingFeaturesPMI[\"SvnAsQuery_maxPMI\"] = processedData_SVN_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.SvnAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_SVN_dataProcessingFeaturesPMI.drop('SvnAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_SVN_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_SVN_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(processedData_JIRA_dataProcessingCountVectorizer)\n",
    "termFrequencies = findTermFrequencies(processedData_JIRA_dataProcessingCountVectorizer, intermediateData_JIRA_dataProcessing.Jira_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, intermediateData_JIRA_dataProcessing.Jira_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_JIRA_dataProcessingFeaturesPMI = pd.DataFrame()\n",
    "\n",
    "#Calculate PMI stats for each jira\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_PMI\"] = processedData_dataProcessingCartesian.apply(lambda x: calcPMIList(x.Jira_natural_text, \n",
    "                                                                                                                                  termFrequencies, \n",
    "                                                                                                                                  termPairFrequencies, \n",
    "                                                                                                                                  intermediateData_JIRA_dataProcessing.Jira_natural_text),axis=1)\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_avgPMI\"] = processedData_JIRA_dataProcessingFeaturesPMI.apply(lambda x: calcAvgPMI(x.JiraAsQuery_PMI), axis=1)\n",
    "processedData_JIRA_dataProcessingFeaturesPMI[\"JiraAsQuery_maxPMI\"] = processedData_JIRA_dataProcessingFeaturesPMI.apply(lambda x: calcMaxPMI(x.JiraAsQuery_PMI), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "processedData_JIRA_dataProcessingFeaturesPMI.drop('JiraAsQuery_PMI', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "processedData_JIRA_dataProcessingFeaturesPMI.to_pickle(path= \"../data/03_processed/processedData_JIRA_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-helen",
   "metadata": {},
   "source": [
    "## Coherenance Score (CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parliamentary-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c'}\n"
     ]
    }
   ],
   "source": [
    "def createDocumentDictionary(documentCollection):\n",
    "    values = documentCollection\n",
    "    keys = list(range(1, len(values)+1))\n",
    "    docDictionary = dict(zip(keys, values))\n",
    "    return(docDictionary)\n",
    "\n",
    "#Method to create all possible pairs and find the sim score\n",
    "def createDocumentPairs(documentCollection, CountVectorizer, CountTF_IDF):\n",
    "    values = documentCollection\n",
    "    keys = list(range(1, len(values)+1))\n",
    "    docDictionary = dict(zip(keys, values))\n",
    "    pairCombinationList = list(itertools.combinations(keys, 2))\n",
    "    documentPairList = []\n",
    "    \n",
    "    for pairCombination in pairCombinationList:\n",
    "        #Calculate similarity between both documents.\n",
    "        similarity = calculateCosineSimilarity(docDictionary[pairCombination[0]], \n",
    "                                               docDictionary[pairCombination[1]], \n",
    "                                               CountVectorizer, \n",
    "                                               CountTF_IDF)\n",
    "        \n",
    "        documentPairList.append({'document1ID': pairCombination[0], \n",
    "                                 'document2ID': pairCombination[1], \n",
    "                                 'similarity': similarity})\n",
    "    return(documentPairList)\n",
    "\n",
    "#Calculate coherence score\n",
    "def calcCSList(query, documentDictionary, documentPairList):   \n",
    "    CSOfTermList = []\n",
    "    for queryTerm in query:    \n",
    "        #Find all documents where the term occurs in:\n",
    "        docsContainingTerm = []\n",
    "        \n",
    "        for documentID, documentContent in documentDictionary.items():\n",
    "            if(isinstance(documentContent, list)):\n",
    "                if queryTerm in documentContent:\n",
    "                    docsContainingTerm.append(documentID)\n",
    "\n",
    "        #Create pairs for all docs containing term.\n",
    "        documentContainingTermPairList = list(itertools.combinations(docsContainingTerm, 2))\n",
    "    \n",
    "        similarityList = []\n",
    "        for pair in documentContainingTermPairList:\n",
    "            for item in documentPairs:\n",
    "                if(item['document1ID'] == pair[0] and item['document2ID'] == pair[1]):\n",
    "                    similarityList.append(item['similarity'])\n",
    "                    break\n",
    "        similaritySummation = sum(similarityList)\n",
    "        DQ = len(docsContainingTerm)\n",
    "        try:\n",
    "            CSOfTerm = similaritySummation / (DQ*DQ-1)\n",
    "        except: #Math error (e.g. division by 0)\n",
    "            CSOfTerm = np.nan\n",
    "        CSOfTermList.append(CSOfTerm)\n",
    "        \n",
    "    print(CSOfTermList)\n",
    "\n",
    "    \n",
    "\n",
    "values = ['a', 'b', 'c']\n",
    "keys = list(range(1, len(values)+1))\n",
    "dictionary = dict(zip(keys, values))\n",
    "print(dictionary) # {'a': 1, 'b': 2, 'c': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spare-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\scipy\\spatial\\distance.py:728: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating query quality features in 9 minutes and 32.721861362457275 seconds\n"
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "documentPairs = createDocumentPairs(intermediateData_SVN_dataProcessing.Commit_natural_text, processedData_SVN_dataProcessingCountVectorizer, processedData_SVN_dataProcessingTF_IDF)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "express-eligibility",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e16a53843767>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdoc3Term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'basic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'check'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'whether'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'given'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'done'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'creat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdocDictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDocumentDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediateData_SVN_dataProcessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCommit_natural_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcalcCSList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc3Term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocDictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocumentPairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mendTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-baacebfe483b>\u001b[0m in \u001b[0;36mcalcCSList\u001b[1;34m(query, documentDictionary, documentPairList)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocumentContainingTermPairList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocumentPairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document1ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document2ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                     \u001b[0msimilarityList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'similarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "doc1 = ['basic', 'valid', 'check', 'whether', 'given', 'name', 'done', 'object', 'creat', 'admin', 'overview', 'page', 'ie', 'region', 'legal', 'base', 'act', 'process', 'activ', 'wizard', 'step1', 'step2', 'sub', 'valid', 'step1', 'act', 'processor', 'safeguard', 'save', 'act', 'data', 'control', 'save', 'processor', 'safeguard', 'new', 'edit', 'act', 'data', 'subject', 'save', 'data', 'subject', 'new', 'edit', 'act', 'region', 'save', 'data', 'categori', 'new', 'edit', 'act', 'data', 'categori', 'save', 'depart', 'region', 'set', 'region', 'new', 'edit', 'data', 'control', 'new', 'edit', 'bco', 'secur', 'measur', 'save']\n",
    "doc1Term = ['basic']\n",
    "doc2Term = ['basic', 'valid']\n",
    "doc3Term = ['basic', 'valid', 'check', 'whether', 'given', 'name', 'done', 'object', 'creat']\n",
    "docDictionary = createDocumentDictionary(intermediateData_SVN_dataProcessing.Commit_natural_text)\n",
    "calcCSList(doc3Term, docDictionary, documentPairs)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-hello",
   "metadata": {},
   "source": [
    "Merge all Query qualities into 1 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Documents Features\n",
    "#SVN as Query\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesIDF.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesICTF = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesICTF.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesEntropy = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesEntropy.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesQueryScope = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesQueryScope.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesSCS = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCS.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesSCQ = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesSCQ.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesPMI = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "#JIRA as Query\n",
    "processedData_JIRA_dataProcessingFeaturesIDF = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesIDF.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesICTF = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesICTF.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesEntropy = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesEntropy.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesQueryScope = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesQueryScope.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesSCS = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCS.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesSCQ = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesSCQ.pkl\")\n",
    "#processedData_JIRA_dataProcessingFeaturesPMI = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesPMI.pkl\")\n",
    "\n",
    "processedData_dataProcessingFeaturesQueryQuality = pd.concat([processedData_SVN_dataProcessingFeaturesIDF,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesICTF,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesEntropy,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesSCQ,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesPMI,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesIDF,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesICTF,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesEntropy,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesQueryScope,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCS,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesSCQ, \n",
    "                                                  #processedData_JIRA_dataProcessingFeaturesPMI,\n",
    "                                                             ], axis=1)\n",
    "\n",
    "#remove intermediate columns\n",
    "processedData_dataProcessingFeaturesQueryQuality.drop(['SvnAsQuery_IDF', \n",
    "                                                       'SvnAsQuery_ICTF', \n",
    "                                                       'SvnAsQuery_Entropy', \n",
    "                                                       'SvnAsQuery_SCQ',\n",
    "                                                       'JiraAsQuery_IDF', \n",
    "                                                       'JiraAsQuery_ICTF', \n",
    "                                                       'JiraAsQuery_Entropy', \n",
    "                                                       'JiraAsQuery_SCQ',\n",
    "                                                      ], axis = 1, inplace=True)\n",
    "#Save results in pickle\n",
    "processedData_dataProcessingFeaturesQueryQuality.to_pickle(path= \"../data/03_processed/processedData_dataProcessingFeaturesQueryQuality.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData_dataProcessingFeaturesQueryQuality.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-notice",
   "metadata": {},
   "source": [
    "## 3.7 Preprocess Data - Load and transform feature families needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Features\n",
    "processedData_dataProcessingFeaturesTime = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesTime.pkl')\n",
    "processedData_dataProcessingFeaturesStakeholder = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesStakeholder.pkl')\n",
    "processedData_dataProcessingFeaturesVsm = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsm.pkl')\n",
    "\n",
    "#Documents Features\n",
    "processedData_JIRA_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesTotalWordCount.pkl\")\n",
    "processedData_JIRA_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_SVN_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_dataProcessingFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_UNION_dataProcessingFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "\n",
    "#All Natural Text Features\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText = pd.read_pickle(r\"../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText.pkl\")\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText = pd.read_pickle(r\"../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText.pkl\")\n",
    "\n",
    "#Model Features\n",
    "processedData_dataProcessingFeaturesVsmJiraToCommitComments = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToCommitComments.pkl')\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraComments = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraComments.pkl')\n",
    "processedData_dataProcessingFeaturesVsmCommitToJiraComments = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmCommitToJiraComments.pkl')\n",
    "\n",
    "\n",
    "processedData_dataProcessingFeaturesVsmModelsToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmModelsToJiraDescription.pkl')\n",
    "processedData_dataProcessingFeaturesVsmModelToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmModelsToJiraDescription.pkl')\n",
    "processedData_dataProcessingFeaturesVsmJiraToModelDescription = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesVsmJiraToModelDescription.pkl')\n",
    "\n",
    "\n",
    "#Query Qualities\n",
    "processedData_dataProcessingFeaturesQueryQuality = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingFeaturesQueryQuality.pkl')\n",
    "\n",
    "\n",
    "#Merge features into 1 dataframe\n",
    "processedData_dataProcessingFeatures = pd.concat([processedData_dataProcessingFeaturesTime,\n",
    "                                                  processedData_dataProcessingFeaturesStakeholder,\n",
    "                                                  #processedData_dataProcessingFeaturesVsm,\n",
    "                                                 # processedData_dataProcessingFeaturesVsmJiraToCommitComments,\n",
    "                                                  #processedData_dataProcessingFeaturesVsmCommitToJiraComments,\n",
    "                                                  #processedData_dataProcessingFeaturesVsmModelToJiraDescription,\n",
    "                                                  #processedData_dataProcessingFeaturesVsmJiraToModelDescription,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesUniqueWordCount,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesUniqueWordCount,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesTotalWordCount,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesTotalWordCount,\n",
    "                                                  processedData_JIRA_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                  processedData_SVN_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                  processedData_UNION_dataProcessingFeaturesOverlapPercentage,\n",
    "                                                  processedData_dataProcessingFeaturesVsmJiraToCommitAllNaturalText,\n",
    "                                                  processedData_dataProcessingFeaturesVsmCommitToJiraAllNaturalText,\n",
    "                                                  processedData_dataProcessingFeaturesQueryQuality\n",
    "                                                 ], axis=1)\n",
    "#Set the NaN to 0\n",
    "processedData_dataProcessingFeatures = processedData_dataProcessingFeatures.fillna(0)\n",
    "\n",
    "#Saving feature names for later use\n",
    "processedData_dataProcessingFeatureNames = list(processedData_dataProcessingFeatures.columns)\n",
    "\n",
    "#Transform pandas data frame into numpy arrays\n",
    "processedData_dataProcessingFeatures = np.array(processedData_dataProcessingFeatures)\n",
    "\n",
    "#Load labels\n",
    "processedData_dataProcessingLabels = pd.read_pickle(r'../data/03_processed/processedData_dataProcessingLabels.pkl')\n",
    "processedData_dataProcessingLabels = np.array(processedData_dataProcessingLabels[\"is_valid\"])\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "##           Academy Dataset\n",
    "#########################################################################################################\n",
    "\n",
    "#Load Features\n",
    "processedData_academyFeaturesTime = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesTime.pkl')\n",
    "processedData_academyFeaturesStakeholder = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesStakeholder.pkl')\n",
    "processedData_academyFeaturesVsm = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsm.pkl')\n",
    "\n",
    "#Documents Features\n",
    "processedData_JIRA_academyFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_academyFeaturesUniqueWordCount.pkl\")\n",
    "processedData_SVN_academyFeaturesUniqueWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_academyFeaturesUniqueWordCount.pkl\")\n",
    "processedData_JIRA_academyFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_academyFeaturesTotalWordCount.pkl\")\n",
    "processedData_SVN_academyFeaturesTotalWordCount = pd.read_pickle(r\"../data/03_processed/processedData_SVN_academyFeaturesTotalWordCount.pkl\")\n",
    "processedData_JIRA_academyFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_JIRA_academyFeaturesOverlapPercentage.pkl\")\n",
    "processedData_SVN_academyFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_SVN_academyFeaturesOverlapPercentage.pkl\")\n",
    "processedData_UNION_academyFeaturesOverlapPercentage = pd.read_pickle(r\"../data/03_processed/processedData_UNION_academyFeaturesOverlapPercentage.pkl\")\n",
    "\n",
    "\n",
    "#All Natural Text Features\n",
    "processedData_academyFeaturesVsmJiraToCommitAllNaturalText = pd.read_pickle(r\"../data/03_processed/processedData_academyFeaturesVsmJiraToCommitAllNaturalText.pkl\")\n",
    "processedData_academyFeaturesVsmCommitToJiraAllNaturalText = pd.read_pickle(r\"../data/03_processed/processedData_academyFeaturesVsmCommitToJiraAllNaturalText.pkl\")\n",
    "\n",
    "#Model Features\n",
    "\n",
    "#processedData_academyFeaturesVsmModelsToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmModelsToJiraDescription.pkl')\n",
    "processedData_academyFeaturesVsmModelToJiraDescription = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmModelToJiraDescription.pkl')\n",
    "processedData_academyFeaturesVsmJiraToModelDescription = pd.read_pickle(r'../data/03_processed/processedData_academyFeaturesVsmJiraToModelDescription.pkl')\n",
    "\n",
    "#Merge features into 1 dataframe\n",
    "processedData_academyFeatures = pd.concat([processedData_academyFeaturesTime,\n",
    "                                                  processedData_academyFeaturesStakeholder,\n",
    "                                                  processedData_academyFeaturesVsm,\n",
    "                                                  processedData_academyFeaturesVsmModelToJiraDescription,\n",
    "                                                  processedData_academyFeaturesVsmJiraToModelDescription,\n",
    "                                                  processedData_JIRA_academyFeaturesUniqueWordCount,\n",
    "                                                  processedData_SVN_academyFeaturesUniqueWordCount,\n",
    "                                                  processedData_JIRA_academyFeaturesTotalWordCount,\n",
    "                                                  processedData_SVN_academyFeaturesTotalWordCount,\n",
    "                                                  processedData_JIRA_academyFeaturesOverlapPercentage,\n",
    "                                                  processedData_SVN_academyFeaturesOverlapPercentage,\n",
    "                                                  processedData_UNION_academyFeaturesOverlapPercentage,\n",
    "                                                  processedData_academyFeaturesVsmJiraToCommitAllNaturalText,\n",
    "                                                  processedData_academyFeaturesVsmCommitToJiraAllNaturalText\n",
    "                                                 ], axis=1)\n",
    "#Set the NaN to 0\n",
    "processedData_academyFeatures = processedData_academyFeatures.fillna(0)\n",
    "\n",
    "#Saving feature names for later use\n",
    "processedData_academyFeatureNames = list(processedData_academyFeatures.columns)\n",
    "\n",
    "#Transform pandas data frame into numpy arrays\n",
    "processedData_academyFeatures = np.array(processedData_academyFeatures)\n",
    "\n",
    "#Load labels\n",
    "processedData_academyLabels = pd.read_pickle(r'../data/03_processed/processedData_academyLabels.pkl')\n",
    "processedData_academyLabels = np.array(processedData_academyLabels[\"is_valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-pasta",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "First select which data set to train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = processedData_dataProcessingFeatures\n",
    "labels = processedData_dataProcessingLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-spencer",
   "metadata": {},
   "source": [
    "## 4.1 Create a Test and Training set (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(modelsData_trainFeatures, \n",
    " modelsData_testFeatures, \n",
    " modelsData_trainLabels, \n",
    " modelsData_testLabels) = train_test_split(features,\n",
    "                                           labels,\n",
    "                                           test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-somalia",
   "metadata": {},
   "source": [
    "## 4.2 Modeling - Rebalancing the Training set (OLD, only used for quick testing)\n",
    "Select a dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample with SMOTE and random undersample for imbalanced dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#Visualise class distribution before rebalancing\n",
    "summariseClassDistribution(modelsData_trainFeatures, \n",
    "                           modelsData_trainLabels)\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.01)\n",
    "under = RandomUnderSampler(sampling_strategy=0.4)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dataset\n",
    "modelsData_trainFeatures, modelsData_trainLabels = pipeline.fit_resample(modelsData_trainFeatures, modelsData_trainLabels)\n",
    "\n",
    "#Visualise class distribution after rebalancing\n",
    "summariseClassDistribution(modelsData_trainFeatures, \n",
    "                           modelsData_trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-packet",
   "metadata": {},
   "source": [
    "## 4.3 Modeling - Random Forest (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 100 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(modelsData_trainFeatures, modelsData_trainLabels.astype(bool));\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = rf, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "forest_importances = pd.Series(importances, index=processedData_dataProcessingFeatureNames)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-survivor",
   "metadata": {},
   "source": [
    "## 4.4 Modeling - XGBoost (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Instantiate xgboost\n",
    "GXBoost = xgb.XGBClassifier(#scale_pos_weight=1,\n",
    "                            learning_rate=0.17,\n",
    "                            colsample_bytree = 0.5,\n",
    "                            subsample = 0.9,\n",
    "                            objective='binary:logistic',\n",
    "                            n_estimators=500,\n",
    "                            max_depth=9,\n",
    "                            gamma=0.02,\n",
    "                            n_jobs=-1\n",
    "                           # seed=27\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on training data\n",
    "GXBoost.fit(modelsData_trainFeatures, modelsData_trainLabels);\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = GXBoost, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-submission",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (OLD, only used for quick testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# random search logistic regression model on the sonar dataset\n",
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# define model\n",
    "model = xgb.XGBClassifier()\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_estimators'] = [200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "space['max_depth'] = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "space['subsample'] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "space['gamma'] = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "space['colsample_bytree'] = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "space['learning_rate'] = [0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2]\n",
    "\n",
    "\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, n_iter=500, scoring='precision', n_jobs=-1, cv=cv, random_state=1)\n",
    "# execute search\n",
    "result = search.fit(modelsData_trainFeatures, modelsData_trainLabels)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "timeDif = str(time.time() - startTime)\n",
    "print(\"Finished after \" + timeDif + \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# random search logistic regression model on the sonar dataset\n",
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# define model\n",
    "model = RandomForestClassifier(n_estimators = 500)\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_estimators'] = [200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "space['max_depth'] = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "space['min_samples_split'] = [1, 2, 3]\n",
    "\n",
    "\n",
    "\n",
    "# define search\n",
    "search = RandomizedSearchCV(model, space, n_iter=100, scoring='precision', n_jobs=-1, cv=cv, random_state=1)\n",
    "# execute search\n",
    "result = search.fit(modelsData_trainFeatures, modelsData_trainLabels)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "timeDif = str(time.time() - startTime)\n",
    "print(\"Finished after \" + timeDif + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-wrapping",
   "metadata": {},
   "source": [
    "# Model - Pipeline for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps = [['smote', SMOTE(sampling_strategy=1, n_jobs=-1)],\n",
    "                                ['classifier', RandomForestClassifier(n_jobs=-1)]])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=3,shuffle=True)\n",
    "\n",
    "# define search space\n",
    "space = dict() \n",
    "space['classifier__n_estimators'] = [1000, 1100, 1200, 1300, 1400]\n",
    "space['classifier__max_depth'] = [9, 10, 11, 12, 14, 15, 16]\n",
    "space['classifier__min_samples_split'] = [1, 2, 3]\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(estimator = pipeline, \n",
    "                            param_distributions=space, \n",
    "                            n_iter=100, \n",
    "                            scoring='precision', \n",
    "                            n_jobs=-1, \n",
    "                            cv = stratified_kfold)\n",
    "\n",
    "optimizedRFModel = search.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute best fit: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = optimizedRFModel.best_score_\n",
    "test_score = optimizedRFModel.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print('Best Hyperparameters: %s' % optimizedRFModel.best_params_)\n",
    "\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = optimizedRFModel, \n",
    "                     testFeatures = X_test, \n",
    "                     testLabels = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-phase",
   "metadata": {},
   "source": [
    "# Model - Pipeline for GXBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "\n",
    "GXBoostPipeline = Pipeline(steps = [['smote', SMOTE(sampling_strategy=0.1, n_jobs=-1)],\n",
    "                                    ['under', RandomUnderSampler(sampling_strategy=0.5)],\n",
    "                                ['classifier', xgb.XGBClassifier(n_jobs=-1)]])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=3,shuffle=True)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['classifier__n_estimators'] = [450, 500, 550, 600, 650, 700, 750, 800, 850, 900]\n",
    "space['classifier__max_depth'] = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "space['classifier__subsample'] = [0.7, 0.8, 0.9, 1.0]\n",
    "space['classifier__learning_rate'] = [0.17, 0.18, 0.19, 0.2]\n",
    "space['classifier__colsample_bytree'] = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "space['classifier__gamma'] = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "GXBoostSearch = RandomizedSearchCV(estimator = GXBoostPipeline, \n",
    "                            param_distributions=space, \n",
    "                            n_iter=100, \n",
    "                            scoring='precision', \n",
    "                            n_jobs=-1, \n",
    "                            cv = stratified_kfold)\n",
    "\n",
    "optimizedGXBoostModel = GXBoostSearch.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute best fit: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = optimizedGXBoostModel.best_score_\n",
    "test_score = optimizedGXBoostModel.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print('Best Hyperparameters: %s' % optimizedGXBoostModel.best_params_)\n",
    "\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = optimizedGXBoostModel, \n",
    "                     testFeatures = X_test, \n",
    "                     testLabels = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-accreditation",
   "metadata": {},
   "source": [
    "# Model Pipeline - Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import the model we are using\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Instantiate Light GBM\n",
    "LightGBM_Model = lgb.LGBMClassifier( n_jobs=-1, n_estimators = 500, max_depth = 9)                         \n",
    "\n",
    "# Train the model on training data\n",
    "LightGBM_Model.fit(modelsData_trainFeatures, modelsData_trainLabels);\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = LightGBM_Model, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-current",
   "metadata": {},
   "source": [
    "# Model Pipeline - Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import the model we are using\n",
    "import catboost as cb\n",
    "\n",
    "# Instantiate Light GBM\n",
    "CatBoost_Model = cb.CatBoostClassifier()\n",
    "\n",
    "# Train the model on training data\n",
    "CatBoost_Model.fit(modelsData_trainFeatures, modelsData_trainLabels);\n",
    "\n",
    "#Display the model performance    \n",
    "showModelPerformance(trainedModel = CatBoost_Model, \n",
    "                     testFeatures = modelsData_testFeatures, \n",
    "                     testLabels = modelsData_testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
