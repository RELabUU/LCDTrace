{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Import Self-written Functions\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from d00_utils.calculateTimeDifference import calculateTimeDifference #Function to calc time difference\n",
    "from d01_data.loadCommits import loadCommits #Function to load SVN data\n",
    "from d02_intermediate.cleanCommitData import cleanCommitData #Function to clean commit data\n",
    "from d02_intermediate.cleanJiraData import cleanJiraData #Function to clean JIRA data\n",
    "\n",
    "from d03_processing.createFittedTF_IDF import createFittedTF_IDF #Function to see if a trace is valid\n",
    "from d03_processing.createCorpusFromDocumentList import createCorpusFromDocumentList #Function to create a corpus\n",
    "from d03_processing.checkValidityTrace import checkValidityTrace #Function to see if a trace is valid\n",
    "from d03_processing.calculateTimeDif import calculateTimeDif #Calculate the time difference between 2 dates in seconds\n",
    "from d03_processing.checkFullnameEqualsEmail import checkFullnameEqualsEmail #Check if fullName is equal to the email\n",
    "from d03_processing.calculateCosineSimilarity import calculateCosineSimilarity #Calculate the cos similarity\n",
    "from d03_processing.calculateDocumentStatistics import *\n",
    "\n",
    "from d03_processing.calculateQueryQuality import *\n",
    "from d03_processing.normalize_data import *\n",
    "\n",
    "from d04_model_evaluation.model_evaluation import *\n",
    "\n",
    "#Display full value of a column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-sellers",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw JIRA data as a pandas dataframe\n",
    "#jira_df_raw = pd.read_excel('../data/01_raw/JIRA_MXShop.xlsx')\n",
    "jira_df_raw = pd.read_csv('../data/01_raw/JIRA_DataProcessing.csv')\n",
    "\n",
    "#Import raw svn data as a pandas dataframe\n",
    "svn_df_raw = loadCommits('../data/01_raw/data-processing-svn-dump.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-utility",
   "metadata": {},
   "source": [
    "# 2. Clean Raw Data\n",
    "## 2.1 Clean Raw Data - SVN Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "svn_df_clean = cleanCommitData(svn_df_raw)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "svn_df_clean.to_excel(excel_writer = \"../data/02_intermediate/svn_df_clean.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "svn_df_clean.to_pickle(path= \"../data/02_intermediate/svn_df_clean.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished cleaning after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-campbell",
   "metadata": {},
   "source": [
    "## 2.2 Clean Raw Data - JIRA Data\n",
    "Clean the raw data of the SVN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename key to Issue key\n",
    "jira_df_raw = jira_df_raw.rename({'Key': 'Issue key'}, axis=1)\n",
    "\n",
    "#Clean Data sets\n",
    "jira_df_clean = cleanJiraData(dataFrame = jira_df_raw, cleanComments = False, commentAmount = 39)\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "jira_df_clean.to_excel(excel_writer = \"../data/02_intermediate/jira_df_clean.xlsx\", index = False)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "jira_df_clean.to_pickle(path= \"../data/02_intermediate/jira_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-bankruptcy",
   "metadata": {},
   "source": [
    "## 2.4 Clean Raw Data - Create Corpora\n",
    "Create the corpora for JIRA Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create JIRA corpus for dataProcessing dataset\n",
    "jira_corpus_summary  = createCorpusFromDocumentList(jira_df_clean.Summary)\n",
    "jira_corpus_description = createCorpusFromDocumentList(jira_df_clean.Description)\n",
    "\n",
    "#Merge all JIRA Corpora into 1 corpus\n",
    "jira_corpus_all = [i+\" \"+j for i,j in zip(jira_corpus_summary,\n",
    "                                          jira_corpus_description)]\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/jira_corpus_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_summary, f)\n",
    "\n",
    "with open('../data/02_intermediate/jira_corpus_description.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_description, f)\n",
    "\n",
    "with open('../data/02_intermediate/jira_corpus_all.pkl', 'wb') as f:\n",
    "    pickle.dump(jira_corpus_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-syntax",
   "metadata": {},
   "source": [
    "Create the corpora for SVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corpus for log messages\n",
    "svn_corpus_log = createCorpusFromDocumentList(svn_df_clean.Logs)\n",
    "\n",
    "#Create corpus for unit names\n",
    "svn_corpus_unitname = createCorpusFromDocumentList(svn_df_clean.Unit_names)\n",
    "\n",
    "#Create corpus for entire commit (log message + model)\n",
    "svn_corpus_all = createCorpusFromDocumentList(svn_df_clean.Logs + svn_df_clean.Unit_names)\n",
    "\n",
    "#Save intermediate pickles\n",
    "with open('../data/02_intermediate/svn_corpus_log.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_log, f)\n",
    "\n",
    "with open('../data/02_intermediate/svn_corpus_unitname.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_unitname, f)\n",
    "\n",
    "with open('../data/02_intermediate/svn_corpus_all.pkl', 'wb') as f:\n",
    "    pickle.dump(svn_corpus_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-collaboration",
   "metadata": {},
   "source": [
    "# 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code block when you've restarted the kernel, and want to use previously gained results.\n",
    "#Load JIRA Corpora\n",
    "jira_corpus_summary = pd.read_pickle(\"../data/02_intermediate/jira_corpus_summary.pkl\")\n",
    "jira_corpus_description = pd.read_pickle(\"../data/02_intermediate/jira_corpus_description.pkl\")\n",
    "jira_corpus_all = pd.read_pickle(\"../data/02_intermediate/jira_corpus_all.pkl\")\n",
    "\n",
    "#Load SVN corora\n",
    "svn_corpus_log = pd.read_pickle(\"../data/02_intermediate/svn_corpus_log.pkl\")\n",
    "svn_corpus_unitname = pd.read_pickle(\"../data/02_intermediate/svn_corpus_unitname.pkl\")\n",
    "svn_corpus_all = pd.read_pickle(\"../data/02_intermediate/svn_corpus_all.pkl\")\n",
    "\n",
    "#Load clean datasets\n",
    "jira_df_clean = pd.read_pickle(\"../data/02_intermediate/jira_df_clean.pkl\")\n",
    "svn_df_clean = pd.read_pickle(\"../data/02_intermediate/svn_df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-mouse",
   "metadata": {},
   "source": [
    "## 3.0 Preprocess Data - Create cartesian product JIRA x Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cartesian products JIRA x Commits\n",
    "cartesian_df = jira_df_clean.merge(svn_df_clean, how='cross')\n",
    "\n",
    "#Drop all rows which do not meet the rules of causality\n",
    "cartesian_df = cartesian_df.drop(cartesian_df[cartesian_df.Jira_created_date > cartesian_df.Commit_date].index)\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "cartesian_df.to_pickle(path= \"../data/03_processed/cartesian_df.pkl\")\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "cartesian_df.to_excel(excel_writer = \"../data/02_intermediate/cartesian_df.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run line below to get cartesian df\n",
    "cartesian_df = pd.read_pickle(r'../data/03_processed/cartesian_df.pkl')\n",
    "\n",
    "cartesian_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-texas",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess Data - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "labels_df = pd.DataFrame() \n",
    "\n",
    "#Create a column, which indicates which traces are valid.\n",
    "labels_df[\"is_valid\"] = cartesian_df.apply(lambda x: checkValidityTrace(x.Issue_key_jira, x.Issue_key_commit), axis=1)\n",
    "print(\"Finished creating labels for dataProcessing\")\n",
    "\n",
    "#Save intermediate results\n",
    "labels_df.to_pickle(path= \"../data/03_processed/labels_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[labels_df[\"is_valid\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-helicopter",
   "metadata": {},
   "source": [
    "## 3.2 Preprocess Data - Create Process-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrames for the time features\n",
    "features_process_related = pd.DataFrame() \n",
    "\n",
    "#Calculate the time features for data Processing Dataset\n",
    "features_process_related['f1_assignee_is_commiter'] = cartesian_df.apply(lambda x: checkFullnameEqualsEmail(x.Assignee, x.Email), axis=1)\n",
    "features_process_related['f2_timedif_issuecreation_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_created_date, x.Commit_date), axis=1)\n",
    "features_process_related['f3_timedif_issueupdated_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_updated_date, x.Commit_date), axis=1)\n",
    "features_process_related['f4_timedif_issueresolved_and_commitcreation'] = cartesian_df.apply(lambda x: calculateTimeDif(x.Jira_resolved_date, x.Commit_date), axis=1)\n",
    "print(\"Finished data Processing\")\n",
    "\n",
    "#Create a pickle file for all intermediate datasets\n",
    "features_process_related.to_pickle(path= \"../data/03_processed/features_process_related.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-associate",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Document Statistics Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrames for document statistics features\n",
    "features_document_statistics = pd.DataFrame() \n",
    "\n",
    "features_document_statistics[\"f5_total_terms_jira\"] = cartesian_df.apply(lambda x: calculateTotalWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "#Calculate total terms JIRA for each trace\n",
    "features_document_statistics[\"f6_total_terms_svn\"] = cartesian_df.apply(lambda x: calculateTotalWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "features_document_statistics[\"f7_unique_terms_jira\"] = cartesian_df.apply(lambda x: calculateUniqueWordCount(x.Jira_natural_text), \n",
    "                                                            axis=1)\n",
    "#Calculate unique terms JIRA for each trace\n",
    "features_document_statistics[\"f8_unique_terms_svn\"] = cartesian_df.apply(lambda x: calculateUniqueWordCount(x.Commit_natural_text), \n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "features_document_statistics[\"f9_overlap_terms_compared_to_jira\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list1'),\n",
    "                                                            axis=1)\n",
    "features_document_statistics[\"f10_overlap_terms_to_svn\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'list2'),\n",
    "                                                            axis=1)\n",
    "features_document_statistics[\"f11_overlap_terms_to_union\"] = cartesian_df.apply(lambda x: calculateOverlapBetweenDocuments(x.Jira_natural_text, x.Commit_natural_text, 'union'),\n",
    "                                                            axis=1)\n",
    "\n",
    "\n",
    "#Save results in pickle\n",
    "features_document_statistics.to_pickle(path= \"../data/03_processed/features_document_statistics.pkl\")\n",
    "\n",
    "#Create a temp XLSX file for all intermediate datasets\n",
    "features_document_statistics.to_excel(excel_writer = \"../data/03_processed/features_document_statistics.xlsx\", index = False)\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating document statistics in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-marijuana",
   "metadata": {},
   "source": [
    "## 3.3 Preprocess Data - Create Information Retrieval Features\n",
    "### 3.3.1 Create tfidf for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataFrame\n",
    "features_information_retrieval = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the count vectorizer and tfidf for the corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#instantiate CountVectorizer() for SVN\n",
    "svn_all_countvectorizer = CountVectorizer()\n",
    "svn_all_tfidf = createFittedTF_IDF(svn_all_countvectorizer, svn_corpus_all)\n",
    "\n",
    "svn_log_countvectorizer = CountVectorizer()\n",
    "svn_log_tfidf = createFittedTF_IDF(svn_log_countvectorizer, svn_corpus_log)\n",
    "\n",
    "svn_unitname_countvectorizer = CountVectorizer()\n",
    "svn_unitname_tfidf = createFittedTF_IDF(svn_unitname_countvectorizer, svn_corpus_unitname)\n",
    "\n",
    "#instantiate CountVectorizer() for JIRA - unigram\n",
    "jira_all_countvectorizer = CountVectorizer()\n",
    "jira_all_tfidf = createFittedTF_IDF(jira_all_countvectorizer, jira_corpus_all)\n",
    "\n",
    "jira_summary_countvectorizer = CountVectorizer()\n",
    "jira_summary_tfidf = createFittedTF_IDF(jira_summary_countvectorizer, jira_corpus_summary)\n",
    "\n",
    "jira_description_countvectorizer = CountVectorizer()\n",
    "jira_description_tfidf = createFittedTF_IDF(jira_description_countvectorizer, jira_corpus_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-audience",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f12_ir_log_and_summary_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                                                 svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f13_ir_log_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Logs, \n",
    "                                                                                                                                    jira_summary_countvectorizer, \n",
    "                                                                                                                                    jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-employer",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f14_ir_log_and_description_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                        svn_log_countvectorizer, \n",
    "                                                                                                                        svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f15_ir_log_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Logs, \n",
    "                                                                                                                                jira_description_countvectorizer, \n",
    "                                                                                                                                jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-department",
   "metadata": {},
   "source": [
    "#### IR Features - Log Message and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f16_ir_log_and_jira_all_log_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                              svn_log_countvectorizer, \n",
    "                                                                                                                              svn_log_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f17_ir_log_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Logs, \n",
    "                                                                                                                              jira_all_countvectorizer, \n",
    "                                                                                                                              jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-holder",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f18_ir_unitname_and_summary_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, \n",
    "                                                                                                                                       svn_unitname_countvectorizer, \n",
    "                                                                                                                                       svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f19_ir_unitname_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Summary, x.Unit_names, \n",
    "                                                                                                                                     jira_summary_countvectorizer, \n",
    "                                                                                                                                     jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-beast",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f20_ir_unitname_and_description_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                                        svn_unitname_countvectorizer, \n",
    "                                                                                                                                        svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f21_ir_unitname_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Description, x.Unit_names, \n",
    "                                                                                                                                          jira_description_countvectorizer, \n",
    "                                                                                                                                          jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-district",
   "metadata": {},
   "source": [
    "#### IR Features - Unit Names and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f22_ir_unitname_and_jira_all_unitname_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, \n",
    "                                                                                                                       svn_unitname_countvectorizer, \n",
    "                                                                                                                       svn_unitname_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f23_ir_unitname_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Unit_names, \n",
    "                                                                                                                                   jira_all_countvectorizer, \n",
    "                                                                                                                                   jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-duplicate",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f24_ir_svn_all_and_summary_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, \n",
    "                                                                                                                             svn_all_countvectorizer, \n",
    "                                                                                                                             svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f25_ir_svn_all_and_summary_summary_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Summary, \n",
    "                                                                                                                            jira_summary_countvectorizer, \n",
    "                                                                                                                            jira_summary_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-forwarding",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f26_ir_svn_all_and_description_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, \n",
    "                                                                                                                            svn_all_countvectorizer, \n",
    "                                                                                                                            svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f27_ir_svn_all_and_description_description_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Commit_natural_text, x.Description, \n",
    "                                                                                                                                    jira_description_countvectorizer, \n",
    "                                                                                                                                    jira_description_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-envelope",
   "metadata": {},
   "source": [
    "#### IR Features - Revision All-Natural Text and JIRA All-Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f28_ir_svn_all_and_jira_all_svn_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, \n",
    "                                                                                                                     svn_all_countvectorizer, \n",
    "                                                                                                                     svn_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate cosine similarity for each trace\n",
    "features_information_retrieval[\"f29_ir_svn_all_and_jira_all_jira_all_as_query\"] = cartesian_df.apply(lambda x: calculateCosineSimilarity(x.Jira_natural_text, x.Commit_natural_text, \n",
    "                                                                                                                      jira_all_countvectorizer, \n",
    "                                                                                                                      jira_all_tfidf), axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_information_retrieval.to_pickle(path= \"../data/03_processed/features_information_retrieval.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished after \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-pressing",
   "metadata": {},
   "source": [
    "## 3.7 Query Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine document counts\n",
    "jira_documentcount = len(jira_df_clean.index)\n",
    "svn_documentcount = len(svn_df_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-stage",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "processedData_SVN_dataProcessingFeaturesIDF = pd.DataFrame()\n",
    "\n",
    "features_qq_specificity = pd.DataFrame()\n",
    "\n",
    "#Calculate temporary IDF stats for each svn\n",
    "features_qq_specificity[\"idf_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Commit_natural_text, \n",
    "                                                                                           svn_all_countvectorizer,\n",
    "                                                                                           svn_all_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f30_avgidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f31_maxidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f32_devidf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-moment",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_log_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Logs, \n",
    "                                                                                       svn_log_countvectorizer, \n",
    "                                                                                       svn_log_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f33_avgidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f34_maxidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f35_devidf_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_log_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-sampling",
   "metadata": {},
   "source": [
    "#### IDF Scores (SVNUnitNames as Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_unitname_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Unit_names, \n",
    "                                                                                              svn_unitname_countvectorizer, \n",
    "                                                                                              svn_unitname_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f36_avgidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f37_maxidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f38_devidf_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-arena",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Jira_natural_text, \n",
    "                                                                                            jira_all_countvectorizer,\n",
    "                                                                                            jira_all_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f39_avgidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f40_maxidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f41_devidf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-gothic",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Summary, \n",
    "                                                                                                jira_summary_countvectorizer,\n",
    "                                                                                                jira_summary_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f42_avgidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f43_maxidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f44_devidf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-specialist",
   "metadata": {},
   "source": [
    "##### IDF Scores (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"idf_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcIDFList(x.Description, \n",
    "                                                                                                    jira_description_countvectorizer,\n",
    "                                                                                                    jira_description_tfidf),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f45_avgidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgIDF(x.idf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f46_maxidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxIDF(x.idf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f47_devidf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevIDF(x.idf_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove IDF stats\n",
    "features_qq_specificity.drop('idf_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-ensemble",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Commit_natural_text,\n",
    "                                                                                            svn_all_countvectorizer,\n",
    "                                                                                            svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f48_avgictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_all_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f49_maxictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f50_devictf_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-anthropology",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Logs, \n",
    "                                                                                             svn_log_countvectorizer, \n",
    "                                                                                             svn_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f51_avgictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_log_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f52_maxictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f53_devictf_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-bikini",
   "metadata": {},
   "source": [
    "#### ICTF Scores (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Unit_names, \n",
    "                                                                                                  svn_unitname_countvectorizer, \n",
    "                                                                                                  svn_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f54_avgictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_svn_unitname_as_query, svn_documentcount), axis=1)\n",
    "features_qq_specificity[\"f55_maxictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f56_devictf_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-japan",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Jira_natural_text, \n",
    "                                                                                              jira_all_countvectorizer, \n",
    "                                                                                              jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f57_avgictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_all_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f58_maxictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f59_devictf_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-freeware",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Summary,\n",
    "                                                                                                 jira_summary_countvectorizer, \n",
    "                                                                                                 jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f60_avgictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_summary_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f61_maxictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f62_devictf_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-offset",
   "metadata": {},
   "source": [
    "#### ICTF Scores (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"ictf_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcICTFList(x.Description,\n",
    "                                                                                                     jira_description_countvectorizer,\n",
    "                                                                                                     jira_documentcount),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f63_avgictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgICTF(x.ictf_jira_description_as_query, jira_documentcount), axis=1)\n",
    "features_qq_specificity[\"f64_maxictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxICTF(x.ictf_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f65_devictf_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevICTF(x.ictf_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove ICTF stats\n",
    "features_qq_specificity.drop('ictf_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-sheffield",
   "metadata": {},
   "source": [
    "#### Entropy (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Commit_natural_text,\n",
    "                                                                                                   svn_all_countvectorizer,\n",
    "                                                                                                   svn_documentcount,\n",
    "                                                                                                   svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "features_qq_specificity[\"f66_avgentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f67_medentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f68_maxentropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f69_deventropy_svn_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75aa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-iraqi",
   "metadata": {},
   "source": [
    "#### Entropy (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "print(\"Time to stop\")\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Logs, \n",
    "                                                                                                 svn_log_countvectorizer, \n",
    "                                                                                                 svn_documentcount,\n",
    "                                                                                                 svn_df_clean.Logs),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f70_avgentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f71_medentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f72_maxentropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "features_qq_specificity[\"f73_deventropy_svn_log_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recorder",
   "metadata": {},
   "source": [
    "#### Entropy (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Unit_names, \n",
    "                                                                                                      svn_unitname_countvectorizer, \n",
    "                                                                                                      svn_documentcount,\n",
    "                                                                                                      svn_df_clean.Unit_names),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f74_avgentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f75_medentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f76_maxentropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "features_qq_specificity[\"f77_deventropy_svn_unitname_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-kingston",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Jira_natural_text, \n",
    "                                                                                                    jira_all_countvectorizer,\n",
    "                                                                                                    jira_documentcount,\n",
    "                                                                                                    jira_df_clean.Jira_natural_text),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f78_avgentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f79_medentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f80_maxentropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "features_qq_specificity[\"f81_deventropy_jira_all_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-technical",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Summary, \n",
    "                                                                                                        jira_summary_countvectorizer,\n",
    "                                                                                                        jira_documentcount,\n",
    "                                                                                                        jira_df_clean.Summary),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f82_avgentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f83_medentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f84_maxentropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "features_qq_specificity[\"f85_deventropy_jira_summary_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-wallace",
   "metadata": {},
   "source": [
    "#### Entropy (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"entropy_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcEntropyList(x.Description,\n",
    "                                                                                                            jira_description_countvectorizer,\n",
    "                                                                                                            jira_documentcount,\n",
    "                                                                                                            jira_df_clean.Description),axis=1)\n",
    "##\n",
    "features_qq_specificity[\"f86_avgentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcAvgEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f87_medentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMedEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f88_maxentropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcMaxEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "features_qq_specificity[\"f89_deventropy_jira_description_as_query\"] = features_qq_specificity.apply(lambda x: calcDevEntropy(x.entropy_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove Entropy stats\n",
    "features_qq_specificity.drop('entropy_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-kazakhstan",
   "metadata": {},
   "source": [
    "##### Query Scope (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f90_queryscope_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Commit_natural_text,\n",
    "                                                                                                         svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-madrid",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f91_queryscope_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Logs,\n",
    "                                                                                                         svn_df_clean.Logs),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-brooks",
   "metadata": {},
   "source": [
    "##### Query Scope (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f92_queryscope_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Unit_names, \n",
    "                                                                                                              svn_df_clean.Unit_names),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-cocktail",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f93_queryscope_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Jira_natural_text,\n",
    "                                                                                                          jira_df_clean.Jira_natural_text),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-mumbai",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f94_queryscope_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Summary, \n",
    "                                                                                                              jira_df_clean.Summary),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-hopkins",
   "metadata": {},
   "source": [
    "##### Query Scope (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f95_queryscope_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcQueryScope(x.Description,\n",
    "                                                                                                                  jira_df_clean.Description),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-yorkshire",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f96_scs_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Commit_natural_text,\n",
    "                                                                                           svn_all_countvectorizer,\n",
    "                                                                                           svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-paint",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f97_scs_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Logs,\n",
    "                                                                                           svn_log_countvectorizer,\n",
    "                                                                                           svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-calendar",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f98_scs_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Unit_names,\n",
    "                                                                                                svn_unitname_countvectorizer,\n",
    "                                                                                                svn_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-cargo",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f99_scs_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Jira_natural_text,\n",
    "                                                                                            jira_all_countvectorizer,\n",
    "                                                                                            jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-keeping",
   "metadata": {},
   "source": [
    "#### Kullback-Leiber divergence (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f100_scs_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Summary, \n",
    "                                                                                                jira_summary_countvectorizer,\n",
    "                                                                                                jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-struggle",
   "metadata": {},
   "source": [
    "##### Kullback-Leiber divergence (JIRA Description as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_specificity[\"f101_scs_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcSCS(x.Description, \n",
    "                                                                                                   jira_description_countvectorizer,\n",
    "                                                                                                   jira_documentcount),axis=1)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_specificity.to_pickle(path= \"../data/03_processed/features_qq_specificity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-commitment",
   "metadata": {},
   "source": [
    "### Query Quality Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-execution",
   "metadata": {},
   "source": [
    "#### SCQ (SVN as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create new dataFrame\n",
    "features_qq_similarity = pd.DataFrame()\n",
    "\n",
    "#Calculate SCQ stats for each svn\n",
    "features_qq_similarity[\"scq_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Commit_natural_text, \n",
    "                                                                                          svn_df_clean.Commit_natural_text,\n",
    "                                                                                          svn_all_countvectorizer,\n",
    "                                                                                          svn_all_tfidf,\n",
    "                                                                                          svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f102_SvnAsQuery_avgSCQ\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_all_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f103_SvnAsQuery_maxSCQ\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_all_as_query), axis=1)\n",
    "features_qq_similarity[\"f104_SvnAsQuery_sumSCQ\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_all_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-technical",
   "metadata": {},
   "source": [
    "#### SCQ (SVNLogs as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Logs, \n",
    "                                                                                          svn_df_clean.Logs,\n",
    "                                                                                          svn_log_countvectorizer,\n",
    "                                                                                          svn_log_tfidf,\n",
    "                                                                                          svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f105_avgscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_log_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f106_maxscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_log_as_query), axis=1)\n",
    "features_qq_similarity[\"f107_sumscq_svn_log_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_log_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-guess",
   "metadata": {},
   "source": [
    "#### SCQ (SVNUnitNames as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Unit_names,\n",
    "                                                                                               svn_df_clean.Unit_names,\n",
    "                                                                                               svn_unitname_countvectorizer,\n",
    "                                                                                               svn_unitname_tfidf,\n",
    "                                                                                               svn_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f108_avgscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_svn_unitname_as_query, svn_documentcount), axis=1)\n",
    "features_qq_similarity[\"f109_maxscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_svn_unitname_as_query), axis=1)\n",
    "features_qq_similarity[\"f110_sumscq_svn_unitname_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_svn_unitname_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-laundry",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Jira_natural_text,\n",
    "                                                                                           jira_df_clean.Jira_natural_text,\n",
    "                                                                                           jira_all_countvectorizer,\n",
    "                                                                                           jira_all_tfidf,\n",
    "                                                                                           jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f111_avgscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_all_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f112_maxscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_all_as_query), axis=1)\n",
    "features_qq_similarity[\"f113_sumscq_jira_all_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_all_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-updating",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Summaries as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Summary, \n",
    "                                                                                               jira_df_clean.Summary,\n",
    "                                                                                               jira_summary_countvectorizer,\n",
    "                                                                                               jira_summary_tfidf,\n",
    "                                                                                               jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f114_avgscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_summary_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f115_maxscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_summary_as_query), axis=1)\n",
    "features_qq_similarity[\"f116_sumscq_jira_summary_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_summary_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-spirit",
   "metadata": {},
   "source": [
    "#### SCQ (JIRA Descriptions as Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_similarity[\"scq_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcSCQList(x.Description, \n",
    "                                                                                                   jira_df_clean.Description,\n",
    "                                                                                                   jira_description_countvectorizer,\n",
    "                                                                                                   jira_description_tfidf,\n",
    "                                                                                                   jira_documentcount),axis=1)\n",
    "\n",
    "features_qq_similarity[\"f117_avgscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcAvgSCQ(x.scq_jira_description_as_query, jira_documentcount), axis=1)\n",
    "features_qq_similarity[\"f118_maxscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcMaxSCQ(x.scq_jira_description_as_query), axis=1)\n",
    "features_qq_similarity[\"f119_sumscq_jira_description_as_query\"] = features_qq_similarity.apply(lambda x: calcSumSCQ(x.scq_jira_description_as_query), axis=1)\n",
    "\n",
    "#Remove SCQ stats\n",
    "features_qq_similarity.drop('scq_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_similarity.to_pickle(path= \"../data/03_processed/features_qq_similarity.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-enterprise",
   "metadata": {},
   "source": [
    "### Query Quality - Term Relatedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-owner",
   "metadata": {},
   "source": [
    "#### PMI (SVN as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_all_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_all_countvectorizer, svn_df_clean.Commit_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Commit_natural_text)\n",
    "\n",
    "#Create new dataFrame\n",
    "features_qq_termrelatedness = pd.DataFrame()\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_all_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Commit_natural_text,\n",
    "                                                                                               termFrequencies, \n",
    "                                                                                               termPairFrequencies, \n",
    "                                                                                               svn_df_clean.Commit_natural_text),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f120_avgpmi_svn_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_all_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f121_maxpmi_svn_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_all_as_query), axis=1)\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-hotel",
   "metadata": {},
   "source": [
    "#### PMI (SVNLogs as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_log_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_log_countvectorizer, svn_df_clean.Logs)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Logs)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_log_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Logs,\n",
    "                                                                                               termFrequencies, \n",
    "                                                                                               termPairFrequencies, \n",
    "                                                                                               svn_df_clean.Logs),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f122_avgpmi_svn_log_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_log_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f123_maxpmi_svn_log_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_log_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_log_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-efficiency",
   "metadata": {},
   "source": [
    "#### PMI (SVNUnitNames as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(svn_unitname_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(svn_unitname_countvectorizer, svn_df_clean.Unit_names)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, svn_df_clean.Unit_names)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_svn_unitname_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Unit_names,\n",
    "                                                                                                    termFrequencies, \n",
    "                                                                                                    termPairFrequencies, \n",
    "                                                                                                    svn_df_clean.Unit_names),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f124_avgpmi_svn_unitname_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_svn_unitname_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f125_maxpmi_svn_unitname_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_svn_unitname_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_svn_unitname_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stomach",
   "metadata": {},
   "source": [
    "#### PMI (JIRA as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_all_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_all_countvectorizer, jira_df_clean.Jira_natural_text)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Jira_natural_text)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_all_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Jira_natural_text, \n",
    "                                                                                                termFrequencies, \n",
    "                                                                                                termPairFrequencies, \n",
    "                                                                                                jira_df_clean.Jira_natural_text),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f126_avgpmi_jira_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_all_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f127_maxpmi_jira_all_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_all_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_all_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-youth",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Summaries as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_summary_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_summary_countvectorizer, jira_df_clean.Summary)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Summary)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_summary_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Summary, \n",
    "                                                                                                   termFrequencies, \n",
    "                                                                                                   termPairFrequencies, \n",
    "                                                                                                   jira_df_clean.Summary),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f128_avgpmi_jira_summary_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_summary_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f129_maxpmi_jira_summary_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_summary_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_summary_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-award",
   "metadata": {},
   "source": [
    "#### PMI (JIRA Descriptions as query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start timer\n",
    "startTime = time.time() \n",
    "\n",
    "#Create pairs and find frequencies\n",
    "termPairs = createTermPairs(jira_description_countvectorizer)\n",
    "termFrequencies = findTermFrequencies(jira_description_countvectorizer, jira_df_clean.Description)\n",
    "termPairFrequencies = findTermPairFrequencies(termPairs, jira_df_clean.Description)\n",
    "\n",
    "#Calculate IDF stats for each svn\n",
    "features_qq_termrelatedness[\"pmi_jira_description_as_query\"] = cartesian_df.apply(lambda x: calcPMIList(x.Description, \n",
    "                                                                                                      termFrequencies, \n",
    "                                                                                                      termPairFrequencies, \n",
    "                                                                                                      jira_df_clean.Description),axis=1)\n",
    "\n",
    "features_qq_termrelatedness[\"f130_avgpmi_jira_description_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcAvgPMI(x.pmi_jira_description_as_query), axis=1)\n",
    "features_qq_termrelatedness[\"f131_maxpmi_jira_description_as_query\"] = features_qq_termrelatedness.apply(lambda x: calcMaxPMI(x.pmi_jira_description_as_query), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_qq_termrelatedness.drop('pmi_jira_description_as_query', axis = 1, inplace=True)\n",
    "\n",
    "#Save results in pickle\n",
    "features_qq_termrelatedness.to_pickle(path= \"../data/03_processed/features_qq_termrelatedness.pkl\")\n",
    "\n",
    "endTime = time.time()\n",
    "timeDifference = calculateTimeDifference(startTime=startTime, endTime=endTime)\n",
    "print(\"Finished creating query quality features in \" + timeDifference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-material",
   "metadata": {},
   "source": [
    "## 3.8 Preprocess Data - Load and transform feature families needed for training\n",
    "Load features and create a normalized set of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Process-Related Features\n",
    "features_process_related = pd.read_pickle(r'../data/03_processed/features_process_related.pkl')\n",
    "\n",
    "#Load IR-Related Features\n",
    "features_information_retrieval = pd.read_pickle(r'../data/03_processed/features_information_retrieval.pkl')\n",
    "\n",
    "#Load Document Statistics Features\n",
    "features_document_statistics = pd.read_pickle(r'../data/03_processed/features_document_statistics.pkl')\n",
    "\n",
    "#Load Query Quality Features\n",
    "features_qq_specificity = pd.read_pickle(r'../data/03_processed/features_qq_specificity.pkl')\n",
    "features_qq_similarity = pd.read_pickle(r'../data/03_processed/features_qq_similarity.pkl')\n",
    "features_qq_termrelatedness = pd.read_pickle(r'../data/03_processed/features_qq_termrelatedness.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Process-Related Features\n",
    "features_process_related_normalized = normalize_data(features_process_related)\n",
    "\n",
    "##Normalize IR-Related Features\n",
    "features_information_retrieval_normalized = normalize_data(features_information_retrieval)\n",
    "\n",
    "#Normalize Document Statistics Features\n",
    "features_document_statistics_normalized = normalize_data(features_document_statistics)\n",
    "\n",
    "#Normalize Query Quality Features\n",
    "features_qq_specificity_normalized = normalize_data(features_qq_specificity)\n",
    "features_qq_similarity_normalized = normalize_data(features_qq_similarity)\n",
    "features_qq_termrelatedness_normalized = normalize_data(features_qq_termrelatedness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-tower",
   "metadata": {},
   "source": [
    "Put all features in a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a single data frame for the non-normalized features\n",
    "features_all_df = pd.concat([features_process_related,\n",
    "                             features_document_statistics,\n",
    "                             features_information_retrieval,\n",
    "                             features_qq_specificity,\n",
    "                             features_qq_similarity,\n",
    "                             features_qq_termrelatedness], axis=1)\n",
    "\n",
    "#Create a single data frame for the normalized features\n",
    "features_all_normalized_df = pd.concat([features_process_related_normalized,\n",
    "                                        features_document_statistics_normalized,\n",
    "                                        features_information_retrieval_normalized,\n",
    "                                        features_qq_specificity_normalized,\n",
    "                                        features_qq_similarity_normalized,\n",
    "                                        features_qq_termrelatedness_normalized], axis=1)\n",
    "\n",
    "#Save into xlsx files\n",
    "features_all_df.to_excel(excel_writer = \"../results/1. Trace Link Feature Data/features_non-normalized.xlsx\", index = False)\n",
    "features_all_normalized_df.to_excel(excel_writer = \"../results/1. Trace Link Feature Data/features_normalized.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
