{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pacakges\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Parameters to configure\n",
    "'''\n",
    "#Set file path\n",
    "dataset = 'Portfolio'\n",
    "\n",
    "# top-X to compute for feature importance\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-normalised datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "df = pd.DataFrame(columns=['algorithm', 'Balancing', 'Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'F0.5', 'Average Precision'])\n",
    "\n",
    "none_rf = pd.read_csv('../results/2. Non-Normalised Results/random_forests/none_results.csv')\n",
    "over_rf = pd.read_csv('../results/2. Non-Normalised Results/random_forests/over_results.csv')\n",
    "under_rf = pd.read_csv('../results/2. Non-Normalised Results/random_forests/under_results.csv')\n",
    "half_rf = pd.read_csv('../results/2. Non-Normalised Results/random_forests/5050_results.csv')\n",
    "\n",
    "\n",
    "file_list_string = ['none', 'over', 'under',' 5050']\n",
    "file_list = [none_rf, over_rf, under_rf, half_rf]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['random_forests'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# XGBoost\n",
    "none_xgb = pd.read_csv('../results/2. Non-Normalised Results/xg_boost/none_results.csv')\n",
    "over_xgb = pd.read_csv('../results/2. Non-Normalised Results/xg_boost/over_results.csv')\n",
    "under_xgb = pd.read_csv('../results/2. Non-Normalised Results/xg_boost/under_results.csv')\n",
    "half_xgb = pd.read_csv('../results/2. Non-Normalised Results/xg_boost/5050_results.csv')\n",
    "\n",
    "file_list = [none_xgb, over_xgb, under_xgb, half_xgb]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['xg_boost'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# LightGBM\n",
    "none_lgbm = pd.read_csv('../results/2. Non-Normalised Results/light_gbm/none_results.csv')\n",
    "over_lgbm = pd.read_csv('../results/2. Non-Normalised Results/light_gbm/over_results.csv')\n",
    "under_lgbm = pd.read_csv('../results/2. Non-Normalised Results/light_gbm/under_results.csv')\n",
    "half_lgbm = pd.read_csv('../results/2. Non-Normalised Results/light_gbm/5050_results.csv')\n",
    "\n",
    "file_list = [none_lgbm, over_lgbm, under_lgbm, half_lgbm]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['light_gbm'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "df.to_excel(excel_writer = f\"../results/Evaluation/{dataset}_F-metrics_non-normalized.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalised datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "df = pd.DataFrame(columns=['algorithm', 'Balancing', 'Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'F0.5', 'Average Precision'])\n",
    "\n",
    "none_rf = pd.read_csv('../results/3. Normalised Results/random_forests/none_results.csv')\n",
    "over_rf = pd.read_csv('../results/3. Normalised Results/random_forests/over_results.csv')\n",
    "under_rf = pd.read_csv('../results/3. Normalised Results/random_forests/under_results.csv')\n",
    "half_rf = pd.read_csv('../results/3. Normalised Results/random_forests/5050_results.csv')\n",
    "\n",
    "file_list_string = ['none', 'over', 'under',' 5050']\n",
    "file_list = [none_rf, over_rf, under_rf, half_rf]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['random_forests'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# XGBoost\n",
    "none_xgb = pd.read_csv('../results/3. Normalised Results/xg_boost/none_results.csv')\n",
    "over_xgb = pd.read_csv('../results/3. Normalised Results/xg_boost/over_results.csv')\n",
    "under_xgb = pd.read_csv('../results/3. Normalised Results/xg_boost/under_results.csv')\n",
    "half_xgb = pd.read_csv('../results/3. Normalised Results/xg_boost/5050_results.csv')\n",
    "\n",
    "file_list = [none_xgb, over_xgb, under_xgb, half_xgb]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['xg_boost'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# LightGBM\n",
    "none_lgbm = pd.read_csv('../results/3. Normalised Results/light_gbm/none_results.csv')\n",
    "over_lgbm = pd.read_csv('../results/3. Normalised Results/light_gbm/over_results.csv')\n",
    "under_lgbm = pd.read_csv('../results/3. Normalised Results/light_gbm/under_results.csv')\n",
    "half_lgbm = pd.read_csv('../results/3. Normalised Results/light_gbm/5050_results.csv')\n",
    "\n",
    "file_list = [none_lgbm, over_lgbm, under_lgbm, half_lgbm]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['light_gbm'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "df.to_excel(excel_writer = f\"../results/Evaluation/{dataset}_F-metrics_normalized.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "none_rf = pd.read_csv('../results/4. Feature Importance Results/random_forests/none_results.csv')\n",
    "over_rf = pd.read_csv('../results/4. Feature Importance Results/random_forests/over_results.csv')\n",
    "under_rf = pd.read_csv('../results/4. Feature Importance Results/random_forests/under_results.csv')\n",
    "half_rf = pd.read_csv('../results/4. Feature Importance Results/random_forests/5050_results.csv')\n",
    "\n",
    "feature_names = none_rf.columns.values.tolist()\n",
    "feature_names = feature_names[1:]\n",
    "df_columns = ['algorithm'] + ['Balancing'] + feature_names\n",
    "\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "file_list_string = ['none', 'over', 'under',' 5050']\n",
    "file_list = [none_rf, over_rf, under_rf, half_rf]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['random_forests'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# XGBoost\n",
    "none_xgb = pd.read_csv('../results/4. Feature Importance Results/xg_boost/none_results.csv')\n",
    "over_xgb = pd.read_csv('../results/4. Feature Importance Results/xg_boost/over_results.csv')\n",
    "under_xgb = pd.read_csv('../results/4. Feature Importance Results/xg_boost/under_results.csv')\n",
    "half_xgb = pd.read_csv('../results/4. Feature Importance Results/xg_boost/5050_results.csv')\n",
    "\n",
    "file_list = [none_xgb, over_xgb, under_xgb, half_xgb]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['xg_boost'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "# LightGBM\n",
    "none_lgbm = pd.read_csv('../results/4. Feature Importance Results/light_gbm/none_results.csv')\n",
    "over_lgbm = pd.read_csv('../results/4. Feature Importance Results/light_gbm/over_results.csv')\n",
    "under_lgbm = pd.read_csv('../results/4. Feature Importance Results/light_gbm/under_results.csv')\n",
    "half_lgbm = pd.read_csv('../results/4. Feature Importance Results/light_gbm/5050_results.csv')\n",
    "\n",
    "file_list = [none_lgbm, over_lgbm, under_lgbm, half_lgbm]\n",
    "\n",
    "count = 0\n",
    "for file in file_list:\n",
    "    temp_list = file.mean(axis=0)\n",
    "    temp_list = temp_list[1:].reset_index(drop=True)\n",
    "    temp_list = temp_list.tolist()\n",
    "    temp_list = ['light_gbm'] + [file_list_string[count]] + temp_list\n",
    "    df.loc[len(df)] = temp_list\n",
    "    count += 1\n",
    "\n",
    "df.to_excel(excel_writer = f\"./results/Evaluation/{dataset}_feature_importance.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import heapq\n",
    "\n",
    "feature_importance = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    row_local = list(row[3:])\n",
    "    top5 = heapq.nlargest(top_n, row_local)\n",
    "    feature_list = [row[1], row[2]]\n",
    "    for item in top5:\n",
    "        index = row_local.index(item)\n",
    "        feature = df.columns[(index+2)]\n",
    "        feature_value = feature, item\n",
    "        feature_list.append(feature)\n",
    "        feature_list.append(item)\n",
    "    feature_importance.append(feature_list)\n",
    "\n",
    "feature_importance = pd.DataFrame(columns=['algorithm', 'balancing', '1', '1', '2', '2', '3', '3', '4', '4', '5', '5', '6', '6', '7', '7',' 8', '8', '9', '9', '10', '10'], data=feature_importance)\n",
    "feature_importance.to_excel(excel_writer= f\"./results/Evaluation/{dataset}_feature_importance_top{top_n}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
